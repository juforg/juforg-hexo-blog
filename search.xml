<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[微软云devops之workitem使用]]></title>
    <url>%2Fessay%2F%E5%BE%AE%E8%BD%AF%E4%BA%91devops%E4%B9%8Bworkitem%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[graph TD A(新建WorkItem)--> CT{类型选择} subgraph 评估环节 CT -->|测出来确认是个缺陷| BUG[类型设为 Bug] CT -->|测试后不确认是否是问题| ISSUE[类型设为 Issue] CT -->|识别出是个可以直接执行的任务| TASK[类型设为 Task] CT -->|是个新需求| REQUIREMENT[类型设为 Requirement] CT -->|判断出是个比较大的模块功能| FEATURE[类型设为 feature] CT -->|识别出是个风险点| RISK[类型设为 risk] CT -->|识别出是个用户级比较大的需求变更| CHANGEREQUEST[类型设为 ChangeRequest] BUG -->CP{是否能够判断具体到开发责任人} FEATURE --> |创建需求子任务| REQUIREMENT ISSUE --> BA>需求分析人员] end CP --> |不能判定开发责任人|DEV_LEADER CP --> |能判定开发责任人|DEVELOP subgraph 分析环节 TASK -->|指派给| DEV_LEADER>开发经理] BA -->|分析为需要修复的bug| CP REQUIREMENT --> DEV_LEADER CHANGEREQUEST --> DEV_LEADER DEV_LEADER --> CAN_DEV{判断是否可以开发} CAN_DEV -->|可以开发实现|DL_ACTIVE((状态改为Active)) DL_ACTIVE --> NEED_DIVIDE{判断是否需要拆分任务} NEED_DIVIDE -->|需要拆分任务|CREATE_CHILD[创建子任务] CREATE_CHILD -->|父任务|CLOSE_SPLIT((状态改为Closed,Split)) end subgraph 开发环节 CREATE_CHILD -->|子任务|DEVELOP NEED_DIVIDE -->|不需要拆分任务,指派给|DEVELOP>开发人员] DEVELOP -->|开始开发|DEV_ACTIVE((状态改为Active)) DEVELOP -->|非自己能够处理的任务|CP DEV_ACTIVE -->|开发完成|DEV_COMPLETE((状态改为Resolved)) DEV_COMPLETE -->|指派给|TESTER>测试人员] TESTER -->|测试通过|CLOSED_PASS((状态改为Closed,Passed)) TESTER -->|测试不通过,状态改为Active|DEVELOP end subgraph 拒绝 BA -->|分析不是一个问题| CLOSED_REJECT CAN_DEV -->|无法开发实现|CLOSED_REJECT((状态改为Closed,Reject)) CLOSED_REJECT -->|指派给|CREATER>创建人] DEVELOP -->|无法开发|CLOSED_REJECT end]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[宋式编程命名风格]]></title>
    <url>%2Fessay%2F%E5%AE%8B%E5%BC%8F%E7%BC%96%E7%A8%8B%E5%91%BD%E5%90%8D%E9%A3%8E%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[What? Why? 命名一致性很重要 在同个项目或同个公司中，同个对象的命名最好一致，方便理解,避免同个名称在不同地方含义不同，或同个含义在不同地方有不同名称 让代码易于新读者理解更重要 使用的缩写一定要满足上一个一致性条件，不要用只有一个人能理解的缩写, 也不要通过砍掉几个字母来缩写单词,随意缩写严重降低了代码的可阅读性 采用有意义的命名（meaningful names）。变量的名字必须准确反映它的含义和内容 遵循当前语言的变量命名规则 不要对不同使用目的的变量使用同一个变量名 不要使用拼音 How? 一些特定的广为人知的缩写 例如用 i 表示迭代变量 T 表示模板参数 已有的命名规范 下划线命名法 全小写 每个逻辑分割点用下划线来标记 例如：test_underline 骆驼命名法 lowerCamelCase 每一个逻辑分割点都有一个大写字母来标记 首字母小写 例如: testCamel 帕斯卡命名法 与骆驼命名法类似,只不过帕斯卡命名法是首字母大写 例如： TestPascal 匈牙利命名法 以一个或者多个小写字母开头作为前缀,该字母要指明用途 dotNet编程环境使用较多 文件命 文件名要全部小写 可以包含下划线 (_) 或连字符 (-) “_” 更好 类命名 名词+名词 采用 帕斯卡命名法 抽象类命名使用 Abstract 或 Base 开头 异常类命名使用 Exception 结尾 测试类命名以它要测试的类的名称开始,以 Test 结尾，但不能以Test开头，否则有动词开头的嫌疑 将设计模式体现在名字中,有利于阅读者快速理解架构设计思想。正例: class OrderFactory; class LoginProxy; 常量命名 形容词/名词+名词 名词短语 所有单词的字母都是大写 使用下划线链接 类型常量 单个XX_TYPE 多个XX_TYPE_XX 变量命名 形容词/名词+名词 名词短语 成员变量 python 采用下划线命名法 java 采用骆驼命名法 后缀名词为主要含义 如：xx_id user_id 如：xx_name user_name 名称后缀 如：xx_count login_count 数量后缀 如: xx_amt loan_amt 金额后缀 如：xx_type login_type 类型后缀 如：xx_date 日期后缀 如：xx_time 时间后缀 如果变量为对象 要么直接将对象名用下划线风格改写为变量名 如果需要定义同个对象的多个变量，则加前缀以示区别 如：user 这里只有后缀已经能够表达清晰 如：sys_user 这里必须加前缀才能把意思表达清晰 如果变量为集合类型，则后缀为具体的集合类型名称 如：xx_list 如：xx_array 如：xx_map 如：xx_set 如：xx_dict 如：xx_json 函数命名 动词+名词 python 采用下划线命名法 java 采用骆驼命名法 前缀动词表达动作清晰 如：add_xxx 增加 如：cal_xxx 计算 如：get_xxx 获取 如：query_xxx search_xxx 查询 如：check_xxx if_xxx 判断 参考 https://github.com/unbug/codelf/wiki https://google.github.io/styleguide/pyguide.html https://www.zhihu.com/question/21440067]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中华民族朝代-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fchina-dynasty-kg%2F%2F</url>
    <content type="text"><![CDATA[中华民族朝代 夏 约前2029年-约前1559年 商 周 西周 起止时间 东周 战国 春秋 西楚 汉 西汉 新朝 玄汉 东汉 三国 曹魏 蜀汉 孙吴 晋 西晋 东晋 南北朝 北朝 五胡十六国 南朝 隋 唐 宋 元 明 无汉之和亲，无唐之结盟，无宋之纳贡 清 民国 中国]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[缓存-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fcache-kg%2F%2F</url>
    <content type="text"><![CDATA[缓存 谈资 本地缓存 分布式缓存 分级缓存 问题 缓存击穿 什么是击穿 大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去 会带来什么问题 会造成某一时刻数据库请求量过大，压力剧增 如何解决 在第一个查询数据的请求上使用一个 互斥锁来锁住它 缓存并发 缓存雪崩 什么是缓存雪崩 当某一时刻发生大规模的缓存失效的情况，比如你的缓存服务宕机了，会有大量的请求进来直接打到DB上面 解决办法 事前 使用集群缓存，保证缓存服务的高可用 事中 ehcache本地缓存 + Hystrix限流&amp;降级,避免MySQL被打死 加锁队列 只允许抢锁成功的请求去库里面读取数据然后将其存入缓存中，再释放锁，让后续的读请求从缓存中取数据 分布式锁 弊端 过多的读请求线程堵塞，将机器内存占满，依然没有能够从根本上解决问题。 事后 开启Redis持久化机制，尽快恢复缓存集群 缓存失效 缓存穿透 什么是缓存穿透 查询不存在数据的现象 穿透带来的问题 致你的数据库由于压力过大而宕掉 解决办法 缓存空值 它的过期时间会很短，最长不超过五分钟 BloomFilter 布隆过滤器 将所有可能存在的数据哈希到一个足够大的 bitmap 中 特点 只要返回数据不存在，则肯定不存在 返回数据存在，但只能是大概率存在 同时不能清除其中的数据 Bloom Filter 有一定的误报率，这个误报率和 Hash 算法的次数 H，以及数组长度 L 都是有关的 热点数据集中失效 解决办法 设置不同的失效时间 互斥锁 缓存一致性问题 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存 更新缓存的四种Design Pattern Cache aside 失效 应用程序先从 cache 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中 命中 应用程序从 cache 中取数据，取到后返回 更新 先把数据存到数据库中，成功后，再让缓存失效 Read through 当缓存失效的时用缓存服务自己来加载，从而对应用方是透明的 查询操作中更新缓存 Write through 当有数据更新的时候 如果没有命中缓存，直接更新数据库，然后返回 如果命中了缓存 则更新缓存 由 Cache 自己更新数据库（这是一个同步操作） Write behind caching 在更新数据的时候，只更新缓存，不更新数据库 redis 工作模型 集群模式 Twemproxy Codis 单机（Standalone） 哨兵机制 功能 监控（Monitoring） 哨兵会不断地检查主节点和从节点是否运作正常 自动故障转移（Automatic failover） 当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 配置提供者（Configurationprovider 客户端在初始化时，通过连接哨兵来获得当前 Redis 服务的主节点地址 通知（Notification） 哨兵可以将故障转移的结果发送给客户端 架构 哨兵节点 哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的 Redis 节点，不存储数据 数据节点 主节点和从节点都是数据节点 部署 主从节点 哨兵节点 单线程的多路 IO 复用模型 过期淘汰机制/数据淘汰策略 volatile-lru 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key allkeys-random 从数据集（server.db[i].dict）中任意选择数据淘汰。 no-enviction 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错 持久化 RDB快照 AOF append-only file Redis 事务 Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。 分布式集群的常见形式 分布式锁 setnx+lua set key value px milliseconds nx 要点 set命令要用set key value px milliseconds nx； value要具有唯一性； 释放锁时要验证value值，不能误解锁； Redlock 数据结构 链表list 命令 lpush、rpush、lpop、rpop、lrange 双向链表 可以直接获得头、尾节点。 set 命令 sadd、spop、smembers、sunion zset/Sorted Set 命令 zadd、zrange、zrem、zcard 字典(Hash) 命令 hget、hset、hgetall 渐进式rehash String 命令 set、get、decr、incr、mget 跳跃表skipList skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的 skiplist和平衡树的时间复杂度都为O(log n)，大体相当 整数集合(intset) 压缩列表(ziplist) 顺序型数据结构 一般用于小数据存储 快速列表(quicklist) 一个由ziplist组成的双向链表 redis3.2 压缩算法，采用的LZF——一种无损压缩算法 HyperLogLog 2.8.9 基数统计的算法 如何保证redis和DB中的数据一致性 开发规范 key名设计 可读性和可管理性 以业务名(或数据库名)为前缀(防止key冲突)，用冒号分隔，比如业务名:表名:id 简洁性 保证语义的前提下，控制key的长度，当key较多时，内存占用也不容忽视 不要包含特殊字符 value设计 拒绝bigkey(防止网卡流量、慢查询) string类型控制在10KB以内，hash、list、set、zset元素个数不要超过5000。 选择适合的数据类型。 实体类型(要合理控制和使用数据结构内存编码优化配置,例如ziplist，但也要注意节省内存和性能之间的平衡) 控制key的生命周期，redis不是垃圾桶。 建议使用expire设置过期时间(条件允许可以打散过期时间，防止集中过期)，不过期的数据重点关注idletime。 命令使用 禁用命令 禁止线上使用keys、flushall、flushdb等 客户端使用 避免多个应用使用一个Redis实例 不相干的业务拆分，公共数据做服务化。 使用带有连接池的数据库 高并发下建议客户端添加熔断功能(例如netflix hystrix) 设置合理的密码 根据自身业务类型，选好maxmemory-policy(最大内存淘汰策略)，设置好过期时间 默认策略是volatile-lru，即超过最大内存后，在过期键中使用lru算法进行key的剔除，保证不过期数据不被删除，但是可能会出现OOM问题 使用Pipeline加速查询速度 相关工具 数据同步 redis间数据同步可以使用：redis-port big key搜索 Pipeline 将一组 Redis 命令进行组装 通过一次 RTT 传输给 Redis 这组 Redis 命令按照顺序执行并将结果返回给客户端 通信协议 Redis Serialization Protocol RESP 实现简单 快速解析 可读性强 支持二进制安全 支持的数据类型 Simple Strings + 非二进制安全字符串 Errors - Integers : Bulk Strings $ 多行字符串 二进制安全 最大长度是512MB 以$+数字开头，以\r\n结束 Arrays * 通过首个字节区分 每一部分结束时，Redis统一使用“\r\n”表示结束 客户端和服务器端通信 如果Redis客户端订阅了Pub/Sub频道 协议就会变成一种推送协议 发布订阅 SUBSCRIBE channel [channel ...] PUBLISH channel message Memcached 数据结构 String 多线程，非阻塞 IO 复用的网络模型 对比 leveldb https://github.com/google/leveldb KV数据库引擎]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[万能图床增加imgur支持]]></title>
    <url>%2Fessay%2F%E4%B8%87%E8%83%BD%E5%9B%BE%E5%BA%8A%E5%A2%9E%E5%8A%A0imgur%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[What? imgur 是个什么鬼 两点信息 : &lt;u&gt;专门的图床网站，跟阿里云腾讯云的对象存储比 职能更单一&lt;/u&gt; &lt;u&gt;国外的，而且 被墙了&lt;/u&gt; Where? https://imgur.com/ https://apidocs.imgur.com/ How? 如果你没有翻墙软件或代理服务器，可以不用往后看了 使用说明 注册账号 新建application以获取接口调用参数 client_id 和client_secret 安装postman imgur 接口 获取token推荐方式 brew cask install postman 启动postman 打开postman后随便点击个api 填上所需参数 https://www.getpostman.com/oauth2/callback https://api.imgur.com/oauth2/authorize https://api.imgur.com/oauth2/token APPLICATION_STATE 点击 request token 这里需要注意，postman 配置了proxy 也不会弹出这个页面，需要配置全局代理才起作用 点击 allow 得到上述参数后开始配置alfred workflow wntc imgur_use 为true 才能使用 不配或 false 不会上传到imgur 使用 快捷键使用 默认快捷键 为 cmd+shift+p ，该方法会将图片上传到所有配置正确的图床， 如果配置了，favor_yun 为 imgur ，则剪贴板默认就是 imgur 的markdown url，如果没配favor_yun，虽然上传了，但是返回的地址第一个上传的云 参考 官方python库]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>alfred</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用架构-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fapp-arch-kg%2F%2F</url>
    <content type="text"><![CDATA[应用架构 高并发架构 网站高并发 异步化 并行化 负载均衡 二层负载均衡 三层负载均衡 四层负载均衡 七层负载均衡 常用工具 硬件 F5 A10 Radware 软件 LVS LVS 主要用来做四层负载均衡 DR 模式 TUN 模式 NAT 模式 Nginx Nginx 主要用来做七层负载均衡 并发性能 官方支持每秒 5 万并发，实际国内一般到每秒 2 万并发 有优化到每秒 10 万并发的，具体性能看应用场景 HAProxy HAProxy 主要用来做七层负载均衡 负载均衡算法 静态负载均衡算法 轮询 Round Robin 随机方式 random 比率 Ratio 优先权 Priority 哈希方式 hash 一致性哈希 consistentHash 动态负载均衡算法 最少连接数 Least Connection 最快响应速度 Fastest 观察方法 Observed 预测法 Predictive 动态性能分配 Dynamic Ratio-APM 动态服务器补充 Dynamic Server Act 服务质量 QoS 服务类型 ToS 规则模式 吞吐量 QPS 每秒内处理请求次数 TPS 每秒内处理事务次数 RT 响应时间 架构图 分支主题 读写分离 分库分表 缓存 搜索引擎 消息队列 重复请求 场景 用户快速多次点击按钮 2）Nginx失败重试机制 3）服务框架失败重试机制 分布式系统中网络的三态性：成功，失败，未知，未知时一般三方系统会定期重试 4）MQ消息重复消费 5）第三方支付支付成功后，因为异常原因导致的多次异步回调； 防重 幂等性 一个操作，不论执行多少次，产生的效果和返回的结果都是一样的 并发控制+返回相同结果 技术方案 利用唯一交易号(流水号)实现 token令牌 要申请，一次有效性，可以限流 redis要用删除操作来判断token，删除成功代表token校验通过 Select+[Insert/Update] 因为两条Sql非原子操作，适合并发量不高的新增或修改场景 防重表 .唯一索引，防止新增脏数据 分布式锁 适合分布式高并发场景或不适用其它方式的场景，比如发验证短信60秒控制，因为控制信息是记录在缓存中的，无法使用乐观锁等方式，因此只能使用分布式锁 乐观锁 悲观锁 状态机幂等 支付缓冲区 幂等的不足 增加了额外控制幂等的业务逻辑，复杂化了业务功能 把并行执行的功能改为串行执行，降低了执行效率 适用 增加、更新 天然幂等 查询操作 删除 发送消息 发起一笔付款请求 创建业务订单 缓存和数据库之间数据一致性问题 机制 Cache Aside模式 从数据库缓存查询，如果缓存没有命中则从数据库中进行查找 缓存命中 当查询的时候发现缓存存在，那么直接从缓存中提取。 缓存失效： 当缓存没有数据的时候，则从database里面读取源数据，再加入到cache里面去。 缓存更新： 当有新的写操作去修改database里面的数据时，需要在写操作完成之后，让cache里面对应的数据失效。 缺陷 一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据 Read Through模式 应用程序始终从缓存中请求数据 如果缓存没有数据，则它负责使用底层提供程序插件从数据库中检索数据 检索数据后，缓存会自行更新并将数据返回给调用应用程序 好处 我们总是使用key从缓存中检索数据, 调用的应用程序不知道数据库， 由存储方来负责自己的缓存处理，这使代码更具可读性， 代码更清晰 缺陷 开发人员需要给编写相关的程序插件，增加了开发的难度性 Write Through模式 当数据发生更新的时候，先去Cache里面进行更新，如果命中了，则先更新缓存再由Cache方来更新database 如果没有命中的话，就直接更新Cache里面的数据 Write Behind Caching模式 先将数据写入到缓存里面，然后再异步的写入到database中进行数据同步 好处 减少我们对于数据的database里面的直接访问 降低压力，同时对于database的多次修改可以进行合并操作，极大的提升了系统的承载能力 缺陷 当cache机器出现宕机的时候，数据会有丢失的可能。 预热 未预热现象 DB重启后，瞬间死亡 服务重启后，访问异常 Warm Up 冷启动/ 解决方式 接口放量 走马观花 把所有的接口都提前访问一遍，让系统对资源进行提前准备 状态保留 系统在死亡时做一个快照，然后在启动时，原封不动的还原回来 普通tcp高并发 消息中间件的高并发 分布式系统 谈资 什么是分布式架构 建立在网络之上的软件系统 在一个分布式系统中，一组独立的计算机展现给用户的是一个统一的整体，就好像是一个系统似的 演进 初始阶段架构 特征：应用程序，数据库，文件等所有资源都放在一台服务器上 应用服务和数据服务以及文件服务分离 特征：应用程序、数据库、文件分别部署在独立的资源上。 ：好景不长，发现随着系统访问量的再度增加，webserver机器的压力在高峰期会上升到比较高，这个时候开始考虑增加一台webserver 使用缓存改善性能 系统访问特点遵循二八定律，即80%的业务访问集中在20%的数据上 特征：数据库中访问较集中的一小部分数据存储在缓存服务器中，减少数据库的访问次数，降低数据库的访问压力。 使用“应用服务器”集群 特征：多台服务器通过负载均衡同时向外部提供服务，解决单台服务器处理能力和存储空间上限的问题。 使用集群是系统解决高并发、海量数据问题的常用手段 过向集群中追加资源，提升系统的并发处理能力，使得服务器的负载压力不再成为整个系统的瓶颈 数据库读写分离 特征：多台服务器通过负载均衡同时向外部提供服务，解决单台服务器处理能力和存储空间上限的问题。 反向代理和CDN加速 特征：采用CDN和反向代理加快系统的访问速度。 描述： 为了应付复杂的网络环境和不同地区用户的访问，通过CDN和反向代理加快用户访问的速度，同时减轻后端服务器的负载压力。 CDN与反向代理的基本原理都是缓存。 “分布式文件”系统 和 “分布式数据库” 特征：数据库采用分布式数据库，文件系统采用分布式文件系统 使用NoSQL和搜索引擎 特征：系统引入NoSQL数据库及搜索引擎。 业务拆分 特征：公共的应用模块被提取出来，部署在分布式服务器上供应用服务器调用。 10、分布式服务 特征：公共的应用模块被提取出来，部署在分布式服务器上供应用服务器调用。 分布式服务应用会面临哪些问题 1、当服务越来越多时，服务URL配置管理变得非常困难，F5硬件负载均衡器的单点压力也越来越大。 2、当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。 3、接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？ 4、服务多了，沟通成本也开始上升，调某个服务失败该找谁？服务的参数都有什么约定？ 5、一个服务有多个业务消费者，如何确保服务质量？ 6、随着服务的不停升级，总有些意想不到的事发生，比如cache写错了导致内存溢出，故障不可避免，每次核心服务一挂，影响一大片，人心慌慌，如何控制故障的影响面？服务是否可以功能降级？或者资源劣化？ 系统拆分 分布式服务框架 分布式锁 redis设计分布式锁 优点 实现简单 性能对比 ZK 和 MySQL 较好 对于一些要求比较严格的场景可以使用 RedLock 可以利用或者借鉴 Redission 缺点 需要维护 Redis 集群 如果要实现 RedLock 需要维护更多的集群 Redisson 开源框架 加锁机制 锁互斥机制 watch dog 自动延期机制 可重入加锁机制 释放锁机制 Redlock zk设计分布式锁 优点 zK 可以不需要关心锁超时时间 支持读写锁 ZK 获取锁会按照加锁的顺序 高可用 缺点 ZK 需要额外维护 性能和 MySQL 相差不大，依然比较差 Curator开源框架 自研分布式锁 谷歌的 Chubby MySQL 缺点 对于高并发的场景并不是很适合 实现起来较为繁琐 优点 理解起来简单 不需要维护额外的第三方中间件 特点 互斥性 可重入性 锁超时 高效，高可用 支持阻塞和非阻塞 支持公平锁和非公平锁(可选) 安全问题 长时间的 GC pause 这个 STW 时间比较长，导致分布式锁进行了释放 时钟发生跳跃 对于 Redis 服务器如果其时间发生了跳跃，肯定会影响我们锁的过期时间 长时间的网络 I/O 分布式事务 TCC Try 尝试执行，完成所有业务检查（一致性），预留必需业务资源（准隔离性 Confirm 确认真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试 Cancel 取消执行，释放 Try 阶段预留的业务资源，Cancel 操作满足幂等性。Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致 框架 ByteTCC LCN 阿里Fescar 2PC XA 分布式会话 Spring Session 分布式id 特点 趋势递增 全局唯一性 信息安全 特定场景下，能生成无规则（或者看不出规则）的序列号 单调递增 常见策略 uuid 优点 性能好、高可扩展性：本地生成，无网络消耗，不需要考虑性能瓶颈 缺点： 无法保证趋势递增 UUID 过长，如果需要在数据库存储，作为主键建立索引效率低。 Snowflake 原理 生成结果为 64 位 Long 型数值 12 位毫秒内的序列 10 位数据机器位 41 位时间戳（毫秒级） 首位符号位：因为 ID 一般为正数，该值为 0 优点 趋势递增，且按照时间有序。 性能高、稳定性高、不依赖数据库等第三方系统。 可以按照自身业务特性灵活分配 bit 位。 缺点 依赖于机器时钟，时钟回拨会造成暂不可用或重复发号。 snowflake 是 Twitter 开源的一个 ID 生成算法 适用场景：要求高性能，可以不连续，数据类型为 long 型。 Flicker Flicker 方案主要思路是设计单独的库表，利用数据库的自增 ID 来生成全局 ID 优点 充分利用了数据库自增 ID 机制，生成的 ID 有序递增。 缺点 依赖于数据库，可用性低 水平扩展困难：定义好了起始值、步长和机器台数之后，如果要添加机器就比较麻烦了。 适用场景：数据量不多，并发量不大。 Redis 因为 Redis 中的所有命令都是单线程的，可以利用 Incrby命令来模拟 ID 的递增。 优点： 不依赖数据库，且性能优于依赖数据库的 Flicker 方案。 缺点 扩展性低，Redis 集群需要设置好初始值与步长。 Reids 宕机可能会生成重复的Id。 适用场景：Redis 集群高可用，并发量高。 Leaf 美团的 Leaf 分布式 ID 生成系统 在 Flicker 策略 与 Snowflake 算法的基础上做了两套优化的方案 Leaf-segment 数据库方案 Leaf-snowflake 方案 分布式发号器 分布式数据库 内聚性 是指每一个数据库分布节点高度自治，有本地的数据库管理系统 透明性 是指每一个数据库分布节点对用户的应用来说都是透明的，看不出是本地还是远程。 在分布式数据系统中，用户感觉不数据是分布的，即用户不须知道关系是否分割，有无副本，数据存在于那个站点以及事物在哪个站点上执行 mycat mariadb postgreSql 分布式文件系统 Hadoop 的 HDFS google的 GFS 淘宝的 TFS 分布式缓存系统 hbase mongdb 分布式计算 分布式一致性 拜占庭将军问题 共识算法 高可用架构 库 哨兵(sentinel) 阿里 Netflix Hystrix 手段 服务冗余 无状态化 超时机制 负载均衡 幂等设计 异步化设计 不关心返回结果的服务 不太重要的服务 服务限流降级熔断 限流 单个应用的限流 Guava 包RateLimiter 分布式限流 aop+redis spring-cloud-starter-alibaba-sentinel current limiting 只允许系统能够承受的访问量进来，超出的会被丢弃 限流策略 基于请求限流 限制总量 限制某个指标的累积上限 直播间的用户总数上限为100万，超过后用户无法进入 抢购商品数量为100，限制抢购用户上限为1万个，超过或直接拒绝 限制时间量 限制一段时间内某个指标的上限 一分钟内只允许1000个用户访问 每秒请求峰值为10万 都需要找到合适的阀值 需要通过性能压测来确定阀值或者逐步优化 基于资源限流 内部资源有 连接数 文件句柄 线程数 请求队列 CPU利用率 实现方式 计数器 维护一个计数器，来一个请求计数加一，达到阈值时，直接拒绝请求 漏斗模式 漏斗很多是用一个队列实现的，当流量过多时，队列会出现积压，队列满了，则开始拒绝请求 Leaky Bucket 令牌桶 令牌通和漏斗模式很像，主要的区别是增加了一个中间人 这个中间人按照一定的速率放入一些token，然后，处理请求时，需要先拿到token才能处理，如果桶里没有token可以获取，则不进行处理 Token Bucket 速率限制（Rate Limiting 网络流量整形（Traffic Shaping 池化技术 天生自带限流基因 限流的一些注意点 限流越早设计约好，架构成型后，不容易加入 限流模块不要成为系统的瓶颈，性能要求高 最好有个开关，可以直接介入 限流发生时，能及时发出通知事件 限流发生时，给用户提供友好的提示 。 应用场景 秒杀 抢购 熔断 防止应用程序不断地尝试可能超时和失败的服务 弃卒保帅 降级 系统将某些不重要的业务或接口的功能降低 降级的思想是丢车保帅 为了解决资源不足和访问量增加的矛盾 在有限的资源情况下，为了能抗住大量的请求，就需要对系统做出一些牺牲 也有理解为强一致将为最终一致，或实时返回降为异步范围 降级策略 异常比例 响应时间 异常数 降级方式 系统后门降级 系统预留后门用于降级，比如提供一个降级URL，访问URL时就执行降级指令 缺点：如果服务器数量多，需要一台一台去操作，效率低 独立系统降级 将降级操作独立到一个单独的系统中，可以实现复杂的权限管理、批量操作等功能 降级的注意点 梳理和分析 哪些是核心流程必须保证的，哪些是可以牺牲的 排队 让用户等待一段时间，而不是像限流方式直接拒绝用户 等待比直接拒绝要好，比如支付请求 排队模块 排队模块 排队模块 削峰填谷 区别 触发原因 熔断是故障引起 降级是从整体负荷考虑 案例 无缝切换线上服务 微服务架构 Spring Cloud 微服务架构 Feign 动态代理 建立连接、构造请求、发起请求、获取响应、解析响应 Eureka 注册中心 专门负责服务的注册与发现 Ribbon 负载均衡 Round Robin 轮询算法 Hystrix 隔离、熔断以及降级的一个框架 resilience4j spring-cloud-starter-alibaba-sentinel 熔断器 三种状态 关闭状态 请求是可以被放行的 打开状态 当熔断器统计的失败次数触发开关时，转为打开状态 所有请求都是不被放行的，直接返回失败 半开状态 打开状态经过一个设定的时间窗口周期后，熔断器才会转换到半开状态 当熔断器处于半开状态时，当前只能有一个请求被放行 这个被放行的请求获得远端服务的响应后，假如是成功的，熔断器转换为关闭状态，否则转换到打开状态。 Config 分布式配置 sleuth 服务跟踪 bus 消息总线 steam 数据流 task 任务 Spring Cloud Alibaba Sentinel 分布式系统的流量防卫兵 entinel-dashboard 整合Sentine步骤l 第一步：在Spring Cloud应用的 pom.xml中引入Spring Cloud Alibaba的Sentinel模块： 第二步：在Spring Cloud应用中配置sentinel dashboard的访问地址 第三步：创建应用主类，并提供一个rest接口 @SentinelResource 配置限流规则 异常处理 微服务网关 Zuul 负责网络路由的 blocking APIs doesn't support any long lived connections, like websockets spring cloud gateway non-blocking APIs tightly integrated with Spring 目标是微服务架构下的一站式解决方案 分布式链路跟踪系统 业界实现方案 开源 Twitter-Zipkin 主要收集“trace 数据 Apache-HTrace PinPoint 仅能用于 java 服务器 SkyWalking Uber-jaeger 主要收集“trace 数据 CAT 闭源 谷歌Dapper 阿里-鹰眼Tracing 新浪Watchman 唯品会的 Mercury 美团的MTrace 腾讯天机阁 解决的问题 故障定位难 容量评估难 链路梳理难 性能分析难 好处 跨团队的技术解耦 快速迭代 高并发 RPC框架 把复杂性屏蔽，就是RPC框架的职责 client端 序列化、反序列化、连接池管理、负载均衡、故障转移、队列管理，超时管理、异步管理等等 server端 服务端组件、服务端收发包队列、io线程、工作线程、序列化反序列化等 Dubbo 负载均衡 限流降级 异步调用 下线服务 定位始终是一款 RPC 框架 配置中心 Disconf 不再维护 Spring Cloud Config Apollo Nacos etcd 微服务治理 服务雪崩 服务熔断 服务降级 发布和运维 DevOps 和容器层 坑 全链路的压测系统 Kubernetes 服务拆分 服务拆分的前提 有一个持续集成的平台 使得服务在拆分的过程中，保持功能的一致性 API 和 UI 要动静分离 API 由 API 网关统一的管理，这样后端无论如何拆分，可以保证对于前端来讲，是统一的入口 数据库，需要进行良好的设计 要做应用的无状态化 服务拆分的时机 提交代码频繁出现大量冲突 小功能要积累到大版本才能上线，上线开总监级别大会 横向扩展流程复杂，主要业务和次要业务耦合 熔断降级全靠 if-else 服务拆分的方法 服务拆分的规范 服务拆分最多三层，两次调用 基础服务层 用于屏蔽数据库，缓存层，提供原子的对象查询接口。有了这一层，当数据层做一定改变的时候，例如分库分表，数据库扩容，缓存替换等。 主要做数据库的操作和一些简单的业务逻辑，不允许调用其他任何服务 组合服务层 这一层调用基础服务层，完成较为复杂的业务逻辑，实现分布式事务也多在这一层。 可以调用基础服务层，完成复杂的业务逻辑，可以调用组合服务层，不允许循环调用，不允许调用 Controller 层服务。 Controller 层 接口层，调用组合服务层对外 仅仅单向调用，严禁循环调用 将串行调用改为并行调用，或者异步化 接口应该实现幂等 接口数据定义严禁内嵌，透传 规范化工程名 服务雪崩 应对机制 熔断机制 雪崩效应 Service Mesh Istio Linkerd Envoy Conduit Linkerd 2.0 Mesher ServiceComb Dubbo Mesh 鉴权 单点登录（SSO） 产生大量非常琐碎的网络流量和重复的工作 分布式 Session 方案 缺点 共享存储需要一定保护机制 客户端 Token 方案 客户端 Token 与 API 网关结合 所有请求都通过网关 监控系统 日志类 调用链类（Tracing） 度量类（Metrics） CAP 理论 C 一致性Consisteny 两阶段提交协议（2PC） 三阶段提交协议（3PC） Paxos协议 ZAB 协议 在 paxos 的基础上 Raft协议 概念 term 任期，比如新的选举任期，即整个集群初始化时，或者新的Leader选举就会开始一个新的选举任期 大多数 假设一个集群由N个节点组成，那么大多数就是至少N/2+1 状态 每个节点有三种状态 Follower Candidate Leader 日志复制 Log Replication 两个超时 选举超时 心跳超时 Gossip 基本思想 一个节点想要分享一些信息给网络中的其他的一些节点 于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点 应用 Redis Cluster Consul Apache Cassandra 失败容错 Gossip也具备失败容错的能力，即使网络故障等一些问题，Gossip协议依然能很好的运行。因为一个节点会多次分享某个需要传播的信息，即使不能连通某个节点，其他被感染的节点也会尝试向这个节点传播信息 健壮性 Gossip协议下，没有任何扮演特殊角色的节点（比如leader等）。任何一个节点无论什么时候下线或者加入，并不会破坏整个系统的服务质量 可扩展性 不完美的地方 拜占庭问题（Byzantine 如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题 每次的读操作，都能获得最新的数据 A 可用性Availability 每个请求都能在合理的时间内获得符合预期的响应 P 分区容错性Partition tolerance 当节点之间的网络出现问题之后，系统依然能正常提供服务 网络是不可能做到100%可靠的 P（分区容错性）就是一个必选项 任何分布式系统只能同时满足这三项中的两项 CA 理论上是不可能有CA组合 CP AP 架构图 什么是架构 架构是结构和愿景。 什么是架构图 架构图的作用 解决沟通障碍 达成共识 减少歧义 架构图分类 场景视图 逻辑视图 物理视图 处理流程视图 开发视图 案例 电商秒杀 服务无状态 用户session 安全认证 HTTP 基本认证 HTTP Basic Authentication（HTTP 基本认证） HTTP 1.0 提出的一种认证机制 流程 客户端发送 HTTP Request 给服务器 因为 Request 中没有包含 Authorization header，服务器会返回一个 401 Unauthozied 给客户端，并且在 Response 的 Header &quot;WWW-Authenticate&quot; 中添加信息 客户端把用户名和密码用 BASE64 加密后，放在 Authorization Header 中发送给服务器， 认证成功 服务器将 Authorization Header 中的用户名密码取出，进行验证， 如果验证通 过，将根据请求，发送资源给客户端 基于 Session 的认证 基于 Token 的认证 好处 服务端无状态 性能较好 支持移动设备 支持跨程序调用 Token 注销 当用户注销时，Token 的有效时间还没有到，还是有效的 多采用短期令牌，比如令牌有效期是 20 分钟，这样可以一定程度上降低注销后 Token 可用性的风险 JWT JSON Web Token RFC 7519 JWT 一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息 认证流程 客户端调用登录接口（或者获取 token 接口），传入用户名密码。 服务端请求身份认证中心，确认用户名密码正确。 服务端创建 JWT，返回给客户端。 客户端拿到 JWT，进行存储（可以存储在缓存中，也可以存储在数据库中，如果是浏览器，可以存储在 Cookie 中）在后续请求中，在 HTTP 请求头中加上 JWT。 服务端校验 JWT，校验通过后，返回相关资源和数据。 JWT 结构 第一段为头部（Header 关于该 JWT 的最基本的信息 其类型以及签名所用的算法等 第二段为载荷（Payload) 标准中注册的声明 iss：JWT 签发者 sub：JWT 所面向的用户 aud：接收 JWT 的一方 exp：JWT 的过期时间，这个过期时间必须要大于签发时间 nbf：定义在什么时间之前，该 JWT 都是不可用的 iat：JWT 的签发时间 jti：JWT 的唯一身份标识，主要用来作为一次性 token, 从而回避重放攻击。 公共的声明 一般添加用户的相关信息或其他业务需要的必要信息 不建议添加敏感信息，因为该部分在客户端可解密 私有的声明 不建议存放敏感信息 第三段为签名（Signature） 创建签名需要使用 Base64 编码后的 header 和 payload 以及一个秘钥 通过 header 中声明的加密方式进行加盐 secret 组合加密 HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 每一段内容都是一个 JSON 对象 将每一段 JSON 对象采用 BASE64 编码 将编码后的内容用. 链接一起就构成了 JWT 字符串 优点 跨语言，JSON 的格式保证了跨语言的支撑 基于 Token，无状态 占用字节小，便于传输 OAuth 2.0 特点 简单 ：不管是 OAuth 服务提供者还是应用开发者，都很容易于理解与使用； 安全 ：没有涉及到用户密钥等信息，更安全更灵活； 开放： 任何服务提供商都可以实现 OAuth，任何软件开发商都可以使用 OAuth； 不向后兼容 OAuth 1.0 2012 年 10 月RFC 6749 四大角色 客户端 客户端是代表资源所有者对资源服务器发出访问受保护资源请求的应用程序 资源拥有者 资源拥有者是对资源具有授权能力的人 资源服务器 资源所在的服务器 授权服务器 为客户端应用程序提供不同的 Token，可以和资源服务器在统一服务器上，也可以独立出去 四种授权方式 授权码模式（authorization code） 流程 用户访问客户端，后者将前者导向认证服务器。 用户选择是否给予客户端授权。 假设用户给予授权，认证服务器将用户导向客户端事先指定的&quot;重定向 URI&quot;（redirection URI），同时附上一个授权码。 客户端收到授权码，附上早先的&quot;重定向 URI&quot;，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 认证服务器核对了授权码和重定向 URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。 简化模式（implicit） 跳过了&quot;授权码&quot;这个步骤 流程 客户端将用户导向认证服务器。 用户决定是否给于客户端授权。 假设用户给予授权，认证服务器将用户导向客户端指定的&quot;重定向 URI&quot;，并在 URI 的 Hash 部分包含了访问令牌。 浏览器向资源服务器发出请求，其中不包括上一步收到的 Hash 值。 资源服务器返回一个网页，其中包含的代码可以获取 Hash 值中的令牌。 浏览器执行上一步获得的脚本，提取出令牌。 浏览器将令牌发给客户端。 密码模式（Resource Owner Password Credentials） 用户向客户端提供自己的用户名和密码 通常用在用户对客户端高度信任的情况下 客户端模式（Client Credentials） SSO单点登录 Single Sign On 指在多系统应用群中登录一个系统，便可在其他所有系统中得到授权而无需再次登录 登录 注销 通信方式 sso-client 1、拦截子系统未登录用户请求，跳转至sso认证中心； 2、接收并存储sso认证中心发送的令牌； 3、与sso-server通信，校验令牌的有效性； 4、建立局部会话； 5、拦截用户注销请求，向sso认证中心发送注销请求； 6、接收sso认证中心发出的注销请求，销毁局部会话。 sso-server 认证中心 1、验证用户的登录信息； 2、创建全局会话； 3、创建授权令牌； 4、与sso-client通信发送令牌； 5、校验sso-client令牌有效性； 6、系统注册； 7、接收sso-client注销请求，注销所有会话。 中台 分类 业务中台 提供重用服务，例如用户中心、订单中心之类的开箱即用可重用能力，为战场提供了空军支援能力，随叫随到，威力强大 数据中台 提供数据分析能力，帮助从数据中学习改进，调整方向，为战场提供了海军支援能力； 算法中台 提供算法能力，帮助提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡； 技术中台 提供自建系统部分的技术支撑能力，帮助解决基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备； 研发中台 提供自建系统部分的管理和技术实践支撑能力，帮助快速搭建项目、管理进度、测试、持续集成、持续交付，是前台特种兵的训练基地； 组织中台 为项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方。]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[推荐系统-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Ftuijian-kg%2F%2F</url>
    <content type="text"><![CDATA[推荐系统 源数据 用途 豆瓣音乐推荐 购物推荐 好友推荐 app推荐 CF协同过滤 - 基于用户的推荐 - 基于物品的推荐 架构图 传统 亚马逊推荐架构 淘宝推荐架构 工具 上采样 对少的数据进行复制多份。 缺点： 训练出来的模型可能导致过拟合 下采样 对多的数据进行随机抽取，抽取后的数据量和负例保持一致。训练出来的模型可能导致欠拟合 最大似然估计 模型使用 1. 获取 通过训练得出的各个app之间的关联特征权重 2. 有各个用户的历史下载记录 3. 有各个应用的属性信息以及个属性的权重值 4. 可以取同个待选app与最近5个下载app 关联特征权重之和 得出来的矩阵 ，加上各个商品信息的权重矩阵，即为推荐列表矩阵 问题 冷启动 一般步骤 1. 需求分析 架构推荐方案 2. 数据清洗 得到训练数据]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fhbase-kg%2F%2F</url>
    <content type="text"><![CDATA[Hbase 谈资 特点 - 高可靠性 - 高性能 - 面向列 - 可伸缩 - 实时读写 - 分布式 底层用字节数组存储 与关系型数据库对比 关系型数据库的优点 - 容易理解 - 使用方便 - 易于维护 关系型数据库的瓶颈 - 高并发读写需求 - 海量数据的读写性能低 - 扩展性和可用性差 rowkey设计 案例 通话记录的存储与查询 rowkey:手机号+(Long.Maxvalue-时间戳) 微博案例 角色权限 部门子部门 原则 长度原则 定长 越小越好 2的幂次方 散列性 取反 hash 高位随机串 唯一性 保证key的唯一 根据实际业务来 protobuf 提供了自动生成java类的功能 java类，提供了对象的序列化和反序列化 存储模型 row key - 不能重复 - 字典顺序排序 - 只能存储64k的字节 列族 不大于3个 列名 以列族作为前缀 可动态加入 celll单元格 可动态加入 字节数组 时间戳 - 默认是1 - 时间倒序排序，最新最前 - 64位整型 - 默认精确到毫秒，可以主动设置 Hlog (wal log) HLogkey 数据归属信息 - table - region - sequence number - timestamp value - 就是hbase的 keyvale对象 API操作 查询 new Get(key) new Scan [startrow,endrow) table.get(List(Get)) filter PrefixFilter 前缀过滤器 KeyOnlyFilter 只返回行，不返回column值 InclusiveStopFilter 扫描到这一行停止 FirstKeyOnlyfilter 扫描到每一行的第一列返回 ColumnPrefixFilter 筛选出前缀匹配的列 ValueFilter SkipFilter ColumnCountFilter 每行最大返回多少列 SingleColumnFilter 插入 new Put(rowkey) table.put(List(Put))批量插入 删除 new Delete 架构 namespace client master - 负责负载均衡 - 发现失效region server 重新分配region 为region server 分配region 管理客户端对表单增加，删除，修改（DDL) Region server 维护region region 对应一张表 store 对应一个列族 memstore 溢写会生成stoefile 多个列簇任何一个溢写会触发本server上的所有memstore溢写，浪费IO zookeeper - 保证集群只有一个master - 存储 region的寻址入口 - 监控regionserver的上线下线信息，并通知master 存储了hbase的源数据信息 Hlog 优化 热点问题 rowkey设计有问题 对rowkey打撒 预分区 Column Family 某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O In Memory 创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中 MaxVersion time to Live 设置数据生命周期，过期自动删除 compact &amp; split minor compact major compact 二级索引 es+hbase 也可以做分页 写表操作 多HTable并发写 参数设置Auto Flush]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fjava-kg%2F%2F</url>
    <content type="text"><![CDATA[Java知识 数据类型 byte 1byte short 2byte int 4byte long 8byte char 2byte boolean 1byte float 4byte double 8btye 集合类 线程安全(Thread-safe) Vector Stack HashTable 无论是key还是value都不允许有null值的存在 Properties 持久的属性集 StringBuffer ConcurrentHashMap 用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问 1.7 分段锁segment 1.8 CAS 和synchronized CopyOnWriteArrayList 在修改时先复制一个快照来修改，改完再让内部指针指向新数组 因为对快照的修改对读操作来说不可见，所以只有写锁没有读锁，加上复制的昂贵成本，典型的适合读多写少的场景 如果更新频率较高，或数组较大时，还是Collections.synchronizedList(list)，对所有操作用同一把锁来保证线程安全更好 监听器 ConcurrentSkipListMap 并发优化的SortedMap 以SkipList结构实现 size（）同样不能随便调，会遍历来统计 ConcurrentSkipListSet 内部是ConcurrentSkipListMap的并发优化的SortedSet CopyOnWriteArraySet 内部是CopyOnWriteArrayList的并发优化的Set Queue BlockingQueue TransferQueue LinkedTransferQueue 确保一次传递完成 游戏服务器转发消息 SynchronousQueue 容量为0 ArrayBlockingQueue LinkedBlockingQueue 基于链表实现 PriorityBlockingQueue 也是基于数组存储的二叉堆 DelayQueue 执行定时任务 具有过期时间的缓存 多考生考试 Deque双向队列 ConcurrentLinkedDeque 基于链表，实现了依赖于CAS的无锁算法。 void push(E e):将给定元素”压入”栈中 E pop():将栈首元素删除并返回 操作 boolean offLast(E e)/offFirst(E e) 从队尾/首插入元素 E pollFirst()/pollLast() 移除对首/尾元素 E peekFirst()/peekLast() 查看队首/队尾元素 ConcurrentLinkedQueue 非阻塞式的 操作 poll():从队首删除并返回该元素 peek():返回队首元素，但是不删除 boolean offer(E e):将元素追加到队列末尾,若添加成功则返回true 遵循原则：FIFO(first input,first output)先进先出原则 非线程安全的集合 ArrayList 以数组实现 节约空间，但数组有容量限制 超出限制时会增加50%容量，用System.arraycopy()复制到新的数组 默认第一次插入元素时创建大小为10的数组 LinkedList 以双向链表实现 复杂度O(n/2) 既是List，也是Queue ArrayDeque 以循环数组实现的双向Queue HashMap 基于key hash查找Entry对象存放到数组的位置 在扩大容量时须要重新计算hash jdk1.8之前并发操作hashmap时为什么会有死循环的问题 hash碰撞 对于hash冲突采用链表的方式去解决 O(1)+O(n) 1.8之后超过默认阈值8就用 用红黑树替代链表 O(1)+O(logn) hash的实现 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16) 负载因子(默认0.75) 不保证数据有序 LinkedHashMap LinkedHashMap是Hash表和链表的实现 依靠着双向链表保证了迭代顺序是插入的顺序。 TreeMap 典型的基于红黑树的Map实现 保持key的大小顺序 读写复杂度log(n) 红黑树则没有好的无锁算法 HashSet 基于HashMap实现，无容量限制 不保证数据的有序； LinkedHashSet 基于HashMap和双向链表的实现 TreeSet 基于TreeMap实现的 利用TreeMap的特性，实现了set的有序性 内部是TreeMap的SortedSet StringBulider EnumMap 键为枚举类型的特殊的Map实现 所有的Key也必须是一种枚举类型 EnumMap是使用数组来实现的 SortedMap 支持基于CAS的无锁算法 PriorityQueue 应用：求 Top K 大/小 的元素 用平衡二叉最小堆实现的优先级队列，不再是FIFO 不再是FIFO，而是按元素实现的Comparable接口或传入Comparator的比较结果来出队 其iterator（）的返回不会排序 数值越小，优先级越高，越先出队 初始大小为11，空间不够时自动50%扩容 逻辑结构是一棵完全二叉树 存储结构其实是一个数组 小顶堆 WeakHashMap 弱引用map 就是Key键是一个弱引用的键，如果Key键被回收，则在get该map中值后，会自动remove掉value 如果Key键始终被强引用，则是无法被回收的 线程 线程的几个状态 分支主题 new runable running block destory 创建线程 继承Thread 实现Runable 实现Callable 线程安全 CAS 一条CPU并发原语 sun.misc.Unsafe 利用处理器的CMPXCHG 存在ABA问题 自旋时间可能过长 AQS CAS Compare and Swap（比较并交换 也叫非阻塞同步（Non-blocking Synchronization .需要读写的内存值 V 2.进行比较的值 A 3.拟写入的新值 B 锁 Lock ReentrantLock 默认是非公平锁 通过AQS的来实现线程调度 可重入锁 悲观锁 是个类，就比较灵活 设置等待时间，避免死锁 灵活实现多路通知 synchronized 一种非公平锁 可重入锁 自己重新获得已经获得的锁 独享锁 悲观锁 锁定的是对象 静态方法锁定的是Class对象 遇到异常自动释放锁 是个关键字 ReadWriteLock 允许读读 ReentrantReadWriteLock AQS 各种锁的定义 独享锁 / 共享锁 独享锁：该锁每一次只能被一个线程所持有 共享锁：该锁可被多个线程共有，典型的就是ReentrantReadWriteLock里的读锁，它的读锁是可以被共享的，但是它的写锁确每次只能被独占 通过AQS来实现 公平锁 / 非公平锁 公平锁是指多个线程按照申请锁的顺序来获取锁 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象 可重入锁 / 不可重入锁 可重入锁指的是可重复可递归调用的锁 与可重入锁相反，不可递归调用，递归调用就发生死锁 互斥锁 / 读写锁 加锁后，任何其他试图再次加锁的线程会被阻塞，直到当前进程解锁 读写锁既是互斥锁，又是共享锁，read模式是共享，write是互斥(排它锁)的 读写锁有三种状态：读加锁状态、写加锁状态和不加锁状态 乐观锁 / 悲观锁 悲观锁共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程 乐观锁 每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现 乐观锁适用于多读的应用类型，这样可以提高吞吐量 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 分段锁 分段锁其实是一种锁的设计，并不是具体的一种锁 对于1.7 ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作 偏向锁 / 轻量级锁 / 重量级锁 锁的状态 无锁状态 偏向锁状态 轻量级锁状态 重量级锁状态 四种状态都不是Java语言中的锁 自旋锁 spinlock 是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） 线程池 Executor execute ExecutorService Java 1.5 引入 ThreadPoolExecutor 可以自定义线程池 参数 corePoolSize 为线程池的基本大小。 maximumPoolSize 为线程池最大线程大小。 keepAliveTime 和 unit 则是线程空闲后的存活时间。 workQueue 用于存放任务的阻塞队列。 handler 当队列和最大线程池都满了之后的饱和策略。 Executors Factory and utility methods Executors. newSingleThreadExecutor(); 线程池就一个线程 保证任务执行前后顺序 Executors. newCachedThreadPool() 弹性 Executors. newFixedThreadPool(5) 固定个数 Executors.newScheduledThreadPool 定时执行线程池 ForkJoinPool 分而治之思想 Java 1.7 引入 最适合的是计算密集型的任务 Executors.newWorkStealingPool 根据cpu核数决定线程数 内部ForkJoinPool 1.8引入 Callable concurrent 有返回值 Runnable java.lang 无返回值 ThreadLocal 优点 充分利用cpu资源 避免了在处理短时间任务时创建与销毁线程的代价 目的 线程是稀缺资源，不能频繁的创建 解耦作用；线程的创建于执行完全分开，方便维护 应当将其放入一个池子中，可以给其他任务进行复用 执行流程 提交一个任务，线程池里存活的核心线程数小于线程数corePoolSize时，线程池会创建一个核心线程去处理提交的任务。 如果线程池核心线程数已满，即线程数已经等于corePoolSize，一个新提交的任务，会被放进任务队列workQueue排队等待执行。 当线程池里面存活的线程数已经等于corePoolSize了,并且任务队列workQueue也满，判断线程数是否达到maximumPoolSize，即最大线程数是否已满，如果没到达，创建一个非核心线程执行提交的任务。 如果当前的线程数达到了maximumPoolSize，还有新的任务过来的话，直接采用拒绝策略处理。 拒绝策略 AbortPolicy(抛出一个异常，默认的) DiscardPolicy(直接丢弃任务) DiscardOldestPolicy（丢弃队列里最老的任务，将当前这个任务继续提交给线程池） CallerRunsPolicy（交给线程池调用所在的线程进行处理) 内存可见性 Java内存模型（JMM，Java Memory Model） 每个线程都有自己独立的工作内存 里面保存该线程使用到的变量的副本 两条规定 所有的变量都存储在主内存中 线程对共享变量的所有操作都必须在自己的工作内中进行，不能直接从相互内存中读写 不同线程之间无法直接访问其他线程工作内存中的变量 线程间变量值得传递需要通过主内存来完成 共享变量可见性的实现原理 把工作内存1中更新过的共享变量刷新到主内存中 将主内存中最新的共享变量的值更新到工作内存2中 共享变量在线程间不可见的原因 线程的交叉执行 2&gt;重排序结合线程交叉执行 3&gt;共享变量更新后的值没有在工作内存与主内存间及时更新 Java语言层面支持的可见性实现方式 final也可以保证内存可见性 volatile 保证可见性 synchronized 可以实现互斥锁（原子性），即同步。但很多人都忽略其内存可见性这一特性 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 既保证可见性又保证原子性 一个线程对共享变量值的修改，能够及时的被其他线程看到 线程通信 wait/notify/sleep wait释放锁 notify/sleep不释放锁 CyclicBarrier CountDownLatch 管道通信 PipedWriter Atomic包 AtomicInteger 用户态 Semaphore 基于java同步器AQS 用来控制同时访问特定资源的线程数量，通过协调各个线程，以保证合理的使用资源 访问特定资源前，必须使用acquire方法获得许可，如果许可数量为0，该线程则一直阻塞，直到有可用许可 访问资源后，使用release释放许可 同步阻塞 同步非阻塞 异步阻塞 异步非阻塞 ThreadLocal 用途 保存线程上下文信息，在任意需要的地方可以获取 Spring的事务管理，用ThreadLocal存储Connection 线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失 ThreadLocal无法解决共享对象的更新问题 JVM JVM 内存结构 堆Heap 堆中存放对象（对象实例） 堆溢出(Out Of Memory error) OOM 的 8 种原因、及解决办法 Java 堆空间 造成原因 无法在 Java 堆中分配对象 吞吐量增加 应用程序无意中保存了对象引用，对象无法被 GC 回收 应用程序过度使用 finalizer。finalizer 对象不能被 GC 立刻回收。finalizer 由结束队列服务的守护线程调用，有时 finalizer 线程的处理能力无法跟上结束队列的增长 解决方案 使用 -Xmx 增加堆大小 修复应用程序中的内存泄漏 GC 开销超过限制 造成原因 java 进程98%的时间在进行垃圾回收，恢复了不到2%的堆空间，最后连续5个（编译时常量）垃圾回收一直如此 解决方案 使用 -Xmx 增加堆大小 使用 -XX:-UseGCOverheadLimit 取消 GC 开销限制 修复应用程序中的内存泄漏 请求的数组大小超过虚拟机限制 造成原因 应用程序试图分配一个超过堆大小的数组 解决方案 使用 -Xmx 增加堆大小 修复应用程序中分配巨大数组的 bug Perm gen 空间 造成原因 类的名字、字段、方法与类相关的对象数组和类型数组,JIT 编译器优化 当 Perm gen 空间用尽时，将抛出异常 解决方案 使用 -XX: MaxPermSize 增加 Permgen 大小 不重启应用部署应用程序可能会导致此问题。重启 JVM 解决 Metaspace 造成原因 从 Java 8 开始 Perm gen 改成了 Metaspace，在本机内存中分配 class 元数据（称为 metaspace）。如果 metaspace 耗尽，则抛出异常 解决方案 通过命令行设置 -XX: MaxMetaSpaceSize 增加 metaspace 大小 取消 -XX: maxmetsspacedize 减小 Java 堆大小,为 MetaSpace 提供更多的可用空间 为服务器分配更多的内存 可能是应用程序 bug，修复 bug 无法新建本机线程 造成原因 内存不足，无法创建新线程。由于线程在本机内存中创建，报告这个错误表明本机内存空间不足 解决方案 为机器分配更多的内存 减少 Java 堆空间 修复应用程序中的线程泄漏。 增加操作系统级别的限制 用户进程数增大 (-u) 1800 使用 -Xss 减小线程堆栈大小 杀死进程或子进程 造成原因 内核任务：内存不足结束器，在可用内存极低的情况下会杀死进程 解决方案 将进程迁移到不同的机器上 给机器增加更多内存 发生 stack_trace_with_native_method 造成原因 本机方法（native method）分配失败 打印的堆栈跟踪信息，最顶层的帧是本机方法 解决方案 使用操作系统本地工具进行诊断 堆划分 年轻代 Eden空间 From Survivor空间 To Survivor空间 8:1:1的比例 老年代 =堆空间大小-年轻代大空间大小 特点 存取速度较慢 堆的优势是可以动态地分配内存大小 线程共享 整个 Java 虚拟机只有一个堆，所有的线程都访问同一个堆 在虚拟机启动时创建 垃圾回收的主要场所 栈Stack 栈划分 java虚拟机栈 局部变量区 操作数栈 JVM栈（JVM Stacks） 线程私有的 生命周期与线程相同 栈帧 局部变量表 操作数栈 动态链接 方法出口信息 本地方法栈 用于方法的执行 Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一 栈中存放数据（变量 方法及对象的引用） 特点 数据可以共享 存放基本类型 存取速度比堆要快 栈溢出(Stack Overflow error) 如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常 当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常 出现 StackOverFlowError 时，内存空间可能还有很多。 压栈出栈过程 方法区(JDK1.7) 常量，静态变量 -XX:PermSize -XX:MaxPermSize 别名Non-Heap(非堆) 各个线程共享的内存区域 “永久代”（Permanent Generation 内存回收效率低 整个虚拟机中只有一个方法区 要回收目标是：对常量池的回收；对类型的卸载 运行时常量池 元数据区(JDK1.8) Metaspace -XX:MaxMetaspaceSize 使用本地内存 不在虚拟机中 程序计数器 Program Counter Register 一块较小的内存空间 线程私有，每条线程都有自己的程序计数器 生命周期：随着线程的创建而创建，随着线程的结束而销毁 是唯一一个不会出现OutOfMemoryError的内存区域 作用 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了 直接内存 Direct Memory（堆外内存 操作直接内存 直接内存的大小不受 Java 虚拟机控制，但既然是内存，当内存不足时就会抛出 OutOfMemoryError 异常 内存分配与回收策略 对象优先在 Eden 分配 Eden 区 from Survivor、to Survivor 区 老年代分配 当分配一个大对象时(大的数组，很长的字符串) 策略 静态 栈式 堆式 指针碰撞 空闲列表 内存管理 内存分配 内存回收 内存管理优化小技巧 1）尽量使用直接量，eg：String javaStr = &quot;小学徒的成长历程&quot;; 2）使用StringBuilder和StringBuffer进行字符串连接等操作; 3）尽早释放无用对象; 4）尽量少使用静态变量; 5）缓存常用的对象:可以使用开源的开源缓存实现，eg：OSCache，Ehcache; 6）尽量不使用finalize()方法; 7）在必要的时候可以考虑使用软引用SoftReference。 数组及其内存管理 数组变量存在栈区 数组对象存在堆内存 多维数组的本质是一维数组 堆内存on-heap 堆外内存off-heap 堆外缓存 Ehcache Chronical Map： OHC Ignite 堆外内存必须要由我们自己释放 虚拟机对象探秘 垃圾收集策略与算法 判定对象是否存活 引用计数法 很难解决对象之间的循环引用问题 可达性分析法 从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象 GC Roots 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈方法引用的对象； 强引用（Strong Reference） 软引用（Soft Reference） 弱引用（Weak Reference） 虚引用（Phantom Reference） 回收方法区内存 判定废弃常量 判定无用的类 该类的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例； 加载该类的 ClassLoader 已经被回收； 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 标记-清除算法 标记 标记出所有需要回收的对象 清除 在标记完成后统一回收掉所有被标记的对象 它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的 缺点 效率问题 空间问题 产生大量不连续的内存碎片 复制算法 内存分配时也就不用考虑内存碎片等复杂情况 代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低 标记-整理算法 Mark-Compact 分代收集算法 新生代 老年代 GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短 Generational Collection 垃圾收集器 CMS Concurrent Mark Sweep 优点：并发收集、低停顿 缺点 对 CPU 资源敏感（会和服务器抢资源）。 无法处理浮动垃圾。浮动垃圾是指 Java 业务代码与垃圾收集器并发执行过程中又产生的垃圾，这种垃圾只有等到下一次 GC 的时候再进行清理。 它使用的回收算法 “标记-清除” 算法会导致大量的内存空间碎片产生。 1.9 标记为废弃 基于“标记-清除”算法实现 过程 并发清除 重新标记 并发标记 初始标记 -XX:+UseConcMarkSweepGC 使用CMS收集器 G1 面向服务器应用端的垃圾收集器 针对多核 CPU 以及大容量内存的机器 运作步骤 初始标记(Init Marking，STW) 并发标记(Concurrent Marking) 最终标记(Remark，STW 筛选回收(Clearnup，STW) 19.之后是默认的 可预测停顿 这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型 空间整合 G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC Serial 收集器 1.3 单线程 Stop The World 新生代采用复制算法 老年代采用标记-整理算法 -XX:+UseSerialGC 串行收集器 Serial Old 收集器 ParNew 收集器 Serial 收集器的多线程版 新生代并行，老年代串行 新生代复制算法、老年代标记-压缩 -XX:+UseParNewGC ParNew收集器 -XX:ParallelGCThreads 限制线程数量 Parallel Scavenge 收集器 跟 ParNew 收集器一样 关注吞吐量 -XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 Parallel Old 收集器 是Parallel Scavenge收集器的老年代版本 使用多线程和“标记－整理”算法 -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 JVM 性能调优 使用 64 位 JDK 管理大内存 可能面临的问题 内存回收导致的长时间停顿； 现阶段，64位 JDK 的性能普遍比 32 位 JDK 低； 需要保证程序足够稳定，因为这种应用要是产生堆溢出几乎就无法产生堆转储快照（因为要产生超过 10GB 的 Dump 文件），哪怕产生了快照也几乎无法进行分析； 相同程序在 64 位 JDK 消耗的内存一般比 32 位 JDK 大，这是由于指针膨胀，以及数据类型对齐补白等因素导致的。 使用 32 位 JVM 建立逻辑集群 可能遇到的问题 尽量避免节点竞争全局资源，如磁盘竞争，各个节点如果同时访问某个磁盘文件的话，很可能导致 IO 异常； 很难高效利用资源池，如连接池，一般都是在节点建立自己独立的连接池，这样有可能导致一些节点池满了而另外一些节点仍有较多空余； 各个节点受到 32 位的内存限制； 大量使用本地缓存的应用，在逻辑集群中会造成较大的内存浪费，因为每个逻辑节点都有一份缓存，这时候可以考虑把本地缓存改成集中式缓存。 调优工具 jdk自带的工具 jdk自带的工具 VisualVM MAT Java heap分析工具 Eclipse Memory Analyzer jmap命令生产堆文件 GChisto 不再维护，不能识别最新jdk的日志文 gcviewer GC Easy web工具, http://gceasy.io Java GC 分析 命令动态查看 命令 jps JVM Process Status Tool 示指定系统内所有的HotSpot虚拟机进程 jstat JVM statistics Monitoring 监视虚拟机运行时状态信息的命令 显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据 jmap 用于生成heap dump文件 jhat JVM Heap Analysis Tool 与jmap搭配使用，用来分析jmap生成的dump jstack 用于生成java虚拟机当前时刻的线程快照 jinfo (JVM Configuration info) 实时查看和调整虚拟机运行参数 频繁GC的原因 人为原因 在代码中调用System#GC或者Runtime#GC方法。 框架原因 在java程序调用相关框架时，框架内部调用了GC方法 内存原因 当heap大小设置比较小时，会引起频繁的GC，所以在类似于Spark这样对内存性能要求比较高的应用程序运行时，应可能给heap分配较大的内存，这样可以减少频繁的GC现象的发生 其他原因 当构建的对象实例化十分频繁并且释放该对象也较为频繁时，同样会产生频繁GC现象 分析步骤 通过 top命令查看CPU情况，如果CPU比较高，则通过 top-Hp&lt;pid&gt;命令查看当前进程的各个线程运行情况，找出CPU过高的线程之后，将其线程id转换为十六进制的表现形式，然后在jstack日志中查看该线程主要在进行的工作。这里又分为两种情况 如果是正常的用户线程，则通过该线程的堆栈信息查看其具体是在哪处用户代码处运行比较消耗CPU； 如果该线程是 VMThread，则通过 jstat-gcutil&lt;pid&gt;&lt;period&gt;&lt;times&gt;命令监控当前系统的GC状况，然后通过 jmapdump:format=b,file=&lt;filepath&gt;&lt;pid&gt;导出系统当前的内存数据。导出之后将内存情况放到eclipse的mat工具中进行分析即可得出内存中主要是什么对象比较消耗内存，进而可以处理相关代码； 如果通过 top 命令看到CPU并不高，并且系统内存占用率也比较低。此时就可以考虑是否是由于另外三种情况导致的问题。具体的可以根据具体情况分析： 如果是接口调用比较耗时，并且是不定时出现，则可以通过压测的方式加大阻塞点出现的频率，从而通过 jstack查看堆栈信息，找到阻塞点； 如果是某个功能突然出现停滞的状况，这种情况也无法复现，此时可以通过多次导出 jstack日志的方式对比哪些用户线程是一直都处于等待状态，这些线程就是可能存在问题的线程； 如果通过 jstack可以查看到死锁状态，则可以检查产生死锁的两个线程的具体阻塞点，从而处理相应的问题。 类加载 类文件结构 魔数 头 4 个字节 魔数相当于文件后缀名，只不过后缀名容易被修改，不安全，因此在 Class 文件中标识文件类型比较合适 版本信息 4 个字节是版本信息 5-6 字节表示次版本号， 7-8 字节表示主版本号 常量池 字面值常量 、被 final 修饰的值 符号引用 类和接口的全限定名、字段的名字和描述符、方法的名字和描述符 访问标志 两个字节代 用于识别一些类或者接口层次的访问信息 这个 Class 是类还是接口；是否定义为 public 类型；是否被 abstract/final 修饰 类索引、父类索引、接口索引集合 Class 文件中由这三项数据来确定类的继承关系 字段表集合 存储本类涉及到的成员变量，包括实例变量和类变量，但不包括方法中的局部变量 方法表集合 与属性表类似 属性表集合 生命周期 加载 通过类的全限定名获取该类的二进制字节 加载.class文件的方式 从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在Java堆中生成一个代表这个类的 java.lang.Class对象，作为对方法区中这些数据的访问入口 验证 确保被加载的类的正确性 文件格式验证 元数据验证 字节码验证 符号引用验证 准备 为类的 静态变量分配内存，并将其初始化为默认值 数据类型默认的零值（如0、0L、null、false等 如果类字段的字段属性表中存在 ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值 解析 把类中的符号引用转换为直接引用 主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行 类的静态变量赋予正确的初始值 初始化 执行类构造器 &lt;clinit&gt;() 方法的过程 方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static {} 块）中的语句合并产生的 类初始化时机 创建类的实例，也就是new的方式 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（如 Class.forName(“com.shengsiyuan.Test”)） 初始化某个类的子类，则其父类也会被初始化 Java虚拟机启动时被标明为启动类的类（ JavaTest），直接使用 java.exe命令来运行某个主类 初始化步骤 1、假如这个类还没有被加载和连接，则程序先加载并连接该类 2、假如该类的直接父类还没有被初始化，则先初始化其直接父类 3、假如类中有初始化语句，则系统依次执行这些初始化语句 使用 卸载 结束生命周期 执行了 System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 类加载器 启动类加载器（Bootstrap ClassLoader） &lt;JAVA_HOME&gt;\lib 目录 扩展类加载器（Extension ClassLoader &lt;JAVA_HOME&gt;\lib\ext 应用程序类加载器（Application ClassLoader） classpath 自定义 ClassLoader 机制 全盘负责 当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托 先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 双亲委派模型 系统类防止内存中出现多份同样的字节码 保证Java程序安全稳定运行 缓存机制 缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 方式 命令行启动应用时候由JVM初始化加载 2、通过Class.forName()方法动态加载 3、通过ClassLoader.loadClass()方法动态加载 常用参数 -Xms -Xmx 堆 XX:NewSize 新生代 -Xss=256k 线程栈大小。 -XX:+PrintHeapAtGC 当发生 GC 时打印内存布局。 XX:+HeapDumpOnOutOfMemoryError 发送内存溢出时 dump 内存 -XX:PermSize设置永久代最小空间大小。1.8 无 gc 虚拟机分类 sun HotSpot 对象的内存布局 对象头（Header） 哈希码 GC 分代年龄 锁状态标志 线程持有的锁 偏向线程 ID 偏向时间戳 实例数据（Instance Data） 成员变量的值 父类成员变量和本类成员变量 对齐填充（Padding） 用于确保对象的总长度为 8 字节的整数倍 对象的创建过程 类加载检查 为新生对象分配内存 指针碰撞 空闲列表 初始化 为对象中的成员变量赋上初始值 设置对象头信息 调用对象的构造函数方法进行初始化 对象的访问方式 句柄访问方式 直接指针访问方式 IBM J9 对象 引用 强引用（Strong Reference） 当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误 软引用（Soft Reference） 内存空间足够，垃圾回收器就不会回收它 如果内存空间不足了，就会回收这些对象的内存 如何应用软引用避免OOM 弱引用（Weak Reference） 只具有弱引用的对象拥有更短暂的生命周期 虚引用（Phantom Reference） 虚引用主要用来跟踪对象被垃圾回收器回收的活动 变量 成员变量 类体内定义的变量 静态变量 局部变量 形参 方法内局部变量 代码块内局部变量 共享变量 如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的共享变量 对象头 对象的hashcode 对象的分代年龄 是否偏向锁的标识为 锁的标志位 新建对象的方式 new 反射 Object.clone 方法 反序列化 Unsafe.allocateInstance 方法 内存对齐 JDK各版本新特性 jdk1.5 自动装箱与拆箱 枚举 静态导入 可变参数 内省 泛型 For-Each循环 注解 协变返回类型 jdk1.6 AWT新增加了两个类:Desktop和SystemTray 使用JAXB2来实现对象与XML之间的映射 StAX，一种利用拉模式解析(pull-parsing)XML文档的API 使用Compiler API，动态编译Java源文件 轻量级Http Server API 插入式注解处理API 提供了Console类用以开发控制台程序 对脚本语言的支持如: ruby,groovy, javascript Common Annotations 嵌入式数据库 Derby JDK1.7 对Java集合（Collections）的增强支持，可直接采用[]、{}的形式存入对象 在Switch中可用String 数值可加下划线用作分隔符 支持二进制数字 简化了可变参数方法的调用 调用泛型类的构造方法时 Boolean类型反转 char类型的equals方法 安全的加减乘除 Map集合支持并发请求 JDK1.8 接口的默认方法 Lambda 表达式 替代匿名内部类 对集合进行迭代 实现map与reduce 与函数式接口Predicate配合 函数式接口 使用 :: 关键字来传递方法或者构造函数引用 多重注解 Optional 类 stream reduce sum integers.stream().reduce(Integer::sum) concat strs.stream().reduce(&quot;&quot;, String::concat); min integerStream.reduce(Integer.MAX_VALUE, Integer::min); integerStream1.mapToInt(i -&gt; i).min(); max integerStream.reduce(Integer.MIN_VALUE, Integer::max); integerStream1.mapToInt(i -&gt; i).max(); jdk1.9 Java 平台级模块系统 Linking JShell : 交互式 Java REPL 改进的 Javadoc 集合工厂方法 改进的 Stream API 私有接口方法 HTTP/2 多版本兼容 JAR G1是Java 9中的默认GC 轻量级的 JSON API 响应式流（Reactive Streams) API jdk1.10 局部变量类型推断 GC改进和内存管理 线程本地握手（JEP 312） 备用内存设备上的堆分配（JEP 316） 在 OpenJDK 中提供一组默认的根证书颁发机构证书 jdk1.11 HTTP 客户端（标准） ChaCha20 和 Poly1305 密码算法 低开销堆分析 传输层安全性（TLS）1.3 ZGC：可扩展的低延迟垃圾收集器 IO 相关概念 同步 A调用B，B的处理是同步的，在处理完之前他不会通知A，只有处理完之后才会明确的通知A 同步指的是被调用方做完事情之后再返回 异步 A调用B，B的处理是异步的，B在接到请求后先告诉A我已经接到请求了，然后异步去处理，处理完之后通过回调等方式再通知A 异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方 阻塞 A调用B，A一直等着B的返回，别的事情什么也不干 阻塞指的是调用方一直等待别的事情什么都不做 非阻塞 A调用B，A不用一直等着B的返回，先去忙别的事情了 非阻塞指的是调用方先去忙别的事情 阻塞、非阻塞说的是调用者 同步、异步说的是被调用者 IO模型 AIO异步非阻塞 Asynchronous IO 适用于连接数目多且连接比较长（重操作）的架构 相册服务器 编程比较复杂 JDK7开始支持 BIO同步阻塞 Blocking IO 并发处理能力低，通信耗时，依赖网速 模式简单，使用方便 以流的方式处理数据 数据的读取写入必须阻塞在一个线程内等待其完成 适用于连接数目比较小且固定的架构 NIO同步非阻塞 Non-Block IO 以块的方式处理数据 适用于连接数目多且连接比较短（轻操作）的架构 聊天服务器 编程比较复杂 组件 Channels Buffers capacity容量、 position位置、 limit限制 Selectors 多路复用器 IO多路复用 IO Multiplexing Selector Linux中的epoll socket 输入 InputStream FileInputStream 访问文件 PipedInputStream 访问管道 ByteArrayInputStream 访问数组 FilterInputStream BufferedInputStream 缓冲流 PushbackInputStream 推回输入流 DataInputStream 特殊流 ObjectInputStream 对象流 Reader FileReader CharArrayReader PipedReader StringReader BufferedReader InputStreamReader FilterReader PushbackReader 输出 OutputStream FileOutputStream ByteArrayOutputStream PipedOutputStream FilterOutputStream BufferedOutputStream PrintStream DataOutputStream ObjectOutputStream Writer FileWriter CharArrayWriter PipedWriter StringWriter BufferedWriter FilterWriter PrintWriter 框架 Netty 内存管理设计 内存划分成一个个16MB的Chunk 每个Chunk又由2048个8KB的Page组成 对每一次内存申请，都将二进制对齐 多个Chunk又可以组成一个ChunkList FastThreadLocal ThreadLocal在用法上面基本差不多 mina 序列化 谷歌Protobuf Kryo json FastJson xml hession thrift text bytes 动态代理 jdk动态代理 cglib动态代理 框架 spring 设计模式 简单工厂模式 又叫做静态工厂方法（StaticFactory Method）模式 不属于23种GOF设计模式 spring中的BeanFactory就是简单工厂模式的体现 工厂方法模式 单例模式 Spring下默认的bean均为singleton 可以通过singleton=“true|false” 或者 scope=&quot;?&quot;来指定 保证一个类仅有一个实例，并提供一个访问它的全局访问点 spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory 但没有从构造器级别去控制单例，这是因为spring管理的是是任意的java对象 适配器模式 包装器模式 一种是类名中含有Wrapper 另一种是类名中含有Decorator 基本上都是动态地给一个对象添加一些额外的职责 代理模式 pring的Proxy模式在aop中有体现 比如JdkDynamicAopProxy和Cglib2AopProxy Spring实现这一AOP功能的原理就使用代理模式 观察者模式 spring中Observer模式常用的地方是listener的实现。如ApplicationListener 策略模式 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换 模板方法模式 JdbcTemplate执行回调函数 Spring Boot 自动配置、起步依赖、Actuator、命令行界面(CLI) 启动流程 通过 SpringFactoriesLoader 查找并加载所有的 SpringApplicationRunListeners 创建并配置当前应用将要使用的 Environment ③Spring Boot 应用在启动时会输出这样的东西 ④根据是否是 Web 项目，来创建不同的 ApplicationContext 容器 ⑤创建一系列 FailureAnalyzer ⑥初始化 ApplicationContext。 ⑦调用 ApplicationContext 的 refresh() 方法，完成 IOC 容器可用的最后一道工序 ⑧查找当前 context 中是否注册有 CommandLineRunner 和 ApplicationRunner，如果有则遍历执行它们 ⑨执行所有 SpringApplicationRunListener 的 finished() 方法 核心功能 独立运行Spring项目 内嵌servlet容器 提供starter简化Maven配置 自动装配Spring 准生产的应用监控 无代码生产和xml配置 优缺点 优点 快速构建项目 对主流开发框架的无配置集成 项目可独立运行，无须外部依赖Servlet容器 极大的提高了开发、部署效率 与云计算的天然集成 缺点 如果你不认同spring框架，也许这就是缺点 CLI 控制台命令工具 常用的注解 @RestController @Controller 用于定义控制器类 负责将用户发来的URL请求转发到对应的服务接口（service层 @ResponseBody 表示该方法的返回结果直接写入HTTP response body中 标注控制层组件 @RequestMapping 提供路由信息 负责URL到Controller中的具体函数的映射 @SpringBootApplication @ComponentScan 组件扫描，可自动发现和装配一些Bean。 @EnableAutoConfiguration 自动配置。 @Configuration 等同于spring的XML配置文件；使用Java代码可以检查类型安全 @ImportResource 用来加载xml配置文件。 @Autowired 自动导入 byType方式 当加上（required=false）时，就算找不到bean也不报错 @Component 可配合CommandLineRunner使用，在程序启动后执行一些基础任务。 泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @PathVariabl 获取参数。 @JsonBackReference 解决嵌套外链问题。 @RepositoryRestResourcepublic 配合spring-boot-starter-data-rest使用。 @Import 用来导入其他配置类。 @Service 一般用于修饰service层的组件 @Repository 修饰的DAO或者repositories类 @Bean 用@Bean标注方法等价于XML中配置的bean。 放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理 @Value 注入Spring boot application.properties配置的属性的值 @Inject 等价于默认的@Autowired，只是没有required属性； @Qualifier 当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定 @Resource(name=”name”,type=”type”) 没有括号内内容的话，默认byName 与@Autowired干类似的事 Actuator 在应用程序里提供众多 Web 接口 通过它们了解应用程序运行时的内部状况 配置接口 度量接口 其它接口 IOC 用于模块解耦 正向控制 传统通过new的方式 反向控制 通过容器注入对象 DI 依赖注入，只关心资源使用，不关心资源来源 AOP 基本原理 动态代理 创建一个代理对象来代理原对象的行为 代理对象拥有原对象行为执行的控制权，在这种模式下，我们基于代理对象在原对象行为执行的前后插入代码来实现 AOP 相对于静态AOP更加灵活 切入的关注点需要实现接口。对系统有一点性能影响 静态织入 修改原对象，在原对象行为的执行前后注入代码来实现 AOP 行为注入 在编译期，切面直接以字节码的形式编译到目标字节码文件中 对系统无性能影响 注解的方式实现分布式锁 面向切面编程 性能监控 在方法调用前后记录调用时间，方法执行太长或超时报警 缓存代理 缓存某方法的返回值，下次执行该方法时，直接从缓存里获取 软件破解 使用AOP修改软件的验证类的判断逻辑 记录日志 在方法执行前后记录系统日志 工作流系统 工作流系统需要将业务代码和流程引擎代码混合在一起执行，那么我们可以使用AOP将其分离，并动态挂接业务 权限验证 方法执行前验证是否有权限执行当前方法，没有则抛出没有权限执行异常，由业务代码捕捉 相关注解 @Aspect 切面 @After 通知方法会在目标方法返回或抛出异常后调用 @AfterRetruening 通常方法会在目标方法返回后调用 @AfterThrowing 通知方法会在目标方法抛出异常后调用 @Around 通知方法将目标方法封装起来 @Before 通知方法会在目标方法执行之前执行 @Pointcut 切点 连接点（Joinpoint） 配置 cglib &lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt; JDK代理 &lt;aop:aspectj-autoproxy/&gt; 实现AOP的方式 经典的基于代理的AOP @AspectJ注解驱动的切面 纯POJO切面 注入式AspectJ切面 mvc 过程 （8）通过View返回给请求者（浏览器） （7）DispaterServlet把返回的Model传给View。 （6）ViewResolver会根据逻辑View查找实际的View。 （5）处理器处理完业务后，会返回一个ModelAndView对象，Model是返回的数据对象，View是个逻辑上的View。 （4）HandlerAdapter会根据Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 （3）解析到对应的Handler后，开始由HandlerAdapter适配器处理。 （2）DispatcherServlet根据请求信息调用HandlerMapping，解析请求对应的Handler。 （1）客户端（浏览器）发送请求，直接请求到DispatcherServlet。 统一异常处理 目标 消灭95%以上的 try catch 代码块，以优雅的 Assert(断言) 方式来校验业务的异常情况 关注业务逻辑，而不用花费大量精力写冗余的 try catch 代码块 原理 在独立的某个地方，比如单独一个类，定义一套对各种异常的处理机制，然后在类的签名加上注解@ControllerAdvice，统一对 不同阶段的、不同异常 进行处理 不同阶段的异常 进入Controller前的异常 handleServletException handleBindException 参数校验异常 handleValidException 参数校验异常 让404也抛出异常 spring.mvc.throw-exception-if-no-handler-found=true Service 层异常 handleBusinessException 处理自定义的业务异常 handleBaseException handleException 处理所有未知的异常，比如操作数据库失败的异常 实战 自定义异常 BaseException code message BusinessException 用 Assert(断言) 替换 throw exception BusinessExceptionAssert 将 Enum 和 Assert 结合 ResponseEnum code message 定义统一异常处理器类 @ControllerAdvice 统一返回结果 基类 BaseResponse code message 通用返回结果类 CommonResponse data 继承 BaseResponse 简写R new R&lt;&gt;(data) 数据带有分页信息 QueryDataResponse 继承自 CommonResponse 把 data 字段的类型限制为 QueryDdata QueryDdata 分页信息相应的字段 totalCount pageNo pageSize records 简写QR new QR&lt;&gt;(queryData) ErrorResponse 国际化 常用注解 @ControllerAdvice @ExceptionHandler @InitBinder @ModelAttribute @AllArgsConstructor @Getter @Slf4j @Component @Controller @PostConstruct 声明一个Bean对象初始化完成后执行的方法 @Value @Profile 加载指定配置文件时才起作用 @ConditionalOnExpression 特定条件下生效 Spring Session Interceptor拦截器 HandlerInterceptor HandlerInterceptorAdapter preHandle postHandle afterCompletion WebRequestInterceptor WebRequestInterceptor WebFlux REST Docs Spring Data JPA Java PersistenceAPI 查询语言是面向对象而非面向数据库的 Spring Data JPA始终需要JPA提供程序，如Hibernate或Eclipse Link redis REST Solr elastic search Neo4J Apache Hadoop spring的生命周期 首先容器启动后，对bean进行初始化 按照bean的定义，注入属性 检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给bean，如BeanNameAware等 以上步骤，bean对象已正确构造，通过实现BeanPostProcessor接口，可以再进行一些自定义方法处理。如:postProcessBeforeInitialzation。 BeanPostProcessor的前置处理完成后，可以实现postConstruct，afterPropertiesSet,init-method等方法， 增加我们自定义的逻辑， 通过实现BeanPostProcessor接口，进行postProcessAfterInitialzation后置处理 接着Bean准备好被使用啦。 容器关闭后，如果Bean实现了DisposableBean接口，则会回调该接口的destroy()方法 通过给destroy-method指定函数，就可以在bean销毁前执行指定的逻 mybatis 将 JSON 型字段映射到 Java 类 TypeHandler 设计模式 Builder模式 SqlSessionFactory的创建 适合查询多的场景，新增修改依然建议用orm，mybatis plus 支持orm Disruptor 线程间通信的高效低延时的内存消息组件 Shiro DelegatingFilterProxy的功能是通知Spring将所有的Filter交给ShiroFilter管理 与SpringMVC集成 配置前端过滤器 Dubbo Drools 规则引擎 MapStruct 类似深拷贝 使用纯java方法调用的源和目标对象之间的映射 通过生成代码完成繁琐和容易出错的代码逻辑 json fastjson jackson Jackson将null值转化为&quot;&quot; SPI JDBC JNDI JAXP 正则表达式 记录日志 使用slf4j 门面模式的日志框架 有利于维护和各个类的日志处理方式统一 实现方式统一使用: Logback框架 什么时候应该打日志 当你遇到问题的时候，只能通过debug功能来确定问题，你应该考虑打日志，良好的系统，是可以通过日志进行问题定为的。 当你碰到if…else 或者 switch这样的分支时，要在分支的首行打印日志，用来确定进入了哪个分支 经常以功能为核心进行开发，你应该在提交代码前，可以确定通过日志可以看到整个流程 基本格式 必须使用参数化信息的方式: logger.debug(&quot;Processing trade with id:[{}] and symbol : [{}] &quot;, id, symbol); 要进行字符串拼接,那样会产生很多String对象，占用空间，影响性能 对于debug日志，必须判断是否为debug级别后，才进行使用: 使用[]进行参数变量隔离 这样的格式写法，可读性更好，对于排查问题更有帮助。 不同级别的使用 ERROR 影响到程序正常运行 打开配置文件失败 所有第三方对接的异常(包括第三方返回错误码) 所有影响功能使用的异常，包括:SQLException和除了业务异常之外的所有异常(RuntimeException和Exception) WARN 不应该出现但是不影响程序 有容错机制的时候出现的错误情况 找不到配置文件，但是系统能自动创建配置文件 即将接近临界值的时候 缓存池占用达到警告线 业务异常的记录 当接口抛出业务异常时，应该记录此异常 INFO 系统运行信息 Service方法中对于系统/业务状态的变更 主要逻辑中的分步骤 外部接口部分 客户端请求参数(REST/WS) 调用第三方时的调用参数和调用结果 对于复杂的业务逻辑，需要进行日志打点，以及埋点记录 比如电商系统中的下订单逻辑，以及OrderAction操作(业务状态变更)。 对于整个系统的提供出的接口(REST/WS)，使用info记录入参 调用其他第三方服务时，所有的出参和入参是必须要记录的 ,job需要记录开始和结束 DEBUG 可以填写所有的想知道的相关信息 生产环境需要关闭DEBUG信息 如果在生产情况下需要开启DEBUG,需要使用开关进行管理，不能一直开启。 TRACE 特别详细的系统运行完成信息 业务代码中，不要使用.(除非有特殊用意，否则请使用DEBUG级别替代) 反射 异常 类层次结构图 Throwable Error（错误） 程序无法处理的错误 LinkageError NoClassDefFoundError VirtualMachineError Exception（异常） IOException RuntimeException NullPointerException ArithmeticException ArrayIndexOutOfBoundException IndexOutOfBoundsException unchecked exception（非检查异常） SQLException web filter 过滤器 基于回调函数实现，必须依靠容器支持 因为需要容器装配好整条FilterChain并逐个调用 容器 tomcat jetty 读写分离 领域模型 VO View Object 通常是请求处理层传输的对象 通过 Spring 框架的转换后，往往是一个 JSON 对象 BO Business Object 业务逻辑层封装业务逻辑的对象 聚合了多个数据源的复合对象 DO Data Object 与数据库表结构一一对应 通过 DAO 层向上传输数据源对象 DTO Data Transfer Object 远程调用对象 RPC 服务提供的领域模型 PO API rpc Hessian 基于HTTP协议 采用二进制编解码 protobuf-rpc-pro Protocol Buffers 协议的 基于 Netty 底层的 NIO 技术 Avro 支持HTTP，TCP两种协议 特点 任何语言 远程过程调用, 很简单的概念, 像调用本地服务(方法)一样调用服务器的服务(方法). RPC 框架需提供一种透明调用机制让使用者不必显式的区分本地调用和远程调用 目标 让构建分布式计算（应用）更容易 在提供强大的远程调用能力时不损失本地调用的语义简洁性 架构 客户端(Client)：服务调用方 客户端存根(Client Stub 存放服务端地址信息，将客户端的请求参数打包成网络消息，再通过网络发送给服务方 服务端存根(Server Stub) 接受客户端发送过来的消息并解包，再调用本地服务 服务端(Server)： 真正的服务提供者 分类 同步阻塞调用 WebService 底层使用http协议 RMI tcp java专属 java的原生序列化 客户方等待调用执行完成并返回结果 异步非阻塞调用 JMS(Java Message Service) 客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果 协议编解码 协议消息设计 消息头 magic : 协议魔数，为解码设计 header size: 协议头长度，为扩展设计 version : 协议版本，为兼容设计 st : 消息体序列化类型 hb : 心跳消息标记，为长连接传输层心跳设计 ow : 单向消息标记， rp : 响应消息标记，不置位默认是请求消息 status code: 响应消息状态码 reserved : 为字节对齐保留 message id : 消息 id body size : 消息体长度 消息体 采用序列化编码，常见有以下格式 xml : 如 webservie soap json : 如 JSON-RPC binary: 如 thrift; hession; kryo 等 框架 GRPC Thrift Dubbo HTTP RESTful API Representational State Transfer表述性状态传递 GET、PUT、DELETE、POST 原则 网络上的所有事物都被抽象为资源 每个资源都有一个唯一的资源标识符 同一个资源具有多种表现形式(xml,json等) 对资源的各种操作不会改变资源标识符 所有的操作都是无状态的 HTTP+URI+XML /JSON 的技术来实现 GraphQL SOAP Web Service 注解 @SafeVarargs Lambda]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fmechine-learning-kg%2F%2F</url>
    <content type="text"><![CDATA[机器学习 历史 1. Python R 第一代工具 单机 2. Mahaout MR 第二代工具 分布式 3. Spark MLlib 第三代工具 分布式 迭代 4. H2O 5. Flink 体现 - 计算：云计算 - 推理：专家系统 - 灵敏：事件驱动 - 知识：数据仓库 - 检索：搜索引擎 - 智慧：机器学习 用途 - 分类 - 预测 - 聚类 - 推荐 算法 有监督学习 分类 逻辑回归 支持多分类 线性不可分割情况 方案：映射至高纬 流程 svm支持向量机 只支持二分类 线性回归Liner Regression 一元线性回归 多元线性回归 预测 贝叶斯分类算法 朴素贝叶斯 拉普拉斯估计 拉普拉斯估计本质上是给频率表中的每个计数加上一个较小的数，这样就保证了 每一类中每个特征发生概率非零 文本向量化 用途 垃圾邮件分类 只能做二分类 推荐 关联规则 支持度 置信度 apriori原则 无监督 学习 聚类 给事物打标签 kmeans聚类算法 无监督 给事物打标签 刚开始选的点会影响到聚类结果 距离测度 - 欧氏距离测度(EuclideanDistanceMeasure) - 平方欧氏距离测度(SquaredEuclideanDistanceMeasure) - 曼哈顿距离测度(ManhattanDistanceMeasure - 余弦距离测度(CosineDistanceMeasure) - 谷本距离测度(TanimotoDistanceMeasure) 同时表现夹角和距离的距离测度 - 加权距离测度(WeightedDistanceMeasure 聚类数 肘部法 算法流程 1. 适当选择c个类的初始中心 2. 在第K次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到 距离最短的中心所在的类 3. 利用均值等方法更新该类的中心值 4. 对于多有的c个聚类中心，如果利用2,3的迭代法更新后，值保持不变，则 迭代结束，否则继续迭代 算法缺陷 - 聚类中心的个数K 需要事先给定，但在实际中这个 K 值的选定是非常 难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个 类别才最合适 - Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致 完全不同的聚类结果。(可以使用Kmeans++算法来解决) Kmeans++算法 工具 梯度下降法 SGD随机梯度下降 误差函数/损失函数 存在最小值 只能做二分类 L-BFGS拟牛顿法 L-BFGS为SGD的优化方法，它的训练速度比SGD快 还可能做多分类 鲁棒性调优 正则化 L1正则化 适合降低维度 惩罚系数 一般都不能大于1 有的趋近于1，有的趋近于0，稀疏编码 L2正则化 也称为岭回归，有很强的概率意义 整体值变小 整体的W同时变小，岭回归 数值优化 归一化 最大值最小值法 - 缺点 - 抗干扰能力弱 - 受离群值得影响比较大 - 间容易没有数据 方差归一化 - 优点 - 抗干扰能力强，和所有数据都有关, 求标准差需要所有值的介入，重要有离群值的话，会被抑 制下来 会使得各个W基本数量级一致 缺点 - 最终未必会落到0到1之间 - 同增同减问题 均值归一化 每个数量减去平均值 做机器学习的大公司 - 百度 - 谷歌 AlphaGo - 脸书 开发步骤 1. 收集数据 2. 准备输入数据 3. 分析输入数据 4. 训练算法 5. 测试算法 6. 使用算法 术语 标签 标签是我们要预测的事物，即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物 特征 特征是输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定 样本 样本是指数据的特定实例：x 有标签样本 有标签样本同时包含特征和标签 无标签样本 无标签样本包含特征，但不包含标签 模型 模型定义了特征与标签之间的关系 训练 创建或学习模型 推断 将训练后的模型应用于无标签样本 回归 回归模型可预测连续值 分类模型 分类模型可预测离散值]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Falgorithms-design-kg%2F%2F</url>
    <content type="text"><![CDATA[算法与数据结构 TopK问题 全局排序 局部排序 堆 分治法 减治法 随机选择 排序 选择排序 简单选择排序 Selection Sort 稳定性 不稳定 时间复杂度 O(n2) 工作原理 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾 堆排序Heap Sort 分支主题 交换排序 快速排序Quick Sort） 思想 通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序 冒泡排序 Bubble Sort 归并排序Merge Sort） 稳定性 稳定 时间复杂度 O(nlogn） 工作原理 采用分治法（Divide and Conquer 将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序 计数排序Counting Sort 稳定性 稳定 时间复杂度 O(n+k) 工作原理 采用分治法（Divide and Conquer 将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序 基数排序 桶排序 复杂度为O(n) 插入排序 简单插入排序 Insertion Sort 时间复杂度 O(n2) 稳定性 稳定 工作原理 通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入 希尔排序 缩小增量排序 稳定性 不稳定 时间复杂度 O(n1.3) 工作原理 数1问题 位移法 求与法 n&amp;(n-1) 斐波那契数列f(n) 递归法 正推法 通项公式法 f(n)=(1/√5){[(1+√5)/2]^n -[(1-√5)/2]^n} 查表法 时间复杂度 第一大类，简单规则 规则一：“有限次操作”的时间复杂度往往是O(1) 规则二：“for循环”的时间复杂度往往是O(n) 规则三：“树的高度”的时间复杂度往往是O(lg(n)) 二分查找的时间复杂度是O(lg(n)) 快速排序的时间复杂度n(lg(n)) 第二大类：组合规则 通过简单规则的时间复杂度，来求解组合规则的时间复杂度 第三大类，递归求解 数据结构 树 二叉树 堆 链表 广义表 线性表 哈希表 图 二叉树 数据一致性算法 paxos hhash]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark-知识图谱]]></title>
    <url>%2Fmindmap%2F%2Fknowledgegraph%2Fspark-kg%2F%2F</url>
    <content type="text"><![CDATA[spark 谈资 伯克利开源的 scala编写的 API Scala Python Java 运行模式 本地模式local 本地测试用 Standalone 任务提交方式 client提交方式 用于调试 1. client模式提交任务后，会在客户端启动Driver进程。 2. Driver会向Master申请启动Application启动的资源。 3. 资源申请成功，Driver端将task发送到worker端执行。 4. worker将task执行结果返回到Driver端。 cluster提交方式 1. cluster模式提交应用程序后，会向Master请求启动Driver. 2. Master接受请求，随机在集群一台节点启动Driver进程。 3. Driver启动后为当前的应用程序申请资源。 4. Driver端发送task到worker节点上执行。 5. worker将执行情况和执行结果返回给Driver端 yarn 任务提交方式 client 1. 客户端提交一个Application，在客户端启动一个Driver进程。 2. 应用程序启动后会向RS(ResourceManager)发送请求，启动 AM(ApplicationMaster)的资源。 3. RS收到请求，随机选择一台NM(NodeManager)启动AM。这 里的 NM 相当于 Standalone 中的 Worker 节点。 4. AM启动后，会向RS请求一批container资源，用于启动Executor. 5. RS会找到一批NM返回给AM,用于启动Executor。 6. AM会向NM发送命令启动Executor。 7. Executor启动后，会反向注册给Driver，Driver发送task到 Executor,执行情况和结果返回给 Driver 端。 cluster Mesos spark core DAG 有向无环图 RDD 弹性分布式数据集 5个特点 1. RDD是由一系列的partition组成的。 2. 函数是作用在每一个partition(split)上的。 3. RDD之间有一系列的依赖关系。 4. 分区器是作用在K,V格式的RDD上。 5. RDD提供一系列最佳的计算位置。 血统Lineage 代码流程 创建SparkConf对象 可以设置 Application name。 可以设置运行模式及资源需求。 创建SparkContext对象 基于Spark的上下文创建一个RDD，对RDD进行处理。 transformation转换 RDD &gt; RDD 缓存RDD，避免多次读取 应用程序中要有Action类算子来触发Transformation类算子执行。 action执行 RDD &gt; 结果 关闭Spark上下文对象SparkContext。 算子 输入算子 输出算子 转换算子Transformations map filter flapmap sample 判断是否数据倾斜 reduce reducebykey sortByKey/sortBy groupbykey join leftOuterjoin rightOuterJoin fullOuterJoin union intersection 取两个集合的交集 subtract 差集 mapPartition 执行次数与分区数相同 mapPartittionWithIndex distinct map+reducebykey+map cogroup repartition 增加或较少partition coalesce 减少分区 groupByKey zip 纵向合并两个rdd，没个数据一一对应 zipWithIndex 单个rdd 动作算子Action count collect first foreach take foreachPartition countByKey countByValue reduce 控制算子 cache 提高性能 persist checkpoint 解决容错 执行原理 当RDD的job执行完毕后，会从finalRDD从后往前回溯 当回溯到某一个RDD调用了checkpoint方法，会对当前的 RDD 做一个标记 Spark框架会自动启动一个新的job，重新计算这个RDD的数 据，将数据持久化到 HDFS 上 wide宽依赖 会有shuffle narrow窄依赖 stage stage 是由一组并行的 task 组成 pipeline 越多并行读越高 管道计算模式 广播变量 简介 优点 不用广播变量 累加器 全局计算 Shuffle HashShuffle 普通机制 每一个maptask将不同结果写到不同的buffer中，每个buffer 的大小为 32K。buffer 起到数据缓存的作用。 每个buffer文件最后对应一个磁盘小文件。 reduce task 来拉取对应的磁盘小文件。 产生的磁盘小文件的个数：M * R 合并机制 磁盘文件数： C* R SortShuffle 普通机制 磁盘文件数：2M bypass机制 磁盘文件数：2M 文件寻址 调优 BlockManager 内存管理 静态内存管理 统一内存管理 资源管理任务调度 spark粗粒度 MR细粒度 优点 ：集群资源能够充分利用 缺点：task自己申请资源导致启动变慢 术语 master standalone管理资源的主节点 cluster manager 在集群上获取外部资源的服务 worker node standalone 运行于从节点，管理从节点资源 application 基于spark的程序，包含了driver和executor worker executor 默认分配内存1G dirver Driver 负责应用程序资源的申请 任务的分发。 连接worker executor worker节点为application启动的一个进程 负责执行任务， 每个application都有独立的executor task 被送到executor上的工作单元 job task 的并行计算，相当于action stage job被拆分后的单元 pipline 优化 取top(N) 用手写排序，可以提高效率 webUi Master HA]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库-知识图谱]]></title>
    <url>%2Fmindmap%2Fknowledgegraph%2Fdatabase-kg%2F%2F</url>
    <content type="text"><![CDATA[数据库 连接池 第一代 c3p0 proxool dbcp BoneCP 第二代 HikariCP HikariCP MySQL 性能优化 引擎 InnoDb 支持事务 默认引擎 MyISAM 不支持事务 Memory 内存 Ndb 集群 arachive 事务 ACID A原子性 C完整性 I隔离性 D-持久性 分布式事务 TCC Try Confirm Cancel 2pc 锁 表锁 行锁 死锁 脏读 幻读 不可重复读 索引 类型 btree 平衡树 hash 约束 主键约束 外键约束 检查约束 唯一约束]]></content>
      <categories>
        <category>脑图</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[storm进阶之drpc]]></title>
    <url>%2Fbigdata%2Fstorm%E8%BF%9B%E9%98%B6%E4%B9%8Bdrpc%2F</url>
    <content type="text"><![CDATA[What? DRPC (Distributed RPC) – 分布式远程过程调用 DRPC 是通过一个 DRPC 服务端(DRPC server)来实现分布式 RPC 功能的。 &lt;!-- more --&gt; DRPC Server 负责接收 RPC 请求，并将该请求发送到 Storm中运行的 Topology， 等待接收 Topology 发送的处理结果，并将该结果返回给发送请求的客户端。 – (其实，从客户端的角度来说，DPRC 与普通的 RPC 调用并没有什么区别。) 客户端通过向 DRPC 服务器发送待执行函数的名称以及该函数的参数来获取处理结果。实现该函数的拓扑使用一个 DRPCSpout 从 DRPC 服务器中接收一个函数调用流。DRPC 服务器会为每个函数调用都标记了一个唯一的 id。随后拓扑会执 行函数来计算结果，并在拓扑的最后使用一个名为 ReturnResults 的 bolt 连接到 DRPC 服务器，根据函数调用的 id 来将函数 调用的结果返回。 DRPC设计目的 为了充分利用Storm的计算能力实现高密度的并行实时计算 (Storm接收若干个数据流输入，数据在Topology当中运行完成，然后通过DRPC将结果进 行输出。) 案例 Twitter 中某个URL的受众人数统计 Why? 为了充分利用Storm的计算能力实现高密度的并行实时计算 How? 运行模式 修改配置文件conf/storm.yaml 12drpc.servers:"node1“ 启动DRPC Server bin/storm drpc &gt; logs/drpc.out 2&gt;&amp;1 &amp; 通过StormSubmitter.submitTopology提交拓扑 参考]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>大数据</tag>
        <tag>drpc</tag>
        <tag>strom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息队列的高可用和幂等性以及数据丢失，顺序一致性[转]]]></title>
    <url>%2Fessay%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E5%92%8C%E5%B9%82%E7%AD%89%E6%80%A7%E4%BB%A5%E5%8F%8A%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%EF%BC%8C%E9%A1%BA%E5%BA%8F%E4%B8%80%E8%87%B4%E6%80%A7%2F</url>
    <content type="text"><![CDATA[如何保证消息队列的高可用和幂等性以及数据丢失，顺序一致性 &lt;!-- more --&gt; RabbitMQ的高可用性 RabbitMQ是比较有代表性的，因为是基于主从做高可用性的，我们就以他为例子讲解第一种MQ的高可用性怎么实现。 rabbitmq有三种模式： 单机模式 普通集群模式 镜像集群模式 单机模式 就是demo级别的，一般就是你本地启动了玩玩儿的，没人生产用单机模式 普通集群模式 意思就是在多台机器上启动多个rabbitmq实例，每个机器启动一个。但是你创建的queue，只会放在一个rabbtimq实例上，但是每个实例都同步queue的元数据。完了你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从queue所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个queue所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放queue的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让rabbitmq落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个queue拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性可言了，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。 镜像集群模式 这种模式，才是所谓的rabbitmq的高可用模式，跟普通集群模式不一样的是，你创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，然后每次你写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。这样的话，好处在于，你任何一个机器宕机了，没事儿，别的机器都可以用。坏处在于，第一，这个性能开销也太大了吧，消息同步所有机器，导致网络带宽压力和消耗很重！第二，这么玩儿，就没有扩展性可言了，如果某个queue负载很重，你加机器，新增的机器也包含了这个queue的所有数据，并没有办法线性扩展你的queue那么怎么开启这个镜像集群模式呢？我这里简单说一下，避免面试人家问你你不知道，其实很简单rabbitmq有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候可以要求数据同步到所有节点的，也可以要求就同步到指定数量的节点，然后你再次创建queue的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 kafka的高可用性 kafka一个最基本的架构认识：多个broker组成，每个broker是一个节点；你创建一个topic，这个topic可以划分为多个partition，每个partition可以存在于不同的broker上，每个partition就放一部分数据。 这就是天然的分布式消息队列，就是说一个topic的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上rabbitmq之类的，并不是分布式消息队列，他就是传统的消息队列，只不过提供了一些集群、HA的机制而已，因为无论怎么玩儿，rabbitmq一个queue的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个queue的完整数据。 kafka 0.8以前，是没有HA机制的，就是任何一个broker宕机了，那个broker上的partition就废了，没法写也没法读，没有什么高可用性可言。 kafka 0.8以后，提供了HA机制，就是replica副本机制。每个partition的数据都会同步到吉他机器上，形成自己的多个replica副本。然后所有replica会选举一个leader出来，那么生产和消费都跟这个leader打交道，然后其他replica就是follower。写的时候，leader会负责把数据同步到所有follower上去，读的时候就直接读leader上数据即可。只能读写leader？很简单，要是你可以随意读写每个follower，那么就要care数据一致性的问题，系统复杂度太高，很容易出问题。kafka会均匀的将一个partition的所有replica分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个broker宕机了，没事儿，那个broker上面的partition在其他机器上都有副本的，如果这上面有某个partition的leader，那么此时会重新选举一个新的leader出来，大家继续读写那个新的leader即可。这就有所谓的高可用性了。 写数据的时候，生产者就写leader，然后leader将数据落地写本地磁盘，接着其他follower自己主动从leader来pull数据。一旦所有follower同步好数据了，就会发送ack给leader，leader收到所有follower的ack之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）消费的时候，只会从leader去读，但是只有一个消息已经被所有follower都同步成功返回ack的时候，这个消息才会被消费者读到。 怎么保证消息队列消费的幂等性？ 先大概说一说可能会有哪些重复消费的问题。首先就是比如rabbitmq、rocketmq、kafka，都有可能会出现消费重复消费的问题，正常。因为这问题通常不是mq自己保证的，是给你保证的。然后我们挑一个kafka来举个例子，说说怎么重复消费吧。kafka实际上有个offset的概念，就是每个消息写进去，都有一个offset，代表他的序号，然后consumer消费了数据之后，每隔一段时间，会把自己消费过的消息的offset提交一下，代表我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的offset来继续消费吧。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接kill进程了，再重启。这会导致consumer有些消息处理了，但是没来得及提交offset，尴尬了。重启之后，少数消息会再次消费一次。其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。给你举个例子吧。假设你有个系统，消费一条往数据库里插入一条，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下已经消费过了，直接扔了，不就保留了一条数据？一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性幂等性，我通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧 比如你是写redis，那没问题了，反正每次都是set，天然幂等性 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 还有比如基于数据库的唯一键来保证重复数据不会重复插入多条，我们之前线上系统就有这个问题，就是拿到数据的时候，每次重启可能会有重复，因为kafka消费者还没来得及提交offset，重复数据拿到了以后我们插入的时候，因为有唯一键约束了，所以重复数据只会插入报错，不会导致数据库中出现脏数据 如何保证MQ的消费是幂等性的，需要结合具体的业务来看 数据丢失怎么办（如何保证消息的可靠性传输） 一、rabbitmq 生产者弄丢了数据 生产者将数据发送到rabbitmq的时候，可能数据就在半路给搞丢了，因为网络啥的问题，都有可能。 此时可以选择用rabbitmq提供的事务功能，就是生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，rabbitmq事务机制一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写rabbitmq的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 #####rabbitmq弄丢了数据 就是rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 消费端弄丢了数据 rabbitmq如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，rabbitmq认为你都消费了，这数据就丢了。这个时候得用rabbitmq提供的ack机制，简单来说，就是你关闭rabbitmq自动ack，可以通过一个api来调用就行，然后每次你自己代码里确保处理完的时候，再程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那rabbitmq就认为你还没处理完，这个时候rabbitmq会把这个消费分配给别的consumer去处理，消息是不会丢的。 二、kafka 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了 kafka弄丢了数据 这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了所以此时一般是要求起码设置如下4个参数：给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了我们生产环境就是按照上述要求配置的，这样配置之后，至少在kafka broker端就可以保证在leader所在broker发生故障，进行leader切换时，数据不会丢失 3）生产者会不会弄丢数据如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 数据的顺序性 rabbitmq保证数据的顺序性 如果存在多个消费者，那么就让每个消费者对应一个queue，然后把要发送 的数据全都放到一个queue，这样就能保证所有的数据只到达一个消费者从而保证每个数据到达数据库都是顺序的。 rabbitmq：拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理 kafka保证数据的顺序性 kafka 写入partion时指定一个key，列如订单id，那么消费者从partion中取出数据的时候肯定是有序的，当开启多个线程的时候可能导致数据不一致，这时候就需要内存队列，将相同的hash过的数据放在一个内存队列里，这样就能保证一条线程对应一个内存队列的数据写入数据库的时候顺序性的，从而可以开启多条线程对应多个内存队列（2）kafka：一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可 MQ积压几百万条数据怎么办？ 这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复consumer的问题，让他恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉 新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue 接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据 这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据 等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息 这里我们假设再来第二个坑 假设你用的是rabbitmq，rabbitmq是可以设置过期时间的，就是TTL，如果消息在queue中积压超过一定的时间就会被rabbitmq给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在mq里，而是大量的数据会直接搞丢。这个情况下，就不是说要增加consumer消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。也只能是这样了。假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次 然后我们再来假设第三个坑 如果走的方式是消息积压在mq里，那么如果你很长时间都没处理掉，此时导致mq都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。 作者：_云起 链接：https://www.jianshu.com/p/7a6deaba34d2 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT圈里难读的单词发音]]></title>
    <url>%2Fessay%2FIT%E5%9C%88%E9%87%8C%E9%9A%BE%E8%AF%BB%E7%9A%84%E5%8D%95%E8%AF%8D%E5%8F%91%E9%9F%B3%2F</url>
    <content type="text"><![CDATA[What? 记录遇到的难读的易读错的单词发音 &lt;!-- more --&gt; 单词 音标 发音 Youtube (You-tube [tju:b]) 念 优tiu啵 不念 优吐毙 Skype [ˈskaɪp] 念 死盖破 不念 死盖屁 Adobe [əˈdəʊbi] 念 阿兜笔 不念 阿斗伯 Chrome [krəʊm] 念 克肉姆 C# (C Sharp) 念 C煞破 GNU [(g)nuː] 念 哥怒 GUI [ˈɡui] 念 故意 JAVA [ˈdʒɑːvə] 念 扎蛙 不念 夹蛙 AJAX [ˈeɪdʒæks] 念 诶(ei)贾克斯 不念 阿贾克斯 Ubuntu [uˈbuntuː] 念 巫不恩兔 不念 友邦兔 Debian [ˈdɛbiən] 念 得(dei)变 Linux [ˈlɪnəks] [ˈlɪnʊks] 两种发音 丽娜克斯 和 李扭克斯 都可以 LaTeX [ˈleɪtɛk] [ˈleɪtɛx] [ˈlɑːtɛx] [ˈlɑːtɛk] 雷泰克，拉泰克 都可以 GNOME [ɡˈnoʊm] [noʊm] 两种发音 格弄姆 弄姆 都可以 App [ˈæp] 念阿破（与爱破也比较像，参见音标），不能把三个字母拆开念成A P P。 null [nʌl] 念 闹 jpg [ˈdʒeɪpɛɡ] 念 zhei派个 不念 勾屁记 WiFi [ˈwaɪfaɪ] 念 歪fai mobile [moˈbil] [ˈmoˌbil] [ˈməubail] 膜拜哦 和 牟bou 都可以 integer [ˈɪntɪdʒə] 念 音剃摺儿 不念 阴太阁儿 cache [kæʃ] 念 喀什 不念 卡尺 @ 念 at Tumblr (Tumbler) 念 贪不勒 或 汤不热 nginx (Engine X) 念 恩静 爱克斯（@Lawrence Li有不同意见） Apache [əˈpætʃiː] 念 阿趴气 Lucene [ˈluːsin] 念 鲁信 MySQL [maɪ ˌɛskjuːˈɛl] [maɪ ˈsiːkwəl] 念 买S奎儿 或 买吸扣 都可以 Exposé [ɛksˈpəʊzeɪ] 念 埃克斯剖Z （重音在Z上） RFID 【本条争议颇大】：有人念af rid, ri fid，但是RFID官方念法依然是四个字母分开读R F I D JSON (jason) 念 zhei森 Processing [ˈprəʊsesɪŋ] 重音在Pro上 avatar [ˌævə'tɑr] 念 艾瓦塌儿 solr ['səulə] 发音同 solar ['səulə]----馊了； selenium 美[sɪˈliniəm] Nacos [nɑ:kəʊs] yarn 英 [jɑːn] 美 [jɑn:n] 呀嗯 yum [jʌm] 亚目 phantom [ˈfæntəm; fanˈtəm] 法藤 Where? 推荐个网站 https://zh.forvo.com/login/ 专门发音的 Why? 太多单词不知道怎么读或读错了，特此记录下 参考 https://www.zhihu.com/question/19739907 https://github.com/shimohq/chinese-programmer-wrong-pronunciation?from=singlemessage&amp;isappinstalled=0]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>单词</tag>
        <tag>it</tag>
        <tag>单词发音</tag>
        <tag>易读错词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop体系所有组件默认端口列表]]></title>
    <url>%2Fessay%2FHadoop%E4%BD%93%E7%B3%BB%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Why? Hadoop集群组件太多，默认端口无法记住，有时候需要查看，就在这里罗列下 这里包含我们使用到的组件：HDFS, YARN, Hbase, Hive, ZooKeeper。 &lt;!-- more --&gt; What? 端口 作用 9000 fs.defaultFS，如：hdfs://172.25.40.171:9000 9001 dfs.namenode.rpc-address，DataNode会连接这个端口 50070 dfs.namenode.http-address 50470 dfs.namenode.https-address 50100 dfs.namenode.backup.address 50105 dfs.namenode.backup.http-address 50090 dfs.namenode.secondary.http-address，如：172.25.39.166:50090 50091 dfs.namenode.secondary.https-address，如：172.25.39.166:50091 50020 dfs.datanode.ipc.address 50075 dfs.datanode.http.address 50475 dfs.datanode.https.address 50010 dfs.datanode.address，DataNode的数据传输端口 8480 dfs.journalnode.rpc-address 8481 dfs.journalnode.https-address 8032 yarn.resourcemanager.address 8088 yarn.resourcemanager.webapp.address，YARN的http端口 8090 yarn.resourcemanager.webapp.https.address 8030 yarn.resourcemanager.scheduler.address 8031 yarn.resourcemanager.resource-tracker.address 8033 yarn.resourcemanager.admin.address 8042 yarn.nodemanager.webapp.address 8040 yarn.nodemanager.localizer.address 8188 yarn.timeline-service.webapp.address 10020 mapreduce.jobhistory.address 19888 mapreduce.jobhistory.webapp.address 2888 ZooKeeper，如果是Leader，用来监听Follower的连接 3888 ZooKeeper，用于Leader选举 2181 ZooKeeper，用来监听客户端的连接 60010 hbase.master.info.port，HMaster的http端口 60000 hbase.master.port，HMaster的RPC端口 60030 hbase.regionserver.info.port，HRegionServer的http端口 60020 hbase.regionserver.port，HRegionServer的RPC端口 8080 hbase.rest.port，HBase REST server的端口 10000 hive.server2.thrift.port 9083 hive.metastore.uris]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac+idea+gradle搭建Hadoop开发环境]]></title>
    <url>%2Fessay%2Fmac%E4%B8%8Bidea%E6%90%AD%E5%BB%BAHadoop%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[What? 使用mac os 系统，不用eclipse，不用maven，而是使用idea 和 gradle 来搭建开发环境 &lt;!-- more --&gt; How? mac 上安装gradle 1brew install gradle mac 上安装 idea 1brew cask install intellij-idea 利用idea 创建gradle项目 修改gradle配置文件 引入依赖 ![](http://wntc-1251220317.cossh.myqcloud.com/2018/11/16/1542338465013.png) 将hadoop上的配置文件拷贝到项目resource目录,主要用于打jar包放到服务器上执行任务 新建HDFSOperations 类 练习hdfs的基本操作 注意 System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;); 加上这个环境变量可避免权限问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class HDFSOperations &#123; static Configuration config ; static FileSystem fileSystem; static String resourcePath ; static String pathhdfsStr ; public static void main(String[] args) throws IOException &#123; // 不加此行 本地运行则可能会报权限问题 System.setProperty("HADOOP_USER_NAME", "root"); //1. 初始化Configuration config = new Configuration(); // 提交jar包到服务器时 需把这里fs.defaultFS注释掉 本地运行则放开 config.set("fs.defaultFS","hdfs://sj-node1:8020"); fileSystem = FileSystem.get(config); pathhdfsStr= "/testoperation"; resourcePath=System.getProperty("user.dir")+"/src/main/resources"; mkdir(pathhdfsStr); uploadfiles(); // listFiles(); &#125; public static boolean mkdir(String pathstr) throws IOException &#123; Path path = new Path(pathstr); return fileSystem.mkdirs(path); &#125; public static void uploadfiles() throws IOException &#123; File file = new File(resourcePath+"/qq.txt"); Path path = new Path(pathhdfsStr+"/qq.txt"); FSDataOutputStream fsDataOutputStream= fileSystem.create(path); IOUtils.copyBytes(new FileInputStream(file),fsDataOutputStream,config); System.out.println("上传结束"); &#125; public static void listFiles() throws IOException &#123; Path path = new Path("/"); RemoteIterator&lt;LocatedFileStatus&gt; iterator = fileSystem.listFiles(path, true); while (iterator.hasNext()) &#123; LocatedFileStatus status = iterator.next(); System.out.println(status.getPath().getName()); &#125; &#125;&#125; 执行没有报错即说明基本的操作环境就完成了 集群配置 Server1 server2 server3 server4 Nginx √ Tomcat √ √ √ HDFS NN NN&amp;DN DN DN Zookeeper √ √ ZKFC √ √ quorum √ √ √ Yarn √ √ √ Junoral node √ √ √ Mysql √ Hive 本地模式 server client Hbase √ √ backup master docker hadoop 集群 docker pull kiwenlau/hadoop:1.0 git clone https://github.com/kiwenlau/hadoop-cluster-docker 网络 运行容器 12cd hadoop-cluster-docker./start-container.sh 启动了3个容器，1个master, 2个slave 运行后就进入了hadoop-master容器的/root目录 启动hadoop集群 ./start-hadoop.sh 网页访问 http://localhost:50070 http://localhost:8088/cluster 参考 https://hk.saowen.com/a/07f5cebeee97ceddbe41fb4d27b0f2def7b2be636e37b187a060c618071eb77e https://github.com/trex-group/Big-Data/issues/19 https://www.jianshu.com/p/b75f8bc9346d https://blog.csdn.net/u013063153/article/details/62040128 Intellij结合Maven本地运行和调试MapReduce程序 以本地方式运行mapreduce程序的参数配置 hadoop HA场景下 java客户端远程访问hdfs配置 从 0 开始使用 Docker 快速搭建 Hadoop 集群环境]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E5%B8%88%E7%AC%94%E8%AE%B0%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[What? 单台服务器有性能等瓶颈，请求被合理分配到多台server上，以使能够处理更多请求 nginx 集群 lvs keepalived f5 nginx 官方测试并发支承5万并发 阿里开源的tengine 和nginx能够完美兼容 nignx负载均衡机制 轮训（默认） 按照配置的服务列表一个个分配 缺点 同一个用户，不同请求会分配到不同服务器 加权负载均衡 对性能不一致的机器 给予不同的权重，是性能好的服务器能够处理更多 最少连接数 分配给连接数更少的服务器 ip-hash 保证会话持久 - 缺点 - 如果针对用户是企业用户，一般都是一条宽带外网ip都是一样的，这样的用户都会被分配到同一个server上，达不到负载均衡的效果 Where? 应用服务器前可以加nignx负载，如果nginx需要负载则需要在nginx前加lvs服务 参考 lvs Why? How? 轮训nginx配置 1234567891011http &#123; upstream sjcluster&#123; server sj-node2; server sj-node3; &#125; server &#123; listen 80; server_name node2; location / &#123; proxy_pass http://sjcluster &#125; 权重配置 1234567891011http &#123; upstream sjcluster&#123; server sj-node2 weight=3; server sj-node3; &#125; server &#123; listen 80; server_name node2; location / &#123; proxy_pass http://sjcluster &#125; 最小连接数配置 123456789101112http &#123; upstream sjcluster&#123; least_conn; server sj-node2; server sj-node3; &#125; server &#123; listen 80; server_name node2; location / &#123; proxy_pass http://sjcluster &#125; ip-hash配置 123456789101112http &#123; upstream sjcluster&#123; ip_hash; server sj-node2 ; server sj-node3; &#125; server &#123; listen 80; server_name node2; location / &#123; proxy_pass http://sjcluster &#125; 参考]]></content>
      <categories>
        <category>架构师笔记</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[workflow实现csv2json]]></title>
    <url>%2Fessay%2Fworkflow%E5%AE%9E%E7%8E%B0csv2json%2F</url>
    <content type="text"><![CDATA[What? 将csv文件内容按照要求转换成json格式 下载地址 Where? 写博客的时候用到图表，需要json格式的原始数据 Why? 本博客中的 知识图谱 和 书单 是长期维护的，采用的是百度开源的图表框架echarts，配合hexo的插件hexo-tag-echarts3，需要的是json格式的原始数据，随着年份的增加，内容也会越来越多，因其都有一定的结构，不是简单的列表，所以采用xmind脑图软件记录，修改脑图软件后导出格式如csv，xls等，这里选择csv格式转json，比较方便点 How? 主要使用python脚本，方便调试，源码地址：https://github.com/juforg/csv2json.alfredworkflow 使用codecs库读取csv文件，避免乱码 解析csv数据 组装json数据 输出到剪贴板 applescript 获取当前文件夹路径 123456on alfred_script(q) tell application &quot;Finder&quot; set query to get POSIX path of (folder of the front window as alias) return query end tellend alfred_script applescript 获取选中的文件全路径 123456on alfred_script(q) tell application &quot;Finder&quot; set query to get POSIX path of (item 1 of (get selection as alias list)as alias) return query end tellend alfred_script 参考 https://blog.csdn.net/jenyzhang/article/details/51898150?utm_source=blogxgwz2 https://forum.keyboardmaestro.com/t/getting-the-path-of-currently-selected-file-in-finder/1507/2 https://stackoverflow.com/questions/12129989/getting-finders-current-directory-in-applescript-stored-as-application https://gist.github.com/jonschlinkert/7683131911c0cfd18d5cf8e818adffbc]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alfred-workflow开发]]></title>
    <url>%2Fessay%2Falfred-workflow%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[知识准备 Workflow 支持 Triggers、Inputs、Actions、Utilities,Outputs 共 5 项主要功能 输入包含 Triggers（触发器）和 Inputs（输入触发）； Triggers 中的流程可以触发 Inputs 的流程，反之则不行，同时它们都可以触发其它后续流程。 输出即 Outputs，包含了通知，放大展示、复制到剪切板，写入文本、播放声音、触发其它流程等。 中间 Actions 包含打开文件、在 finder 中展示文件、唤起 app、打开 web search、打开 URL、执行系统命令、执行 iTunes 命令、执行脚本、执行 AppleScript 脚本、在终端中执行命令等。 Utilities 包含了一些公共组件，如变量设置、json 配置、过滤、转换、替换、延时、debug 等。 需求 无论用哪个工具截图后，在剪贴板中都有这个图片的二进制信息，把这个二进制信息自动上传到各大图床平台上 包括但不限于： 阿里云oss 七牛云 腾讯云cos 坚果云 百度云 进阶： 在finder中选中文件，也可把该文件上传到云上 剪贴板中url图片 自动上传到云上 快捷键及使用 tc [oss/qn/ts] wntc [oss/qn/ts] wn [oss/qn/ts] 默认不写参数 上传到所有能够上传的云上并返回优先级最高的那个云的地址 可以填写指定图床的 缩写，只上传到该图床并只返回该图床url 开发 安装操纵macos 剪贴板所需包 sudo pip install PyObjC --user 腾讯云cos 安装官方sdk 下载cos-python-sdk-v5 到workflow目录下 登录腾讯云创建存储桶 在腾讯云控制台创建子账号，使该账号具有存储桶的读写权限 复制SecretI和SecretKey填写到workflow的变量中 阿里云oss 直接安装官方sdk pip install oss2 程序示意图 看到了一个软件https://toolinbox.net/iPic/ 可惜要收费 万能图床源码地址： https://github.com/juforg/wntc.alfredworkflow 参考 https://sspai.com/post/44624 https://github.com/tiann/markdown-img-upload https://blog.csdn.net/u014600626/article/details/53635192 https://blog.csdn.net/linjf520/article/details/52225007 https://www.cnblogs.com/tina-python/p/5508402.html 可能遇到的错误 1 123456789[2018-07-31 11:42:33][ERROR: action.script] Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt; File &quot;/Library/Python/2.7/site-packages/qcloud_cos/__init__.py&quot;, line 1, in &lt;module&gt; from .cos_client import CosS3Client File &quot;/Library/Python/2.7/site-packages/qcloud_cos/cos_client.py&quot;, line 21, in &lt;module&gt; from .cos_threadpool import SimpleThreadPool File &quot;/Library/Python/2.7/site-packages/qcloud_cos/cos_threadpool.py&quot;, line 5, in &lt;module&gt; from six.moves.queue import QueueImportError: No module named queue 注意]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习笔记]]></title>
    <url>%2Fpython%2Fpython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[What Python是一种面向对象的解释型计算机程序设计语言。1989年发明， 特点： 解释型 面向对象 简单易读 跨平台 开发准备 安装 python 配置环境变量 安装开发工具 pycharm 123brew install pyenvbrew install pyenv-virtualenvbrew cask install rabbitmq python版本 显示可安装python版本 pyenv install -l 安装指定Python版本 pyenv install 3.7.2 卸载指定python版本 pyenv uninstall 3.7.2 显示已安装python版本 包括虚拟环境 pyenv versions 设置当前目录python版本 pyenv local 3.7.2 设置当前会话python版本 pyenv shell 3.7.2 重置版本设置 12pyenv shell --unsetpyenv local --unset 虚拟环境 创建某个Python版本的虚拟环境 pyenv virtualenv 3.7.2 wb-venv 激活和停用虚拟环境： pyenv activate wb-venv pyenv deactivate 列出当前所有的虚拟环境： pyenv virtualenvs 删除虚拟环境： pyenv virtualenv-delete wb-venv 数据基本类型 字符串(String) 数字型(Numbers)包括四种 (整形(int) 长整形(L) 浮点型(float) 复数(j) 布尔 元组(Tuple) 定长、内容不可变、元素类型多样 列表(List) 队列结构、可变长度、元素的数据类型多样 字典(Dictionary) 键值对结构、key唯一，数据类型多样、valuey数据类型多样 命名规范 https://www.python.org/dev/peps/pep-0008/ 命名类型 说明 例子 模块名 小写字母，单词之间用_分割 ad_stats.py (业务类型域_操作域_周期域) 包名 小写字母，单词之间用_分割 sys_user 类名 驼峰 AdStats、ConfigUtil 全局变量名(类变量，在 java 中相当于 static 变量 大写字母，单词之间用_分割 NUMBER、COLOR_WRITE 普通变量 小写字母，单词之间用_分割 this_is_a_var 实例变量 以_开头，其他和普通变量一样 _price、_instance_var 私有实例变量(外部访问会报错) 以__开头(2 个下划线)，其他和普通变量一样 __private_var 专有变量 __开头，__结尾，一般为 python 的自有变量，不要以这种方式 __doc__、__class__ 普通函数 小写字母，单词之间用_分割 get_name()、count_number()、 ad_stat() 私有函数(外部访问会报错) 以__开头(2 个下划线)，其他 和普通函数一样 __get_name() 以双下划线开头的（__foo）代表类的私有成员； 以双下划线开头和结尾的（foo）代表python里特殊方法专用的标识，如__init__（）代表类的构造函数。 变量名、包名、模块名 变量名通常由字母、数字、下划线组成，以字母或下划线开头，包名、模块名通常采用简短的小写字母，提高可读性模块名中也可以包含下划线，包命中不推荐使用下划线。 类名、对象名 类名首字母大写，其他字母采用小写。对象名用小写字母。内部使用的类在类名前加单下划线。 函数名 函数名通常采用小写，必要时使用下划线区分单词以提高可读性。 异常名 因为异常也是类，所以类的命名习惯在这里也适用。不同的是，如果异常实际上是个错误，则需要在异常名字的后面使用Error后缀 全局变量名 我们假设这些变量都是在模块内部使用的。命名所遵循的规则跟函数的命名规则基本相同。 Python之父Guido推荐的规范 编码 如无特殊情况, 文件一律使用 UTF-8 编码 python2 如无特殊情况, 文件头部必须加入#-*-coding:utf-8-*-标识 语法 python 字符串拼接 普通拼接 “str ” + var 性能低 占位符有序传参 website = '%s%s%s' % ('python', 'tab', '.com') key-value传参 不考虑顺序 取子字符串 strr = &quot;testsubstr&quot; 取下标1的字符 strr[1] 取下标1开始到3结束 左闭右开 取下标0到2 左闭右开 strr[:2] 取下标2开始到结束 strr[2:] dict字典转json数据 j = json.dumps(dict) 获取变量类型 type(varname) 迭代元祖 12for x in tup: print "元素："+ str(x) Python Number 类型转换 表达式 说明 int(x [,base ]) 将 x 转换为一个整数 long(x [,base ]) 将 x 转换为一个长整数 float(x ) 将 x 转换到一个浮点数 complex(real [,imag ]) 创建一个复数 str(x ) 将对象 x 转换为字符串 repr(x ) 将对象 x 转换为表达式字符串 eval(str ) 用来计算在字符串中的有效 Python 表达式,并返回一个对象 tuple(s ) 将序列 s 转换为一个元组 list(s ) 将序列 s 转换为一个列表 chr(x ) 将一个整数转换为一个字符 unichr(x ) 将一个整数转换为 Unicode 字符 ord(x ) 将一个字符转换为它的整数值 hex(x ) 将一个整数转换为一个十六进制字符串 oct(x ) 将一个整数转换为一个八进制字符串 Python 数学函数 表达式 说明 abs(x) ceil(x) cmp(x, y) 如果 x &lt; y 返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1 exp(x) 返回e的x次幂 fabs(x) float的绝对值 floor(x) 返回下舍整数 log(x) log10(x) 10的x对数 max(x1, x2,...) 返回给定参数的最大值，参数可以为序列。 min(x1, x2,...) 返回给定参数的最小值，参数可以为序列。 modf(x) 回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表 示。 pow(x, y) x**y 运算后的值。 round(x[,n]) 返回浮点数x的四舍五入值，如给出n值，则代表舍入到小数点后的位数 sqrt(x) 返回数字 x 的平方根 Python 随机数函数 表达式 说明 choice(seq) 从序列的元素中随机挑选一个元素，比如 random.choice(range(10))，从 0 到 9中随机挑选一个整数。 randrange ([start,] stop [,step]) 从指定范围内，按指定基数递增的集合中获取一个随机数， 基数缺省值为 1 random() 随机生成下一个实数，它在[0,1)范围内。 seed([x]) 改变随机数生成器的种子 seed。如果你不了解其原理，你不必特别去设定 seed Python 会帮你选择 seed。 shuffle(lst) 将序列的所有元素随机排序 uniform(x,y) 随机生成下一个实数，它在[x,y]范围内。 函数式编程 变量作用域 1global varname 方法体内使用 关键字global 可使用全局变量，否则使用的是局部变量 匿名函数 异常处理 123456try: 正常的操作except 异常名称, 数据: 发生以上多个异常中的一个，执行这块代码else: 如果没有异常执行这块代码 finally 123456try: print list[1]except(TypeError, IndexError), message: print "异常来了", messagefinally: print "执行finally" 自定义异常 1234567891011121314151617class ParamException(RuntimeError): def __init__(self, *args): self.args = argsdef check_params(age): if (age &lt; 15): raise ParamException("年龄不能小于15")def register(age): try: check_params(age) except ParamException,error: print error 文件操作 获取控制台内容 raw_input()函数 access_mode:access_mode 决定了打开文件的模式 'r':读 'w':写 'a':追加 'r+' == r+w(可读可写，放在指针开头，文件若不存在就报错(IOError) 'w+' == w+r(可读可写，文件若不存在就创建) 'a+' ==a+r(可追加可写，文件若不存在就创建) 对应的，如果是二进制文件，就都加一个 b 就好啦: 'rb' 'wb' 'ab' 'rb+' 'wb+' 'ab+' 中文utf-8 一个字符3个字节，其它一个字节 判断空 pandas df = df[(df['file_name'].isnull() | (df['file_name'] == ''))] 面向对象 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每 个对象所共有的属性和方法。对象是类的实例。 类变量:类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员:类变量或者实例变量用于处理类及其实例对象的相关的数据。 方法重写:如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖(override)，也称为方法的重写。 实例变量:定义在方法中的变量，只作用于当前实例的类。 继承:即一个派生类(derived class)继承基类(base class)的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计:一 个 Dog 类型的对象派生自 Animal 类，这是模拟&quot;是一个(is-a)&quot;关系(例图，Dog 是一个 Animal)。 实例化:创建一个类的实例，类的具体对象。 方法:类中定义的函数。 对象:通过类定义的数据结构实例。对象包括两个数据成员(类变量和实例变量)和方法。 多继承 同方法名，写后面的方法生效 Where https://www.python.org/ 参考 https://blog.csdn.net/youngbit007/article/details/61616241 https://www.cnblogs.com/lclq/p/5542902.html https://blog.csdn.net/warm77/article/details/78353632 https://blog.csdn.net/xun527/article/details/76180493 常用的相似度计算方法原理及实现 https://blog.csdn.net/weixin_38225797/article/details/81021691 https://blog.csdn.net/guojingjuan/article/details/50396254 https://mp.weixin.qq.com/s/WsUAtZhhaYo5R-oXOubvjg]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从jekyll转向hexo]]></title>
    <url>%2Fessay%2F%E4%BB%8Ejekyll%E8%BD%AC%E5%90%91hexo%2F</url>
    <content type="text"><![CDATA[当年选择jekyll是因为看中了HCZ Material theme这个主题，折腾了很久才把博客搭建好， &lt;!-- more --&gt; 后来周边人准备些博客的时候已经不推荐使用jekyll了，推荐hexo给好几个人，不用他们折腾，（ps：他们折腾有可能折腾到我），但是自己的博客一直没换，主要环境正常吧。 虽然很久没有发布新博文了，但是草稿一直在新建，这两天心血来潮准备换博客引擎，稍微折腾下就好了]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>essay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6.5安装mysql5.6]]></title>
    <url>%2Fothers%2Fcentos%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[What mysql是 传统关系型数据库 Why centos 一般自带的都是比较老的版本 ，需要新版版 How 查看并删除老版本 yum list installed | grep mysql yum -y remove mysql-libs.x86_64 下载yum源文件并安装 https://dev.mysql.com/downloads/repo/yum/ wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpm rpm -ivh mysql-community-release-el6-5.noarch.rpm 查看一下是否已经有mysql可安装文件 yum repolist all | grep mysql sudo yum-config-manager --disable mysql80-community sudo yum-config-manager --enable mysql57-community 安装mysql 服务器命令 yum install mysql-community-server 启动服务 service mysqld start 进入mysql mysql -u root 1234use mysql;update user set password=123456 where User='root';GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;flush privileges; 注意点 跳过权限 mysqld_safe --skip-grant-tables use mysql; UPDATE user SET Password=PASSWORD('123456') where USER='root'; FLUSH PRIVILEGES; 参考 http://blog.csdn.net/hu_wen/article/details/53817432 问题处理 mysql官方]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos安装jdk]]></title>
    <url>%2Fothers%2Fcentos%E5%AE%89%E8%A3%85jdk%2F</url>
    <content type="text"><![CDATA[What How 下载jdk http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html wget http://download.oracle.com/otn-pub/java/jdk/8u152-b16/aa0333dd3019491ca4f6ddbe78cdb6d0/jdk-8u152-linux-x64.tar.gz?AuthParam=1513491653_780a1246d58a066130a62b431b8be265 创建目录 mkdir -p /usr/local/java 将安装解压到该目录 tar -zxvf jdk-8u152-linux-x64.tar.gz -C /usr/local/java 配置环境变量 vi /etc/profile 1234export JAVA_HOME=/usr/local/java/jdk1.8.0_152export JRE_HOME=$JAVA_HOME/jreexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=./:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 使配置立即生效 source /etc/profile 验证是否配置正确 java -version 注意点 替换版本， 参考 http://blog.csdn.net/hu_wen/article/details/53817432]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>jdk</tag>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NTP时间服务器]]></title>
    <url>%2Fothers%2FNTP%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[ntp.shu.edu.cn (202.120.127.191) 47.92.108.218 ntp.aliyun.com]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cdh实战]]></title>
    <url>%2Fbigdata%2Fcdh%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[What Hadoop发行版 Apache Hadoop Cloudera’s Distribution Including Apache Hadoop（CDH） Hortonworks Data Platform (HDP) MapR EMR sqoop datax flume 安装方式 Cloudera Manager yum rpm tarball 名词 主机 - host 机架 - rack 集群 - cluster 服务 -service 服务实例 - service instance 角色 -role nn dn rs jn 角色实例 -role instance 角色组 -role group 主机模板 -host template parcel -安装包 静态服务池 -static service pool 动态资源池 -dynamic resource pool ansible 运维自动化 Server 管理控制台服务器和应用程序逻辑 负责软件安装、配置 启动和停止服务 管理服务运行的群集 Agent 安装在每台主机上 负责启动和停止进程，配置，监控主机 Management Servicce 由一组角色组成的服务，执行各种监视、报警、和报告 Where? 公共jar包 /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib CDH5.4 http://archive.cloudera.com/cdh5/ Cloudera Manager5.4.3： http://www.cloudera.com/downloads/manager/5-4-3.html Why 版本划分清晰 版本更新速度快 支持Kerberos安全认证 文档清晰 支持多种安装方式（Cloudera Manager方式 Apache Hadoop 不足之处 版本管理混乱 部署过程繁琐、升级过程复杂 兼容性差 安全性低 How 网络按配置 修改主机名 vi /etc/sysconfig/network 根据自己的虚拟机地址修改ip和dns网关 1vi /etc/sysconfig/network-scripts/ifcfg-eth0 vi /etc/udev/rules.d/70-persistent-net.rules 只保留最后一个，并将name改为‘eth0’ 重启，登录了 ping www.baidu.com说明网络没问题了 修改hosts vi /etc/hosts ssh免密钥登录 123456ssh localhostquitssh-keygen -t rsa -P '' -f ~/.ssh/id_rsassh-copy-id localhostssh localhostquit 防火墙关闭 service iptables stop chkconfig iptables off selinux关闭 vi /etc/selinux/config SELINUX=disabled 安装jdk配置环境变量 上传 rpm 和 tar 123456cd /optmkdir javamkdir tarrpm -ivh jdk-7u67-linux-x64.rpm yum install vim -yvim ~/.bash_profile source ~/.bash_profile 安装ntp yum install ntp ntpdate ntp.aliyun.com 安装mysql yum install -y mysql-server service mysqld restart mysql GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; flush privileges; 在这第三方依赖包 yum install -y chkconfig python bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse fuse-libs redhat-lsb 安装Cloudera Manager Server Agent 123mkdir /opt/cloudera-managercd /opt/tartar xvzf cloudera-manager*.tar.gz -C /opt/cloudera-manager 创建用户cloudera-scm useradd --system --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm 配置CM Agent vim /opt/cloudera-manager/cm-5.4.3/etc/cloudera-scm-agent/config.ini 配置CM Server数据库 123456mkdir /usr/share/javamv /opt/tar/mysql-connector-java-5.1.26-bin.jar .mv mysql-connector-java-5.1.26-bin.jar mysql-connector-java.jarmysql grant all on *.* to 'temp'@'%' identified by 'temp' with grant option;grant all on *.* to 'temp'@'cdh-node1' identified by 'temp' with grant option; 创建Parcel目录 1234567cd /opt/cloudera-manager/cm-5.4.3/share/cmf/schema/./scm_prepare_database.sh mysql temp -h cdh-node1 -utemp -ptemp --scm-host cdh-node1 scm scm scmmkdir -p /opt/cloudera/parcel-repochown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repomkdir -p /opt/cloudera/parcelschown cloudera-scm:cloudera-scm /opt/cloudera/parcels 制作CDH本地源 下载好文件CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel以及manifest.json，将这两个文件放到 server节点的/opt/cloudera/parcel-repo下。 打开manifest.json文件，里面是json格式的配置，找到与下载版本相对应的hash码，新 建文件，文件名与你的parel包名一致，并加上.sha后缀，将hash码复制到文件中保存。 启动CM Server、Agent 1234cd /opt/cloudera-manager/cm-5.4.3/etc/init.d/./cloudera-scm-server start./cloudera-scm-agent start 登录 http://cdh-node1:7180/cmf/login 用户密码 admin/admin 克隆镜像并修改配置 vi /etc/syscofig/network 修改主机名 vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改ip vi /etc/udev/rules.d/70-persistent-net.rules 只保留最后一个，并将name改为‘eth0’ 在node1 上 ssh 所有其它机器 使不弹出 确认提示 删除无用的包 ，删除已经安装过的 cloud-manager,不能删除主节点的 1234cd /opt/cloudera-manager/rm -rf *cd /opt/tartar xvzf cloudera-manager*.tar.gz -C /opt/cloudera-manager vim /opt/cloudera-manager/cm-5.4.3/etc/cloudera-scm-agent/config.ini 都改成server_host=cdh-node1 启动mysql，启动server，启动agent，登录管理端http://cdh-node1:7180/cmf/login 选择机器创建集群，然后一步步按照提示解决问题 日志 /opt/cloudra-manager/run/ /opt/cloudera-manager/cm-5.4.3/run/cloudera-scm-agent/process/ccdeploy_hive-conf_etchiveconf.cloudera.hive_-7544799387945245207/logs/stderr.log 问题 https://ask.csdn.net/questions/212420 看日志,应该是jdk目录不对 使得 找不到JAVA_HOME 重装 或者 建立软连接 ln -s '你的jdk目录' /usr/java/jdk1.7]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>云计算</tag>
        <tag>hadoop发行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm实战]]></title>
    <url>%2Fbigdata%2Fstorm%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[What storm是个实时的，分布式，高容错的计算系统 storm进程常驻内存 strom数据不经过磁盘，在内存中处理 &lt;!-- more --&gt; 架构 Nimbus 资源调度 任务分配 接收jar包 Supervisor 接收nimbus分配的任务 启动、停止自己管理的worker进程(当前supervisor上worker数量由配置文件设定) Worker 运行具体处理运算组件的进程(每个Worker对应执行一个Topology的子集) worker任务类型，即spout任务、bolt任务两种 启动executor (executor即worker JVM进程中的一个java线程，一般默认每个executor负责执行一个task任务) 编程模型 DAG Spout数据源 Bolt数据流处理组件 数据分发策略 storm grouping Shuffle grouping 随机分组，随机派发stream里面的tuple，保证每个bolt task接收到的tuple数目大致相同。 轮询，平均分配 Fields Grouping 按字段分组，比如，按&quot;user-id&quot;这个字段来分组，那么具有同样&quot;user-id&quot;的 tuple 会被分到相同的Bolt里的一 个task， 而不同的&quot;user-id&quot;则可能会被分配到不同的task。 None Grouping 不分组，这个分组的意思是说stream不关心到底怎样分组。目前这种分组和Shuffle grouping是一样的效果 All Grouping 广播发送，对于每一个tuple，所有的bolts都会收到 Global Grouping 全局分组，把tuple分配给task id最低的task 。 Direct Grouping 指向型分组， 这是一种比较特别的分组方法，用这种分组意味着消息(tuple)的发送者指定由消息接收者的 哪个task处理这个消息。只有被声明为 Direct Stream 的消息流可以声明这种分组方法。而且这种消息tuple必 须使用 emitDirect 方法来发射。消息处理者可以通过 TopologyContext 来获取处理它的消息的task的id (OutputCollector.emit方法也会返回task的id) Local or shuffle grouping 本地或随机分组。如果目标bolt有一个或者多个task与源bolt的task在同一个工作进程中，tuple将会被随机发 送给这些同进程中的tasks。否则，和普通的Shuffle Grouping行为一致 customGouping 自定义，相当于mapreduce那里自己去实现一个partition一样。 计算模型 Topology– DAG有向无环图的实现 对于Storm实时计算逻辑的封装 即，由一系列通过数据流相互关联的Spout、Bolt所组成的拓扑结构 生命周期:此拓扑只要启动就会一直在集群中运行，直到手动将其kill，否则不会终止 (区别于MapReduce当中的Job，MR当中的Job在计算执行完成就会终止) Tuple – 元组 Stream中最小数据组成单元 Stream – 数据流 从Spout中源源不断传递数据给Bolt、以及上一个Bolt传递数据给下一个Bolt，所形成的这些数据通道即叫做 Stream Stream声明时需给其指定一个Id(默认为Default) 实际开发场景中，多使用单一数据流，此时不需要单独指定StreamId Spout – 数据源 拓扑中数据流的来源。一般会从指定外部的数据源读取元组(Tuple)发送到拓扑(Topology)中 一个Spout可以发送多个数据流(Stream) 可先通过OutputFieldsDeclarer中的declare方法声明定义的不同数据流，发送数据时通过SpoutOutputCollector中的 emit方法指定数据流Id(streamId)参数将数据发送出去 Spout中最核心的方法是nextTuple，该方法会被Storm线程不断调用、主动从数据源拉取数据，再通过emit 方法将数据生成元组(Tuple)发送给之后的Bolt计算 Bolt – 数据流处理组件 拓扑中数据处理均有Bolt完成。对于简单的任务或者数据流转换，单个Bolt可以简单实现;更加复杂场景往往 需要多个Bolt分多个步骤完成 一个Bolt可以发送多个数据流(Stream) 可先通过OutputFieldsDeclarer中的declare方法声明定义的不同数据流，发送数据时通过SpoutOutputCollector中的 emit方法指定数据流Id(streamId)参数将数据发送出去 Bolt中最核心的方法是execute方法，该方法负责接收到一个元组(Tuple)数据、真正实现核心的业务逻辑 任务提交流程 storm 并发机制 worker 进程 一个Topology拓扑会包含一个或多个Worker(每个Worker进程只能从属于一个特定的Topology) 这些Worker进程会并行跑在集群中不同的服务器上，即一个Topology拓扑其实是由并行运行在Storm集群中 多台服务器上的进程所组成 设置Worker进程数 Config.setNumWorkers(int workers) executor 线程 Executor是由Worker进程中生成的一个线程 每个Worker进程中会运行拓扑当中的一个或多个Executor线程 一个Executor线程中可以执行一个或多个Task任务(默认每个Executor只执行一个Task任务)，但是这些 Task任务都是对应着同一个组件(Spout、Bolt) 设置Executor线程数 TopologyBuilder.setSpout(String id, IRichSpout spout, Number parallelism_hint) TopologyBuilder.setBolt(String id, IRichBolt bolt, Number parallelism_hint) 其中， parallelism_hint即为executor线程数 task 实际执行数据处理的最小单元 每个task即为一个Spout或者一个Bolt 设置Task数量 ComponentConfigurationDeclarer.setNumTasks(Number val) Rebalance – 再平衡 即，动态调整Topology拓扑的Worker进程数量、以及Executor线程数量 通过Storm UI 通过Storm CLI storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10 Task数量在整个Topology生命周期中保持不变，Executor数量可以变化或手动调整 Task数量和Executor是相同的，即每个Executor线程中默认运行一个Task任务 Storm 通信机制 Worker进程间的数据通信 Netty Worker内部的数据通信 Disruptor storm 容错机制 集群节点宕机 Nimbus服务器 非Nimbus服务器 进程挂掉 Worker 挂掉时，Supervisor会重新启动这个进程。如果启动过程中仍然一直失败，并且无法向Nimbus发送心跳，Nimbus会将该 Worker重新分配到其他服务器上 Supervisor 无状态(所有的状态信息都存放在Zookeeper中来管理) 快速失败(每当遇到任何异常情况，都会自动毁灭) Nimbus 无状态(所有的状态信息都存放在Zookeeper中来管理) 快速失败(每当遇到任何异常情况，都会自动毁灭) Acker -- 消息完整性的实现机制 Storm的拓扑当中特殊的一些任务 负责跟踪每个Spout发出的Tuple的DAG(有向无环图) Why Storm:纯流式处理 专门为流式处理设计 数据传输模式更为简单，很多地方也更为高效 并不是不能做批处理，它也可以来做微批处理，来提高吞吐 Where http://storm.apache.org How 完全分布式部署 tar zxvf apache-storm-0.10.0.tar.gz 创建日志目录 mkdir logs 在node1 上执行 bin/storm nimbus &gt; logs/nimbus.out 2&gt;&amp;1 &amp; bin/storm ui &gt; ./logs/ui.out 2&gt;&amp;1 &amp; 在node2上执行 bin/storm supervisor &gt; logs/supervisor.out 2&gt;&amp;1 &amp; 在node3 上执行 bin/storm supervisor &gt; logs/supervisor.out 2&gt;&amp;1 &amp; 命令： bin/strom supervisor &gt; logs/supervisor.out 2 &gt;&amp;1 &amp; bin/strom --help bin/strom rebalance wc -w 5 -n 3 -e splitbolt=6]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jekins配合sonar配置]]></title>
    <url>%2Fessay%2Fjekins%E9%85%8D%E5%90%88sonar%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[What ? Sonar是一个用于代码质量管理的开源平台 &lt;!-- more --&gt; Why ? Where? https://www.sonarqube.org/ How? &lt;kbd&gt;sonar安装&lt;/kbd&gt; 下载 useradd sonar passwd sonar cd /opt/soft/ wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-5.6.4.zip unzip sonarqube-5.6.4.zip vi $HOME/.bash_profile 修改如下 123PATH=$PATH:$HOME/binSONAR_HOME=/opt/soft/sonarqube-5.6.4export PATH SONAR_HOME 1yum install postgresql 1234mysql&gt; CREATE USER 'sonar'@'%' IDENTIFIED BY 'sonar';mysql&gt; create database sonar;mysql&gt; grant all privileges on sonar.* to 'sonar'@'%' identified by 'sonar';mysql&gt; flush privileges; 编辑${SONAR_HOME}/conf/sonar.properties配置数据库 123sonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&amp;useConfigs=maxPerformance 启动 ${SONAR_HOME}/bin/linux-x86-64/sonar.sh start 在浏览器中访问 http://localhost:9000/ 修改默认端口为 9009 ，否则和 hdfs冲突 vi ${SONAR_HOME}/conf/sonar.properties sonar.web.port=9009 {SONAR_HOME}/bin/linux-x86-64/sonar.sh restart 插件 如果未联网 将下载后的插件上传到${SONAR_HOME}/extensions\plugins目录下，重新启动sonar 如果联网 http://docs.sonarqube.org/display/SONAR/Installing+a+Plugin admin登录后，点击Administer --&gt;system &gt; update center &lt;kbd&gt;sonarqube scanner&lt;/kbd&gt; https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-2.5.1.zip &lt;kbd&gt;jekins安装&lt;/kbd&gt; 下载 useradd jekins passwd jekins cd /opt/soft/ 1234sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reposudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key`yum install jenkinsservice jenkins start 登录及初始化 http://ip:8080/jenkins 第一次安装jenkins的时候，会提示你要求输入安装密码， cat /var/lib/jenkins/secrets/initialAdminPassword 按照提示将密码粘贴到对应的输入狂即可 修改默认端口 vi /etc/sysconfig/jenkins 1JENKINS_PORT=&quot;8084&quot; service jenkins start 选择 sonar 插件，安装重启 &lt;kbd&gt;maven安装&lt;/kbd&gt; 跳转到软件目录 cd /opt/soft 下载 wget http://mirrors.cnnic.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar zxvf apache-maven-3.3.9-bin.tar.gz 修改环境变量 vi ~/.bash_profile 12export M2_HOME=/opt/soft/apache-maven-3.3.9export PATH=$PATH:$M2_HOME/bin 使生效 source ~/.bash_profile 验证是否成功 mvn -version &lt;kbd&gt;git安装&lt;/kbd&gt; 安装依赖 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel yum install gcc perl-ExtUtils-MakeMaker 下载源码 wget https://github.com/git/git/archive/v2.5.0.tar.gz 解压 12tar zxvf v2.5.0.tar.gzcd git-2.5.0/ 编译安装 12make prefix=/usr/local/git allmake prefix=/usr/local/git install 添加系统变量 echo &quot;export PATH=$PATH:/usr/local/git/bin&quot; &gt;&gt; /etc/bashrc 使生效 source /etc/bashrc 验证 删除源码 rm -rf git-2.5.0 v2.5.0.tar.gz &lt;kbd&gt;svn安装&lt;/kbd&gt; yum install subversion 验证 svnserve --version &lt;kbd&gt;jenkins构建配置&lt;/kbd&gt; 123456sonar.projectKey=mypaysonar.projectName=mypaysonar.projectVersion=1.0sonar.sources=src/mainsonar.binaries=target/classessonar.exclusions=src/main/webapp/assets/**,src/main/webapp/plugins/**,src/main/webapp/mobile/**,src/main/ressources/**]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>jekins sonar svn git centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka实战]]></title>
    <url>%2Fbigdata%2Fkafka%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[What ? 常用场景 系统间的解耦合 峰值压力缓冲 异步（并行）通信 常规的消息系统 不能保证绝对可靠 其它同类产品 Rabit MQ Redis ZeroMQ ActiveMQ producer: 消息生产者 Producer将消息发布到之规定的topic中，同时producer也能决定将此消息归属于哪个partition consumer:消费者 一个consumer属于一个consumer group 需要保存消费 消息的offset,对于offset的保存和使用,有消 费者consumer来控制 当consumer正常消费消 息时,offset将会&quot;线性&quot;的向前驱动,即消息将 依次顺序被消费 group 每个consumer都有对应的group 各个consumer消费不同的partition 因此一个消息在group内只消费一次 broker kafka 集群的server服务，负责处理消息的读写请求，存储消息 topic 一个topic 可以认为是一类消息 一个topic 分成多个partition 每个partition内部消息强有序 消息不经过内存缓冲 根据时间策略 7天 删除 partitions kafka基于文件存储.通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存;可以将一个topic切分多任意多个partitions,来消息保存/消费的效率.此外越多的partitions意味着可以容纳更多的consumer,有效提升并发消费的能力. ISR in-synco replicas 候选人机制 同步延后 Why ? 消息系统的特点 高性能 有序 FIFO 先进先出 持久性 持久化到磁盘且性能好 采用零拷贝技术 顺序读取 分布式 所有构件可以分布式 很灵活 高吞吐 单节点支持上千个客户端，百兆/s Where? http://kafka.apache.org/ How? wget http://mirror.bit.edu.cn/apache/kafka/0.10.1.0/kafka_2.10-0.10.1.0.tgz /opt/soft/kafka_2.10-0.10.1.0.tgz tar zxvf kafka_2.10-0.9.0.1.tgz cd /opt/soft/kafka_2.10-0.9.0.1 vi config/server.properties 修改zookeeper对应地址 zookeeper.connect=sj-node1:2181,sj-node3:2181,sj-node3:2181 scp -r kafka_2.10-0.9.0.1 root@sj-node2:/opt/soft/ scp -r kafka_2.10-0.9.0.1 root@sj-node3:/opt/soft/ scp -r kafka_2.10-0.9.0.1 root@sj-node3:/opt/soft/ 分别修改 broker.id 从0开始 后面 分别是 1， 2， 3，4 修改完配置以后在每个节点启动kafka bin/kafka-server-start.sh config/server.properties 测试 新建一个topic ./bin/kafka-topics.sh -zookeeper sj-node1:2181,sj-node2:2181,sj-node3:2181 -topic test -replication-factor 2 -partitions 5 --create 查看当前的topic ./bin/kafka-topics.sh -zookeeper sj-node1:2181,sj-node2:2181,sj-node3:2181 -list 在一个节点创建一个provider ./bin/kafka-console-producer.sh --broker-list sj-node1:9092,sj-node2:9092,sj-node3:9092 --topic test 在另外一个节点创建一个consumer ./bin/kafka-console-consumer.sh --zookeeper sj-node1:2181,sj-node2:2181,sj-node3:2181 --from-beginning -topic test 删除topic,不会立即生效 ./bin/kafka-topics.sh --delete --topic test --zookeeper sj-node1:2181,sj-node2:2181,sj-node3:2181 查看明细 ./bin/kafka-topics.sh --zookeeper sj-node1:2181 --describe --topic testflume 问题 kafka offset 维护 zookeeper维护 0.9版本以前 kafka自己维护 0.9版本以后 kafka offset 数据存储机制 https://www.cnblogs.com/ITtangtang/p/8027217.html 彻底删除Kafka中的topic 删除kafka存储目录（server.properties文件log.dirs配置，默认为&quot;/tmp/kafka-logs&quot;）相关topic目录 Kafka 删除topic的命令是： ./bin/kafka-topics.sh --delete --zookeeper 【zookeeper server】 --topic 【topic name】 如果kafaka启动时加载的配置文件中server.properties没有配置delete.topic.enable=true，那么此时的删除并不是真正的删除，而是把topic标记为：marked for deletion 此时你若想真正删除它，可以登录zookeeper客户端： 命令：./bin/zookeeper-client 找到topic所在的目录：ls /brokers/topics 找到要删除的topic，执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。 问题 数据丢失 procedure端 只有一个主机，接收数据保存在内存，如果死机，该段时间数据丢失 可以用个备份主机，发送方 ack机制 0:只发送不管leader接收是否成功，1:leader接收成功就下一条，-1:等所有备份机接收成功才下一条 consumer端 如果是低级api，更新偏移量之后，执行的代码错误，导致该条数据未正常处理，如果是高级api，处理时间大于自动提交时间，则至少有一条丢失 解决方式：关闭自动提交，改成手动提交 数据重复消费 0.8版本默认是自动60s提交一次偏移量，如果在自动提交之前出错，已经处理过的偏移量未更新，则下次执行的时候会把那60s内处理过的数据再消费一次 解决方式：关闭自动提交，改成手动提交]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>kafka</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven使用tips]]></title>
    <url>%2Fessay%2Fmaven%E4%BD%BF%E7%94%A8tips%2F</url>
    <content type="text"><![CDATA[命令行根据archetype创建项目 mvn archetype:generate mvn archetype:generate -X debug模式 mvn archetype:generate -DarchetypeCatalog=internal 本地模式 &lt;!-- more --&gt; http://blog.csdn.net/u011303778/article/details/46534601 mac 下添加 自定义 archetype 编辑 /Users/${your_username}/Library/Caches/${your_idea_version}/Maven/Indices/UserArchetypes.xml 12345&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;archetypes&gt; &lt;archetype groupId="org.scala-tools.archetypes" artifactId="scala-archetype-simple" version="1.3" /&gt; &lt;archetype groupId="org.sparkinaction" artifactId="scala-archetype-sparkinaction" version="0.13" repository="https://github.com/spark-in-action/scala-archetype-sparkinaction/raw/master" /&gt;&lt;/archetypes&gt; 安装jar至本地nexus 在maven的conf/setting.xml 配置nexus私服的管理账号 在servers标签下添加server 123456789101112131415&lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;sjrep&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; id可自己定义一个名称 以及私服的管理管的账号密码 在mirrors和profiles下配置nexus私服 在项目的pom.xml中配置 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.10.8:18080/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.10.8:18080/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; id与settings.xml中的server的id对应 当项目package后将jar上传到nexus私服 mvn deploy 找不到archetype cd ~/.m2/ curl http://repo1.maven.org/maven2/archetype-catalog.xml &gt; archetype-catalog.xml 1mvn archetype:generate -DarchetypeCatalog=local]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark进阶之shuffle调优]]></title>
    <url>%2Fbigdata%2FSpark%E8%BF%9B%E9%98%B6%E4%B9%8Bshuffle%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 shuffle file not find taskScheduler不负责重试task，由DAGScheduler负责重试stage spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager 默认值：sort|hash 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议： 如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>shuffle调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark进阶之Sparksql]]></title>
    <url>%2Fbigdata%2FSpark%E8%BF%9B%E9%98%B6%E4%B9%8BsparkSql%2F</url>
    <content type="text"><![CDATA[What ? sparksql 内存列存储 字节码生成技术CG hive on spark 底层spark执行 hive做 存储和sql解析优化 spark on hive spark做 sql 优化和执行 hive做存储 &lt;!-- more --&gt; sparksql 数据源 json jdbc hive hbase 谓词下推 Why ? Where? How? DataFrame 通过json文件创建 DataFrame 123456789101112131415val conf = new SparkConf() conf.setMaster("local").setAppName("jsonfile")val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val df = sqlContext.read.json("sparksql/json")//val df1 = sqlContext.read.format("json").load("sparksql/json")df.show()df.printSchema()//select * from tabledf.select(df.col("name")).show()//select name from table where age&gt;19 df.select(df.col("name"),df.col("age")).where(df.col("age").gt(19)).show() //select count(*) from table group by age df.groupBy(df.col("age")).count().show();/*** 注册临时表 */df.registerTempTable("jtable")val result = sqlContext.sql("select * from jtable") result.show()sc.stop() 通过json格式RDD创建 DataFrame 通过反射创建 DataFrame 12345678910SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("RDD");JavaSparkContext sc = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(sc);JavaRDD&lt;String&gt; lineRDD = sc.textFile("sparksql/person.txt"); JavaRDD&lt;Person&gt; personRDD = lineRDD.map(new Function&lt;String, Person&gt;() &#123;/****/private static final long serialVersionUID = 1L;@Overridepublic Person call(String s) throws Exception &#123; Person p = new Person(); p.setId(s.split(",")[0]); p.setName(s.split(",")[1]); p.setAge(Integer.valueOf(s.split(",")[2])); 动态创建DataFrame java 123456789101112131415161718192021222324SparkConf conf = new SparkConf(); conf.setMaster(&quot;local&quot;).setAppName(&quot;rddStruct&quot;); JavaSparkContext sc = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(sc);JavaRDD&lt;String&gt; lineRDD = sc.textFile(&quot;./sparksql/person.txt&quot;); /*** 转换成Row类型的RDD*/JavaRDD&lt;Row&gt; rowRDD = lineRDD.map(new Function&lt;String, Row&gt;() &#123;/** **/private static final long serialVersionUID = 1L;@Overridepublic Row call(String s) throws Exception &#123; return RowFactory.create();&#125; &#125;);String.valueOf(s.split(&quot;,&quot;)[0]), String.valueOf(s.split(&quot;,&quot;)[1]), Integer.valueOf(s.split(&quot;,&quot;)[2])/*** 动态构建DataFrame中的元数据，一般来说这里的字段可以来源自字符串，也可以来源于外部数据库*/List&lt;StructField&gt; asList =Arrays.asList( DataTypes.createStructField(&quot;id&quot;, DataTypes.StringType, true), DataTypes.createStructField(&quot;name&quot;, DataTypes.StringType, true), DataTypes.createStructField(&quot;age&quot;, DataTypes.IntegerType, true));StructType schema = DataTypes.createStructType(asList);DataFrame df = sqlContext.createDataFrame(rowRDD, schema);df.show();sc.stop(); scala 12345678910val conf = new SparkConf() conf.setMaster("local").setAppName("rddStruct") val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val lineRDD = sc.textFile("./sparksql/person.txt") val rowRDD = lineRDD.map &#123; x =&gt; &#123;val split = x.split(",")RowFactory.create(split(0),split(1),Integer.valueOf(split(2))) &#125;&#125;val schema = StructType(List( StructField("id",StringType,true), StructField("name",StringType,true), StructField("age",IntegerType,true)))val df = sqlContext.createDataFrame(rowRDD, schema) df.show()df.printSchema()sc.stop() 读取JDBC中的数据创建DataFrame(MySql为例) scala 123456789101112val conf = new SparkConf() conf.setMaster("local").setAppName("mysql") val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val reader = sqlContext.read.format("jdbc")reader.option("url", "jdbc:mysql://192.168.179.4:3306/spark") reader.option("driver","com.mysql.jdbc.Driver")reader.option("user","root")reader.option("password","123456")reader.option("dbtable", "score")val score = reader.load()score.show()score.registerTempTable("score")val result = sqlContext.sql("select person.id,person.name,score.score from person,score where person.name = score.name")result.show() 读取parquet文件创建DataFrame scala 123456789101112131415161718val conf = new SparkConf() conf.setMaster("local").setAppName("parquet") val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val jsonRDD = sc.textFile("sparksql/json")val df = sqlContext.read.json(jsonRDD)df.show()/*** 将DF保存为parquet文件 */df.write.mode(SaveMode.Overwrite).format("parquet").save("./sparksql/parquet")df.write.mode(SaveMode.Overwrite).parquet("./sparksql/parquet")/*** 读取parquet文件*/var result = sqlContext.read.parquet("./sparksql/parquet")result = sqlContext.read.format("parquet").load("./sparksql/parquet")result.show()sc.stop() 读取Hive中的数据加载成DataFrame scala 123456789101112131415161718192021222324val conf = new SparkConf()conf.setAppName("HiveSource")val sc = new SparkContext(conf)/** * HiveContext是SQLContext的子类。 */val hiveContext = new HiveContext(sc)hiveContext.sql("use spark")hiveContext.sql("drop table if exists student_infos")hiveContext.sql("create table if not exists student_infos (name string,age int) row format delimited fields terminated by '\t'")hiveContext.sql("load data local inpath '/root/test/student_infos' into table student_infos")hiveContext.sql("drop table if exists student_scores")hiveContext.sql("create table if not exists student_scores (name string,score int) row format delimited fields terminated by '\t'")hiveContext.sql("load data local inpath '/root/test/student_scores' into table student_scores")val df = hiveContext.sql("select si.name,si.age,ss.score from student_infos si,student_scores ss where si.name = ss.name")hiveContext.sql("drop table if exists good_student_infos")/** * 将结果写入到hive表中 */df.write.mode(SaveMode.Overwrite).saveAsTable("good_student_infos")sc.stop() 落地DataFrame，已有api jdbc 1234567/*** 将数据写入到Mysql表中*/val properties = new Properties()properties.setProperty("user", "root")properties.setProperty("password", "1234")result.write.mode(SaveMode.Append).jdbc("jdbc:mysql://192.168.100.111:3306/spark", "result", properties) parquet 12345/*** 将DF保存为parquet文件*/df.write.mode(SaveMode.Overwrite).format("parquet").save("data/parquet")df.write.mode(SaveMode.Overwrite).parquet("data/parquet") hive 1234/*** 将结果写入到hive表中*/df.write.mode(SaveMode.Overwrite).saveAsTable("good_student_infos") 测试java中以下几种情况下不被序列化的问题： 反序列化时serializable 版本号不一致时会导致不能反序列化。 子类中实现了serializable接口，父类中没有实现，父类中的变量不能被序列化,序列化后父类中的变量会得到null。 注意：父类实现serializable接口,子类没有实现serializable接口时，子类可以正常序列化 被关键字transient修饰的变量不能被序列化。 静态变量不能被序列化，属于类，不属于方法和对象，所以不能被序列化。 spark on hive 在 Spark 客户端安装包下 spark-1.6.0/conf 中创建文件 hive-site.xml: 配置 hive 的 metastore 路径 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node1:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动Hive的metastore服务 hive --service metastore 启动zookeeper集群，启动HDFS集群。 自定义函数 UDF 12345678910111213141516171819val conf = new SparkConf()conf.setMaster("local").setAppName("udf")val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc);val rdd = sc.makeRDD(Array("zhansan", "lisi", "wangwu"))val rowRDD = rdd.map &#123; x =&gt; &#123; RowFactory.create(x)&#125;&#125;val schema = DataTypes.createStructType(Array(StructField("name", StringType, true)))val df = sqlContext.createDataFrame(rowRDD, schema)df.registerTempTable("user")// sqlContext.udf.register("StrLen",(s : String)=&gt;&#123;s.length()&#125;)// sqlContext.sql("select name ,StrLen(name) as length from user").showsqlContext.udf.register("StrLen", (s: String, i: Int) =&gt; &#123; s.length() + i&#125;)sqlContext.sql("select name ,StrLen(name,10) as length from user").showsc.stop() UDAF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class MyUDAF extends UserDefinedAggregateFunction &#123;// 聚合操作时，所处理的数据的类型def bufferSchema: StructType = &#123; DataTypes.createStructType(Array(DataTypes.createStructField("aaa", IntegerType, true)))&#125;// 最终函数返回值的类型def dataType: DataType = &#123; DataTypes.IntegerType&#125;def deterministic: Boolean = &#123; true&#125;// 最后返回一个最终的聚合值 要和dataType的类型一一对应def evaluate(buffer: Row): Any = &#123; buffer.getAs[Int](0)&#125;// 为每个分组的数据执行初始化值def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0&#125;//输入数据的类型def inputSchema: StructType = &#123; DataTypes.createStructType(Array(DataTypes.createStructField("input", StringType, true)))&#125;// 最后merger的时候，在各个节点上的聚合值，要进行merge，也就是合并def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getAs[Int](0) + buffer2.getAs[Int](0)&#125;// 每个组，有新的值进来的时候，进行分组对应的聚合值的计算def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getAs[Int](0) + 1&#125;&#125;object UDAF &#123;def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("udaf") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val rdd = sc.makeRDD(Array("zhangsan", "lisi", "wangwu", "zhangsan", "lisi")) val rowRDD = rdd.map &#123; x =&gt; &#123; RowFactory.create(x) &#125; &#125; val schema = DataTypes.createStructType(Array(DataTypes.createStructField("name", StringType, true))) val df = sqlContext.createDataFrame(rowRDD, schema) df.show() df.registerTempTable("user") /** * 注册一个udaf函数 */ sqlContext.udf.register("StringCount", new MyUDAF()) sqlContext.sql("select name ,StringCount(name) from user group by name").show() sc.stop()&#125;&#125; 开窗函数 1234567891011121314151617181920val conf = new SparkConf()conf.setAppName("windowfun")val sc = new SparkContext(conf)val hiveContext = new HiveContext(sc)hiveContext.sql("use spark");hiveContext.sql("drop table if exists sales");hiveContext.sql("create table if not exists sales (riqi string,leibie string,jineInt) "+ "row format delimited fields terminated by '\t'");hiveContext.sql("load data local inpath '/root/test/sales' into table sales");/*** 开窗函数格式:* 【 rou_number() over (partitin by XXX order by XXX) 】 */val result = hiveContext.sql("select riqi,leibie,jine " + "from (" + "select riqi,leibie,jine," + "row_number() over (partition by leibie order by jine desc) rank " + "from sales) t "+ "where t.rank&lt;=3");result.show();sc.stop() 优化 合适的数据列 最有的数据存储格式 内存的使用]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实践]]></title>
    <url>%2Flinux%2Fredis%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[what Redis 是一个开源(BSD 许可)的，内存中的数据结构存储系统， 它可以用作数据库、缓存和消息中间件。拥有丰富的支持主流语言的 客户端，C、C++、Python、Erlang、R、C#、Java、PHP、Objective- C、Perl、Ruby、Scala、Go、JavaScript。 数据结构丰富 Redis 虽然也是键值对数据库，但是和 Memcached 不同的是， Redis 支持多种类型的数据结构，不仅可以是字符串，同时还提供散 列(hashes)，列表(lists)，集合(sets)，有序集合(sorted sets) 等数据结 官网 wget http://download.redis.io/releases/redis-3.2.3.tar.gz tar -zxvf redis-3.2.3.tar.gz mv redis-3.2.3 redis cd redis yum install gcc tcl make test make MALLOC=libc &amp;&amp; make install cd utils/ ./install_server.sh 安装配置 配置自启动 cp /opt/redis-3.2.3/utils/redis_init_script /etc/init.d/redisd 在启动脚本开头添加如下两行注释以修改其运行级别： 1234#!/bin/sh# chkconfig: 2345 90 10# description: Redis is a persistent key-value database# 命令 设置为开机自启动服务器 chkconfig redisd on 打开服务 service redisd start 关闭服务 service redisd stop /usr/sbin/redis-server /etc/redis.conf 使用 进入命令行 rediscli 命令使用 切换数据库 key操作 KEYS pattern EXISTS key EXPIRE key seconds MOVE key db TTL key TYPE key string操作 SET key value [EX seconds] [PX milliseconds] [NX|XX] EX 设置过期时间，秒，等同于 SETEX key seconds value PX 设置过期时间，毫秒，等同于 PSETEX key milliseconds value NX 键不存在，才能设置，等同于 SETNX key value XX 键存在时，才能设置 GET key APPEND key value STRLEN key INCR key INCRBY key increment DECR key DECRBY key decrement GETRANGE key start end SETRANGE key offset value SETEX key seconds value SETNX key value MGET key [key ...] MSET key value [key value ...] MSETNX key value [key value ...] list 操作 LPUSH key value [value ...] RPUSH key value [value ...] LRANGE key start stop LPOP key RPOP key LINDEX key index LLEN key LREM key count value LTRIM key start stop RPOPLPUSH source destination LSET key index value LINSERT key BEFORE|AFTER pivot value SET操作 SAD 增加集合 sadd myset xixi haha SREM 删除 SPOP 无序弹出集合，适用无放回抽奖 SMEMBERS 列出集合内容 smove 移动 sdiff 求差集 sinter 求交集，微博共同关注 sunion 并集 SISMEMBER key member SCARD key sorted set 操作 zadd 增加有序集合 sorted set，使用排行榜场景 zrange 返回有序集key中，指定区间内成员 升序 zrevrange 按分数递减 zincrby 增加分数 zrem 指定删除某个元素 zremrangebyscore 按分数范围删除 zscore zcard zrank hash散列操作 hget hgetall hkeys hvals hmset hsetnx hincrby 持久化 rdb 快照 关闭redis-cli -&gt; CONFIG SET save &quot;&quot; 手动触发redis-cli-&gt; SAVE 清空FLUSHALL aof 保留操作日志，两个都开启，优先使用这个 appendfsyc always每条持久化 appendfsyc everysec 每秒持久化 appendfsnc no操作系统自处理 优缺点 命令去掉用避免被全干掉 auto-aof 主从复制 从服务器配置 slaveof 192.168.1.1 6379 哨兵机制 redis-sentinel redis集群 参考 https://mp.weixin.qq.com/s/TR8oe7c1SlOrk78untXdOA]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tool</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建maven自定义项目骨架archetype]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%E5%B8%88%E7%AC%94%E8%AE%B0%2F%E8%87%AA%E5%AE%9A%E4%B9%89maven%E9%A1%B9%E7%9B%AE%E9%AA%A8%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[第一步 在pom.xml中加入插件 12345&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-archetype-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt;&lt;/plugin&gt; &lt;!-- more --&gt; 第二步 这里首先定义了一个archetype.properties文件在命令行被执行的目录，里面的内容是 12345678tablePrefix=ss_packageName=wywk-archetypegroupId=cn.wywkartifactId=defaultprojectversion=1.0-SNAPSHOTexcludePatterns=archetype.filteredExtensions=projectChName=鱼付宝数据管理 命令 1mvn clean archetype:create-from-project -Darchetype.properties=../archetype.properties -X 修改archetype 参数 安装archetype到本地 mvn install 安装archetype到本地私服服务器 在~/.m2/settings.xml配置文件中servers标签下添加 123456789101112131415&lt;server&gt; &lt;id&gt;wynexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;wynexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;sjrep&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 在archetype项目的pom.xml中添加私服地址 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Micaicms Releases&lt;/name&gt; &lt;url&gt;http://127.0.0.1:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Micaicms Releases&lt;/name&gt; &lt;url&gt;http://127.0.0.1:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 执行命令mvn deploy 创建新项目 mvn archetype:update-local-catalog 使用本地 mvn archetype:generate -DarchetypeCatalog=local 用私服 mvn archetype:generate -DarchetypeCatalog=http://localhost:8081/nexus/content/groups/public -X 参考 https://blog.csdn.net/significantfrank/article/details/41807581]]></content>
      <categories>
        <category>架构师笔记</category>
      </categories>
      <tags>
        <tag>架构师</tag>
        <tag>java</tag>
        <tag>maven</tag>
        <tag>archetype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop实践]]></title>
    <url>%2Fbigdata%2Fsqoop%2F</url>
    <content type="text"><![CDATA[What 将关系数据库 oracle ,mysql,ostgresql 和hadoop 数据进行转换的工具 版本 sqoop1 :1.4.x 使用最多 sqoop2: 1.99.x 同类产品 DataX 阿里顶级数据交换工具 Where http://sqoop.apache.org Why How 下载 `cd /opt/soft/`` wget http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz mv /opt/soft/sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6 修改配置文件 vi $SQOOP_HOME/bin/configure-sqoop 去掉不相干的环境变量检查 cd conf mv sqoop-evn-template.sh sqoop-env.sh vim sqoop-env.sh 修改环境变量 vi ~/.base_profile 12export SQOOP_HOME=/opt/soft/sqoop-1.4.6PATH=$PATH:$SQOOP_HOME/bin 添加数据库驱动包 cp mysql-connecter-jaa-5.1.10.jar $SQOOP_HOME/lib/ 测试 sqoop version 12sqoop-list-databases -connect jdbc:mysql://sj-node1:3306/ -username root -password 123456sqoop-list-tables -connect jdbc:mysql://sj-node1:3306/mysql -username root -password 123456  启动hdfs集群，和yarn集群 导入 导入hdfs 1sqoop import --connect jdbc:mysql://sj-node1:3306/test --username root --password 123456 --table testsqoop -m1 --target-dir hdfs://appcity/sqooptest 导入hive 123sqoop import --connect jdbc:mysql://sj-node1:3306/test \--username root --password 123456 \--table testsqoop -m 1 --hive-import 导入hbase 启动hbase start-hbase.sh 1234sqoop import --connect jdbc:mysql://sj-node1:3306/test \--username root --password 123456 --table testsqoop -m1\ --hbase-table testsqoop --column-family cf1 --hbase-row-key name\ --hbase-create-table 在sqoop自动创建了表testsqoop 导出 hdfs导出mysql，hdfs上文件如果默认以逗号,分割就无需指定分隔符 12sqoop export --connect jdbc:mysql://sj-node1:3306/test \--username root --password 123456 --table testsqoop --export-dir /sqooptest/part-m-00000 hive导出mysql sqoop export --connect jdbc:mysql://sj-node1:3306/test \ --username root --password 123456 --table testsqoop \ --export-dir /user/hive_local/warehouse/testsqoop/part-m-00000 \ --input-fields-terminated-by '\001' 问题 No columns to generate for ClassWriter问题]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs异常记录]]></title>
    <url>%2Fbigdata%2Fhdfs%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[背景 我的hadoop先配置的普通模式启动测试没问题，后来改为HA高可用模式，启动没有问题，进程都在，50070端口访问显示正常，Namenode节点active，secondnamenode节点standby，通过网址http://sj-node1:50070/explorer.html#/访问也正常显示，但是在命令行做文件操作的时候一直报错 这个180.168.41.175ip网上搜索了下发现是电信对不能解析的域名自动给出的ip，比如：ping sdfsdfasdf，有点想不通怎么去访问网络去了。 分析 appcity是自定义的集群名，不应该去访问网络的，所以应该还是配置的问题 core-site.xml 12345678910111213141516171819202122232425262728293031323334353637 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;Indicates the number of retries a client will make to establish a server connection. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;description&gt;Indicates the number of milliseconds a client will wait for before retrying to establish a server connection. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://appcity&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://appcity&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/soft/hadoop-2.5.1/data&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sj-node2:2181,sj-node3:2181,sj-node4:2181&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;appcity&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.appcity&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.appcity.nn1&lt;/name&gt; &lt;value&gt;sj-node1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.appcity.nn2&lt;/name&gt; &lt;value&gt;sj-node2:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.appcity.nn1&lt;/name&gt; &lt;value&gt;sj-node1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.appcity.nn2&lt;/name&gt; &lt;value&gt;sj-node2:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://sj-node2:8485;sj-node3:8485;sj-node4:8485/appcity&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/soft/hadoop-2.5.1/data/jn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 虽然配置文件检查了好几遍也没发现哪不对，最终要的是控制台启动的时候也不抱什么错误，只有在文件操作的时候才报错。网上也搜索不到相关的问题，及解决方案，自己把集群格式化了几遍也没效果，实在没办法，只能逐条配置属性逐条的看最后发现不合理的地方： 1234 &lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; 前面一直坚持value，这里的name里有还有个mycluster,我的集群名叫appcity，mycluster，应该是官网demo上的名称，忘记改了，把它改成我的集群名，重启，果然文件解决]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>hdsf</tag>
        <tag>exception</tag>
        <tag>issue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume实践]]></title>
    <url>%2Fbigdata%2Fflume%2F</url>
    <content type="text"><![CDATA[What Flume 海量日志收集框架 分布式 可扩展 可靠 高可用 0.9x 的架构 OG 单点故障问题,使用zookeeper高可用 1.0x 的架构 NG Agent 由source、channel、sink三大组件组成，如图： source 采集 从Client收集数据，传递给Channel。 可以接收外部源发送过来的数据。 不同的 source，可以接受不同的数据格式。 channel 存储池 接收source的输出 直到有sink消费掉channel中的数据Channel中的数据直到进入到下一个channel中或者进入终端才会被删除 sink写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。 sink 输出 数据库可靠性 end-to-end store on failure best effort 自身可扩展性 1.0自身agent实现扩展 功能可扩展性 Flume自带了很多组件，包括各种agent（file，syslog，HDFS等） Why Where http://flume.apache.org/ avro简介 说明 http://flume.apache.org/FlumeUserGuide.html How 安装 下载 配置flume-env.sh 123cd /opt/soft/apache-flume-1.7.0-bin/confcp flume-env.sh.template flume-env.shvi flume-env.sh 修改java_home 配环境变量 vi ~/.base_profile 添加flume home 12export FLUME_HOME=/opt/soft/apache-flume-1.7.0-binPATH=$PATH:$FLUME_HOME/bin 使立刻生效source ~/.bash_profile 安装telnet yum install telnet.x86_64 配置文件 netcat_logger.conf 1234567891011121314151617181920212223# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动 1flume-ng agent --conf conf --conf-file netcat_logger.conf --name a1 -Dflume.root.logger=INFO,console 需在 netcat_logger.conf 这个目录执行 ，否则需要全路径 测试 ，另开一个ssh 窗口启动 telnet telnet localhost 44444 模式 netcat_logger netcat_hdfs exec_hdfs tailf *.log 1234567891011121314151617181920212223# a1 which ones we want to activate. a1.channels = c1 a1.sources = r1 a1.sinks = k1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /root/flume.log a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://appcity/flumelog/%y-%m-%d a1.sinks.k1.hdfs.rollCount=0 a1.sinks.k1.hdfs.rollInterval=0 a1.sinks.k1.hdfs.rollSize=10240 a1.sinks.k1.hdfs.idleTimeout=5 a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.useLocalTimeStamp=true # Define a memory channel called c1 on a1 a1.channels.c1.type = memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 avro_logger]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>flume</tag>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive实践]]></title>
    <url>%2Fbigdata%2FHive%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[What Hive起源于Facebook，它使得针对Hadoop进行SQL查询成为可能，从而非程序员也可以方便地使用。 hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 &lt;!-- more --&gt; hive元数据信息存在关系型数据库上 表 的名字， 权限，字段，创建者，数据目录 hive接口 cli client wui webui 3中运行模式 内嵌模式 本地模式 mysql和hive在一台服务器上 远程模式 远程一体 mysql和hive不在一台服务器上,hive 的server和client在一台机器上 远程分开 mysql和hive不在一台服务器上,hive 的server和client也在不同的机器上 执行mapreduce 时机 select * from t 单表不执行 select * from t where t&gt;1 单表字段不执行 select count(1) from t 包含函数执行mapreduce任务 Why ? 数据库与数据仓库 非Java编程者对HDFS的数据做mapreduce操作。 使用SQL来快速实现简单的MapReduce 统计，不必开发专门 的MapReduce 应用，学习成本低，十分适合数据仓库的统计 分析。 Where http://hive.apache.org How 安装？ 安装mysql 参考博客中的另一篇2017-12-26-centos安装mysql 不支持mysql5.1 tar zxvf apache-hive-1.2.1-bin.tar.gz 集群上所有服务器都删 1rm -rf /opt/soft/hadoop-2.5.1/share/hadoop/yarn/lib/jline-0.9.94.jar 从hive里拷贝jar到hadoop里 1cp /opt/soft/apache-hive-1.2.1-bin/lib/jline-2.12.jar /opt/soft/hadoop-2.5.1/share/hadoop/yarn/lib/ 拷贝到集群上所有机器 1scp -r /opt/soft/apache-hive-1.2.1-bin/lib/jline-2.12.jar sj-node2:/opt/soft/hadoop-2.5.1/share/hadoop/yarn/lib/ 上传mysql驱动包 mysql-connector-java-5.1.32-bin.jar 到目录/opt/soft/apache-hive-1.2.1-bin/lib 修改配置文件 vi /opt/soft/apache-hive-1.2.1-bin/conf/hive-site.xml输入以下内容 本地模式 1234567891011121314151617181920212223242526272829&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_local/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://sj-node1/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 远程一体模式 将上述里的这个属性改下 1234&lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; - 远程分开模式 - client 端 在sj-node1上 12345678&lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://sj-node2:9083&lt;/value&gt;&lt;/property&gt; - server端 sj-node2 去掉`hive.metastore.local` 这个属性 - 服务端启动方式 `hive --service metastore` 环境变量 vi ~/.bash_profile 加入如下内容 12export HADOOP_HOME=/opt/soft/hadoop-2.5.1export HIVE_HOME=/opt/soft/apache-hive-1.2.1-bin source ~/.bash_profile 提供jdbc服务供链接 hiverserver2 启动服务： $HIVE_HOME/bin/hiverserver2 hive --service metastore 123456789 public static void main(String[] args) &#123; try&#123; Class.forName("org.apache.hive.jdbc.HiveDriver"); Connection conn = DriverManager.getConnection("jdbc:hive2://node5:10000/default") Statement st = conn.createStatement(); ResultSet res = st.executeQuery("select count(*) from t_test"); &#125;&#125; HQL简介 数据类型 primitive_type 原始数据类型 TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE DOUBLE PRECISION STRING 搞定一切 BINARY TIMESTAMP DECIMAL(precision, scale) DATE VARCHAR CHAR array_type 数组数据类型 ARRAY &lt; data_type &gt; map_type MAP &lt; primitive_type, data_type &gt; struct_type STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type UNIONTYPE &lt; data_type, data_type, ... &gt; DDL create database testdata; drop database testdata; use testdata; 删除表 DROP TABLE [IF EXISTS] table_name [PURGE]; 重命名表 ALTER TABLE table_name RENAME TO new_table_name; 创建表 1234567891011create table t_p3( id int , name string, age int, likes array&lt;string&gt; comment 'your like things', address map&lt;string,string&gt; comment 'your address')row format delimited fields terminated by ',' collection items terminated by '-'map keys terminated by ':'lines terminated by '\n'; DML 5种导入方式,字段数相同，类型需匹配 LOAD local load data local inpath '/root/data.txt' overwrite into table t_person; hdfs hdfs dfs -put data.txt /test load data inpath '/test/data.txt' overwrite into table t_person; INSERT INTO From person t1 Insert Owerwrite Table person1 select id,name,age inert owerwrite table per2 select id,name,age; 更新数据 UPDATE tablename SET column = value [, column = value ...] [WHERE expression] 删除数据 DELETE FROM tablename [WHERE expression] #### 文本数据 data.txt -&gt; t_person 1231,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing data2.txt -&gt; t_p2 1231,zshang,182,lishi,163,wang2mazi,20 #### 执行sql查询 123from t_person t1,t_p2insert overwrite table t_p3select t1.id,t_p2.name,t_p2.age,t1.likes,t1.address where t1.id = t_p2.id; 保存结果 保存数据到本地 1hive&gt; insert overwrite local directory '/root/hive2local.txt' row format delimited fields terminated by ',' select * from testhive.t_person; teshive为数据名 保存数据到 HDFS 上: 1insert overwrite directory '/test/testback' select * from testhive.t_p3; 在 shell 中将数据重定向到文件中 1hive -e "select * from testhive.t_person;" &gt; result.txt 备份数据或还原数据 备份数据: 1EXPORT TABLE testhive.t_person TO '/test/testexport'; 删除再还原数据 123drop table testhive.t_person;show tables from testhive;IMPORT table testhive.t_person FROM '/test/testexport' ; ; 外部表(重要) 外部关键字 EXTERNAL 允许您创建一个表,并提供一个位 置,以便 hive 不使用这个表的默认位置。这方便如果你已经生成的数 据。当删除一个外部表,表中的数据不是从文件系统中删除。外部表指向任何 HDFS 的存储位置,而不是存储在配置属性指定的文件夹 hive.metastore.warehouse.dir.中 HIVEserDe序列化 HiveSerDe-SerializerandDeserializer SerDe用于做序列化和反序列 化。构建在数据存储和执行引擎之间，对两者实现解耦。 12345678910111213CREATE TABLE logtbl ( host STRING, identity STRING, t_user STRING, time STRING, request STRING, referer STRING, agent STRING)ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'WITH SERDEPROPERTIES ("input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \"(.*)\" (-|[0-9]*) (-|[0-9]*)")STORED AS TEXTFILE; 原始数据 access_log.txt 123456192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217 导入 1load data local inpath '/root/access_log.txt' into table testhive.logtbl; beeline和hiveserver2 server 端启动 hiveserver2 client端启动 beeline 输入!connect jdbc:hive2://sj-node2:10000 root 123456连接 hive的jdbc 启动hiveserver2 hive的分区 必须要在表定义的时候创建partition 分区字段内容 最好不要特殊字符，否则在hdfs上操作时转义过的就不能查看了 分区里的分区字段不能与表的字段同名 分区删除的时候最好指定一级二级分区名，否则删除不相关分区的子分区 分区重命名，多级分区的重命名，指明父子结构分区名，相当于移动到新的目录树下 分区分为静态分区和动态分区 hive的自定义函数 UDF User-Defined-Function 单输入单输出 UDAF(User- Defined Aggregation Funcation) 多输入单输出 UDTF(User-Defined Table-Generating Functions) 一进多输出 分桶 分桶表是对列值取哈希值的方式，将不同的数据放到不同的文件中 桶的个数相当于reduce的个数 抽样 调优 去格式化无用的- hive内数据迁移 先建表结构，然后用文件直接传送 ， 提高效率 把hive sql 当做MapReduce程序去优化 explain 小文件一般128m内 可以设为本地模式跑，速度会快点 严格模式 nostrict 限制查询 对分区表查询是，必须添加where条件对分区字段查询，否则不能查询 order by 语句必须包含limit限制 hive排序 order by sort by distribute by cluster by hive join join 时将小表（驱动表）放在join左边 在map端join sql方式 select /* +mapjoin(smalltable) */ smalltable.key bigtable.value from smalltable join bigtable on smalltable.key=bigtable.key 开启自动mapjoin set hive.auto.convert.join=true 参考 http://hive.apache.org Hive学习之HiveServer2服务端配置与启动 hive中解决中文乱码 用JAVA代码通过JDBC连接HIVESERVER 疑问 select * from tbl 大表会不会死机 不可以这么操作 原始数据包含分区字段的时候如何建表导入 动态分区导入 备注字段？ -- 可以在sql文件中注释]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>hive</tag>
        <tag>数据仓库</tag>
        <tag>hql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jqGrid翻页无效果debug经历]]></title>
    <url>%2Fbuger%2Fjqgrid%E7%BF%BB%E9%A1%B5%E6%97%A0%E6%95%88%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[现象 直接点击餐单，后不点击查询按钮，在点击翻页按钮，没有问题 &lt;!-- more --&gt; 如果点击查询按钮后，再点击 翻页按钮，页面刷新不翻页 开始调试 打开chrome开发模式查看网络请求 正常情况（未点击查询按钮） 点击菜单 - 点击下页按钮 异常情况 点击查询按钮 - 点击翻页按钮 &gt; 发现是查询逻辑丢掉了page信息 查看代码 list初始化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 jQuery(grid_selector).jqGrid(&#123; url:'list', postData: &#123;'receiptBizType':$('#search_receiptBizType').val()&#125;, datatype: 'json', mtype: "POST", height: "100%", rownumbers: true, rownumWidth:50, colNames:['','回单ID','回单编号','门店共用码','业务类型', '金额','备注','业务日期','状态','订单ID','创建时间','更新时间'], colModel:[ &#123;name:'action',hidden:false,width:30,fixed:true&#125;, &#123;name:'bankReceiptId',sortable:false,editable:false,key:true&#125;, &#123;name:'receiptNo',index:'receipt_no',editable:false&#125;, &#123;name:'commonCode',index:'common_code',editable:false&#125;, // &#123;name:'accoutnName',index:'accoutnName',editable:false&#125;, &#123;name:'receiptBizType',index:'receipt_biz_type',editable:false,formatter:getHideListValue,formatoptions: &#123;code:'TRANS_TYPE'&#125;&#125;, &#123;name:'amount',index:'amount',editable:false,formatter:currencyFormatter, formatoptions:&#123;decimalSeparator:".", thousandsSeparator: ",", decimalPlaces: 2, divide:100,prefix: "￥"&#125;&#125;, //&#123;name:'hamount',index:'hamount',editable:false,formatter:currencyFormatter, formatoptions:&#123;decimalSeparator:".", thousandsSeparator: ",", decimalPlaces: 2, prefix: "￥"&#125;&#125;, &#123;name:'bizRemark',index:'biz_remark',sortable:false,editable:false&#125;, &#123;name:'bizDate',index:'biz_date',editable:false,formatter:dateFormatter,formatoptions:&#123;dateFormat:'yyyy-MM-dd'&#125;&#125;, &#123;name:'state',editable:false,formatter:getHideListValue,formatoptions: &#123;code:'BANK_RECEIPT_STATE'&#125;,unformat:getHideListValueReverse&#125;, &#123;name:'ordId',index:'ordId',sortable:false,editable:false&#125;, &#123;name:'createDate',index:'create_date',editable:false,formatter:dateFormatter,formatoptions:&#123;dateFormat:'yyyy-MM-dd hh:mm:ss'&#125;&#125;, &#123;name:'lastUpdateDate',index:'last_update_date',editable:false,formatter:dateFormatter,formatoptions:&#123;dateFormat:'yyyy-MM-dd hh:mm:ss'&#125;&#125;, ], viewrecords : true, rowNum:10, rowList:[10,20,50], pager : "#grid-pager", multiselect: false, multiboxonly: false, altRows: true, autowidth: true, autoScroll: false, caption: "银行回单列表", emptyrecords: "未查询到数据", jsonReader : &#123; root:"result", total:'totalPages', page:'page', records:'records' &#125;, loadComplete : function() &#123; var table = this; setTimeout(function()&#123; updatePagerIcons(table); &#125;, 0); &#125;, gridComplete : function()&#123; var ids = jQuery(grid_selector).jqGrid('getDataIDs'); for(var i=0;i &lt; ids.length;i++)&#123; var cl = ids[i]; var rowData = $(grid_selector).getRowData(cl); checkbox = "&lt;label&gt;&lt;input name=\"grid-checkbox\" value=\"" + rowData.bankReceiptId + "\"type=\"checkbox\" class=\"ace\"&gt;&lt;span class=\"lbl\"&gt;&lt;/span&gt;&lt;/label&gt;"; jQuery(grid_selector).jqGrid('setRowData',ids[i],&#123;action:checkbox&#125;); &#125; /** * 窗口缩放时，经动态变化宽度 */ $(window).resize(function()&#123; var winwidth=$('.page-content').width(); //当前页面的宽度 $(grid_selector).setGridWidth(winwidth); $('.ui-jqgrid-bdiv').css('width',winwidth+1); &#125;); /** * 点击菜单边框收缩菜单时，动态变化表格宽度 */ $('#sidebar-collapse').click(function()&#123; var winwidth=$('.main-content .col-xs-12').width(); //当前窗口中，一行的宽度 $(grid_selector).setGridWidth(winwidth); $('.ui-jqgrid-bdiv').css('width',winwidth+1); &#125;); &#125;&#125;); 查询按钮逻辑 12345 /*查询按钮*/$('#search').click(function()&#123; jQuery(grid_selector).jqGrid('clearGridData').jqGrid('setGridParam', &#123; postData: $('#search_form').serialize()&#125;).jqGrid('setGridParam', &#123; 'page': 1 &#125;) .trigger("reloadGrid");&#125;); 没发现异常，删除.jqGrid('setGridParam', { postData: $('#search_form').serialize()}).后虽然没过滤，但可以翻页，网上查看发现别人在这块的查询参数都是一个一个拼接的json，应该是.serialize()不是json导致的（虽然后台也能接到这些参数），一个一个拼接太麻烦，而且增加查询字段的时候页面添加这里也得添加，容易忘掉，所以当时就是用form的serialize方法一劳永逸，既然此法不行就换个，把整个form转成json，网上有不少自己写的小工具，逻辑就是取出form的所有输入框，循环拼接json，太麻烦，而且每个页面都得加这个工具类，既然已经有这样的需求应该有更好的轮子，果然到github上找到了一个jquery插件叫 jquery.serializeJSON,将这个插件引入到公共页面，小改下代码,将$('#search_form').serialize()改为$('#search_form').serializeJSON()}) 12345/*查询按钮*/$('#search').click(function()&#123; jQuery(grid_selector).jqGrid('clearGridData').jqGrid('setGridParam', &#123; postData: $('#search_form').serializeJSON()&#125;).jqGrid('setGridParam', &#123; 'page': 1 &#125;) .trigger("reloadGrid");&#125;); 至此问题解决]]></content>
      <categories>
        <category>buger</category>
      </categories>
      <tags>
        <tag>jqGrid js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目收集整理]]></title>
    <url>%2Fessay%2F%E9%A1%B9%E7%9B%AE%E6%94%B6%E9%9B%86%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[有用的项目收集整理 名称 描述 类型 地址 JavaHost 使用虚拟DNS省掉开发环境配置hosts文件 Jar DJNativeSwing Java内置浏览器 Jar Highcharts 图表 Html/CSS http://www.hcharts.cn ECharts 百度图表库 Html/CSS http://echarts.baidu.com jqGrid 表格控件 jQuery插件 http://www.jqgrid.com/ jQuery.bsgrid 表格控件 jQuery插件 http://thebestofyouth.com/bsgrid/ easypoi EasyPoi Excel和 Word简易工具类 jar http://git.oschina.net/jueyue/easypoi ZooKeeper 分布式应用程序协调服务 war http://zookeeper.apache.org/ Grid++Report WEB报表插件 ActiveX http://www.rubylong.cn/ open-falcon 人性化的互联网企业级监控系统 Linux http://http://open-falcon.com/ Disconf 为各种业务平台提供统一的配置管理服务 war https://github.com/knightliao/disconf Lepus（天兔） 数据库企业监控系统 java/.net http://www.lepus.cc/ Apache NiFi 数据处理和分发系统 java http://nifi.apache.org/ Locust 开源负载测试工具 Python http://locust.io/ Vuvuzela 不会暴露他和谁进行通讯消息通讯系统 go https://github.com/davidlazar/vuvuzela ABTestingGateway 基于动态策略的灰度发布系统 nginx https://github.com/SinaMSRE/ABTestingGateway IoTivity 无缝的支持设备到设备的互联 复合 https://www.iotivity.org/ Mycat 用来替代昂贵的Oracle集群 复合 https://github.com/MyCATApache gRPC RPC框架 java https://github.com/grpc/grpc-java Plyr 开发的基于浏览器上的多媒体播放器 HTML5 https://plyr.io/ Plog 一套处理日志流的框架 python https://github.com/SinaMSRE/Plog glot 可以在线运行各种编程语言代码片段的平台 复合 https://glot.io/ PeerVPN 构建虚拟专网的开源软件系统 sh http://www.peervpn.net/ WifiDog 无线认证的软件 sh http://dev.wifidog.org/ Shireframe HTML 实现界面原型的绘制 js https://github.com/tsx/shireframe PayMap 集成（支付宝、微信、银联、光大、邮政支付）支付的小Demo java https://github.com/Martin404/PayMap Python_sdk 优图人脸识别(Face Recognition) Python https://www.oschina.net/p/python_sdk VOOME 人脸标注引擎 ios/android http://www.voome.cn/home/index.shtml 快速开发框架 名称 描述 类型 地址 G4Studio 二次快速开发平台 JavaEE http://git.oschina.net/osworks/G4Studio JFinal 极速 WEB + ORM 框架 Maven http://www.jfinal.com JCOP 轻量级代码生成 Gradle http://git.oschina.net/zhouleib1412/jfinal-code-online jfinal-oauth2.0-login 对第三方登陆的基础封装 JavaEE http://git.oschina.net/596392912/jfinal-oauth2.0-login eova J2EE快速开发平台,基于JFinal Maven http://git.oschina.net/eova/eova tiny 组件化的J2EE开发框架 Maven http://git.oschina.net/tinyframework/tiny JEECG 基于代码生成器的智能开发平台 Maven http://git.oschina.net/jeecg/jeecg AOS 标准功能可复用、通用模块可配置 JavaEE https://git.oschina.net/osworks/AOS Lemon OA 基于Java开发的开源OA Maven http://www.mossle.com/index.do EasyDarwin 一款开源流媒体平台框架 复合 http://www.easydarwin.org/ Codetainer 基于浏览器上的代码运行沙箱 go https://github.com/codetainerapp/codetainer Gor Go HTTP 流量复制工具 go https://github.com/buger/gor NutzWk 基于Nutz的开源企业级开发框架 JavaEE https://github.com/Wizzercn/NutzWk http://www.voome.cn/home/index.shtml API接口开发管理 名称 描述 类型 地址 apiary https://apiary.io/ jQuery.ajaxMock js RAP 管理接口文档,生成Mock数据 java http://thx.github.io/RAP AnyProxy 一个开放式的HTTP/HTTPS代理，你可以灵活控制各种网络数据 node.js http://anyproxy.io/ mock-api 线上restful api的模拟平台 http://mock-api.com/ 机器学习 名称 代码 官网 语言 许可证 TensorFlow https://github.com/tensorflow/tensorflow http://tensorflow.org/ 三星的Veles https://github.com/samsung/veles https://velesnet.ml/ DMTK https://github.com/Microsoft/DMTK http://www.dmtk.io/ Marvin https://github.com/PrincetonVision/marvin http://marvin.is/ SystemML Caffe https://github.com/BVLC/caffe Theano https://github.com/Theano/Theano Torch https://github.com/torch/torch7 Brainstorm https://github.com/IDSIA/brainstorm Chainer https://github.com/pfnet/chainer Deeplearning4j https://github.com/deeplearning4j/deeplearning4j http://deeplearning4j.org/ Marvin https://github.com/PrincetonVision/marvin ConvNetJS https://github.com/karpathy/convnetjs MXNet https://github.com/dmlc/mxnet Neon https://github.com/NervanaSystems/neon Scikit-Learn https://github.com/scikit-learn/scikit-learn PredictionIO https://github.com/PredictionIO/PredictionIO Brain https://github.com/harthur/brain Keras https://github.com/fchollet/keras CNTK https://github.com/Microsoft/CNTK Pattern https://github.com/clips/pattern NuPIC https://github.com/numenta/nupic Vowpal Wabbit https://github.com/JohnLangford/vowpal_wabbit Ruby Warrior https://github.com/ryanb/ruby-warrior XGBoost https://github.com/dmlc/xgboost C++ Apache-2.0 license GoLearn https://github.com/sjwhitworth/golearn go ML_for_Hackers https://github.com/johnmyleswhite/ML_for_Hackers R H2O-2 https://github.com/h2oai/h2o-2 Java Oryx 2 https://github.com/cloudera/oryx Java Shogun https://github.com/shogun-toolbox/shogun C/C++、Python GPLv3 HLearn https://github.com/mikeizbicki/HLearn Haskell MLPNeuralNet https://github.com/nikolaypavlov/MLPNeuralNet Objective-C BSD license Apache Mahout https://github.com/apache/mahout Java Apache Seldon Server https://github.com/SeldonIO/seldon-server Java Datumbox - Framework https://github.com/datumbox/datumbox-framework Java Apache License 2.0 Jubatus https://github.com/jubatus/jubatus C/C++ LGPL Decider https://github.com/danielsdeleo/Decider Ruby 移动开发 名称 描述 官网 代码 授权协议 Bugly 移动应用崩溃检测服务 http://bugly.qq.com/ http://bugly.qq.com/whitebook Bugtags 移动测试 https://bugtags.com/ http://help.bugtags.com/hc/ Calabash 自动化测试框架 Appium 动化测试框架 Robotium 开源的测试框架 Frank 自动化测试框架 UIAutomator 测试框架 notie.js javascript通知插件 react-calculator 基于react.js实现的计算器 http://blog.benoitvallon.com/react-native-nw-react-calculator/#/?_k=tu4ugv weui 基于微信的UI库 http://weui.github.io/weui/ Loaders.css loader动画库 WeX5 开源、免费的应用快速开发框架 http://wex5.com/cn/wex5/ Agile Lite 基于 HTML5+CSS3+JS 的移动应用开发框架 http://git.oschina.net/nandy007/agile-lite mui 高性能App的框架 https://github.com/dcloudio/mui https://github.com/dcloudio/mui MIT Dexposed Xposed AOP 框架 https://github.com/alibaba/dexposed Apache Artemis 超高性能的 Java 对象消息服务器 http://activemq.apache.org/artemis/ GT（随身调） 直接运行在手机上的“集成调测环境” http://gt.qq.com/ https://github.com/TencentOpen/GT 图表 名称 官网 代码 Plotly jqPlot http://www.jqplot.com/ MPLD3 http://mpld3.github.io/ Bokeh https://www.continuum.io/bokeh Highcharts http://www.highcharts.com/ Flotcharts https://github.com/flot/flot MetricsGraphics http://metricsgraphicsjs.org/ Vega https://github.com/vega/vega Chartjs http://www.chartjs.org/ gRaphaël’s http://g.raphaeljs.com/ https://github.com/DmitryBaranovskiy/g.raphael/tree/master ECharts-X http://echarts.baidu.com/ https://github.com/ecomfe/echarts Mapv http://mapv.baidu.com/ https://github.com/huiyan-fe/mapv/ js库 名称 描述 官网 代码 raphael.js 通过SVG/VML+JS实现跨浏览器的矢量图形实现方案 http://raphaeljs.com/ https://github.com/DmitryBaranovskiy/raphael/ Editor.md Nuclear 形似 React 的超轻量级框架 http://alloyteam.github.io/Nuclear/ vConsole 前端调试面板看log https://github.com/WechatFE/vConsole 工作流 名称 描述 官网 代码 Snaker FoxBPM 吸纳了 jBPM3 、 Activiti5、BonitaBPM 等国际开源流程引擎的精髓 https://github.com/fixteam/fixflow bpmn-js BPMN2.0渲染工具包和Web模型 http://bpmn.io/ https://github.com/bpmn-io/bpmn-js myflow 基于raphael.js 一个可嵌入的开源 Markdown 在线编辑器组件 https://pandao.github.io/editor.md/ JFlow 具有.net与java两个版本 http://jflow.cn/ http://git.oschina.net/chichengsoft/JFlow 爬虫 名称 描述 官网 代码 jsoup 用于解析HTML的Java 类库 http://jsoup.org/ https://github.com/jhy/jsoup/ Pholcus 一款纯Go语言编写的高并发、分布式、重量级爬虫软件 https://pholcus.gitbooks.io/docs/ https://github.com/henrylee2cn/pholcus 八爪鱼 http://www.bazhuayu.com/ import.io http://import.io 集搜客 http://www.gooseeker.com/ Bug跟踪工具 名称 描述 官网 Redmine Bugzilla Trac Mantis]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbases实践]]></title>
    <url>%2Fbigdata%2Fhbase%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[What 非关系型数据库 高可靠性 高性能 面向列 可伸缩 实时读写 分布式 对比 关系型数据库的优点 容易理解 使用方便 易于维护 关系型数据库的瓶颈 高并发读写需求 海量数据的读写性能低 扩展性和可用性差 数据库模型 row key 不能重复 字典顺序排序 只能存储64k的字节 列族 小于等于3个 列名 以列族作为前缀 可动态加入 celll单元格 有版本 字节数组 1&#123;row key， column( =&lt;family&gt; +&lt;qualifier&gt;)， version&#125; 时间戳 默认是1 时间倒序排序，最新最前 64位整型 默认精确到毫秒，可以主动设置 Hlog (wal log) HLogkey 数据归属信息 table region sequence number timestamp value 就是hbase的 keyvale对象 体系架构 client zookeeper 保证集群只有一个master 存储 region的寻址入口 监控region master 负责负载均衡 发现失效region server 重新分配region 管理用户CRUD Region server 维护region region d store 一个store对应一个CF(列族) memstore storefile 高表与宽表的选择 查询性能 高表更好 分片能力 高表更细 元数据开发 高表更大 rowkey 多，region 多 meta 数据量大 事务能力 宽表事务性更好 数据压缩比 宽表更高 Where http://hbase.apache.org Why How squirrel sql client cd /opt/soft wget http://mirrors.cnnic.cn/apache/hbase/0.98.24/hbase-0.98.24-hadoop2-bin.tar.gz tar zxvf hbase-0.98.24-hadoop2-bin.tar.gz vi ~/.bash_profile 增加 12export HBASE_HOME=/app/hbase-0.96.2export PATH=$PATH:$HBASE_HOME/bin 发送打其它节点 12scp ~/.bash_profile root@sj-node2:/rootscp ~/.bash_profile root@sj-node3:/root source ~/.bash_profile 生效 vi /opt/soft/hbase-0.98.24-hadoop2/conf/hbase-env.sh 修改 123export JAVA_HOME=/opt/soft/jdk1.7.0_25export HBASE_CLASSPATH=/opt/soft/hadoop-2.5.1export HBASE_MANAGES_ZK=false vim /opt/soft/hbase-0.98.24-hadoop2/conf/hbase-site.xml 123456789101112&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://appcity:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sj-node1,sj-node2,sj-node3&lt;/value&gt;&lt;/property&gt; vim /opt/soft/hbase-0.98.24-hadoop2/conf/regionservers 12sj-node1sj-node2 vim /opt/soft/hbase-0.98.24-hadoop2/conf/backup-masters 1sj-node2 拷贝hdfs的配置文件到hbase配置目录下 cp /opt/soft/hadoop-2.5.1/etc/hadoop/hdfs-site.xml /opt/soft/hbase-0.98.24-hadoop2/conf/ 将配置好的hbase发送到其它节点 scp -r /opt/soft/hbase-0.98.24-hadoop2 root@sj-node2:/opt/soft/ scp -r /opt/soft/hbase-0.98.24-hadoop2 root@sj-node3:/opt/soft/ 在sj-node1节点上启动 start-hbase.sh 在webL浏览器访问 http://sj-node1:60010 试下ha高可用 kill 调用sj-node1上的master 自动切换完成 重新启动sj-node1节点 的hbase则变为backup 命令 打开命令行 hbase shell help list create 'testhbase','cf1' put 'testhbase','rk00001','cf1:id','001' scan 'testhbase' get 'testhbase','rk00001' disable 'testhbase delete 'testhbase','rk00001','cf1:id' 预分区 在创建HBase表的时候默认一张表只有一个region，所有的put操作都会往这一个region中填充数据，当这个一个region过大时就会进行split。如果在创建HBase的时候就进行预分区则会减少当数据量猛增时由于region split带来的资源消耗。 create 'testsplit','cf1',SPLITS =&gt; ['000','100','200'] put 'testsplit','002','cf1:id','002d' java api rowkey 设计 创建删除表 put 数据 单个或多个 get 数据 scan 起始row 终止row 过滤器 前缀 列值（字符串比较） 中文乱码 new String(rs.getRow(),&quot;utf-8&quot;) Bytes.toString(CellUtil.cloneValue(cell).replace(&quot;\x&quot;,&quot;%&quot;),&quot;utf-8&quot;) 优化 表的设计 预分区 row key 长度原则 定长 越小越好 2的幂次方 散列性 取反 hash 高位随机串 唯一性 保证key的唯一 根据实际业务来 列族不能超过3个 setinmemory 大部分能提高效率 TTL 存储时长 compaction 合并默认值 majorcompaction =24 jetter = 0.2 写表操作 多表同时写 htable参数设置 auto flush write buffer wal 批量写 多线程并发写 map reduce 写 读表操作 多htable 并发读 scan 指定列族 关闭result 批量读 blockcache 问题参考 解决Hbase启动后，hmaster会在几秒钟后自动关闭]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark实战]]></title>
    <url>%2Fbigdata%2FSpark%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[What ? Apache Spark 一站式处理方案 sql 离线计算 实时流 机器学习 图计算 &lt;!-- more --&gt; One stack rule them all stream processiong 流式计算 ad hoc queries 热查询 batch processing 批处理 spark的shuffle 当超过一定量之后也会 落地到磁盘 产生 disk io 4种运行模式 local eclipse中用 standalone 自己的集群 client cluster mesos yarn client cluster RDD是基础 resilient distributed dataset 弹性分布式数据集 五大特性 a list of partitions RDD是由一系列的partition组成的。 a function for computing each split函数是作用在每一个partition(split)上的。 list of dependencies on othre RDDsRDD之间有一系列的依赖关系。 optionally, a partitioner for key-value RDDs分区器是作用在K,V格式的RDD上。 optionally, a list of preferred locations to compute each split onRDD提供一系列最佳的计算位置。 主从 mapreduce: RM NM HDFS : NN DM JOB TASK spark ：master worker 容错 重算 根据 lineage persist() 持久化 checkpoint 存磁盘 依赖 窄依赖 narrow 当个rdd转变为一个rdd 不实时处理 lazy 处理 等 map union 宽依赖 wide groupby join 术语 application --&gt; jobs --&gt; stages--&gt; tasks cluster--&gt;WNs --&gt; exeactors --&gt;threads cluster manager 在集群上获取资源的外部服务 （standalone,Mesos,Yarn) master standalone管理资源的主节点 cluster manager 在集群上获取外部资源的服务 worker node standalone 运行于从节点，管理从节点资源 application 基于spark的程序，包含了driver和executor worker executor 默认分配内存1G dirver Driver 负责应用程序资源的申请 任务的分发。 连接worker executor worker节点为application启动的一个进程 负责执行任务， 每个application都有独立的executor task 被送到executor上的工作单元 job task 的并行计算，相当于action stage job被拆分后的单元 pipline 算子 transfomation repartition coalsce action reduce shuffle HashShuffle 普通机制 每一个maptask将不同结果写到不同的buffer中，每个 buffer 的大小为 32K。buffer 起到数据缓存的作用。 每个buffer文件最后对应一个磁盘小文件。 reduce task 来拉取对应的磁盘小文件。 产生的磁盘小文件的个数: M(map task 的个数)*R(reduce task 的个数) 合并机制 SortShuffle 磁盘文件个数 ： 2 * M 普通机制 bypass机制 bypass运行机制的触发条件如下: shuffle reduce task 的 数 量 小 于 spark.shuffle.sort.bypassMergeThreshold 的参数值。这个 值默认是 200。 产生的磁盘小文件为:2*M(map task 的个数) 文件寻址 当maptask执行完成后，会将task的执行情况和磁盘小文件的 地址封装到 MpStatus 对象中，通过 MapOutputTrackerWorker 对象向 Driver 中的 MapOutputTrackerMaster 汇报。 在所有的maptask执行完毕后，Driver中就掌握了所有的磁盘 小文件的地址。 在reducetask执行之前，会通过Excutor中 MapOutPutTrackerWorker 向 Driver 端的 MapOutputTrackerMaster 获取磁盘小文件的地址。 获取到磁盘小文件的地址后，会通过BlockManager中的 ConnectionManager 连接数据所在节点上的 ConnectionManager,然后通过 BlockTransferService 进行数 据的传输。 BlockTransferService 默认启动 5 个 task 去节点拉取数据。默 认情况下，5 个 task 拉取数据量不能超过 48M。 内存管理 静态内存管理 1.6之前 application-&gt;job-&gt;stage-&gt;task(pipline) 统一内存管理 1.6之后 分布式计算容易出现的问题 数据倾斜 groupbykey 大部分数据集中在某个key上 Why ? hadoop的共享数据慢 spark的共享数据快 快的原因：基于内存还有DAG 支持语言api Scala python java Where? http://spark.apache.org How? Spark代码流程 创建SparkConf对象 可以设置 Application name。 可以设置运行模式及资源需求。 创建SparkContext对象 基于Spark的上下文创建一个RDD，对RDD进行处理。 transformation转换 RDD &gt; RDD 缓存RDD，避免多次读取 应用程序中要有Action类算子来触发Transformation类算子执行。 action执行 RDD &gt; 结果 关闭Spark上下文对象SparkContext。 选择缓存策略 Cache() MEMORY_ONLY (默认) MEMORY_AND_DISK OFF_HEAP DISK_ONLY NONE 12345678910111213141516171819202122232425262728293031323334public static void main (String[] args)&#123; SparkCon conf = new SparkConf().setAppName("WordCound").setMaster("local[*]"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRdd(String) lines = sc.textFile("CHANGES.txt"); // JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String,String&gt;()&#123; @Override public Iterable&lt;String&gt; call(String line) throws Exception&#123; return Arrays.asList(line.split(" ")); &#125; &#125;); // JavaPairRDD&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String,String,Integer&gt;()&#123; @Overridepublic Tuple2&lt;String,Integer&gt; call(String word) throws Exception&#123; return new Tuple2&lt;String,Integer&gt;(word,1); &#125; &#125;); JavaPairRDD&lt;Stirng, Integer&gt; results = pairs.reduceByKey(new Function2&lt;Integer,Integer,Integer&gt;()&#123; @Override public Integer call (Integer v1, Integer v2)throws Exception&#123; return v1 + v2; &#125; &#125;) results.sortByKey().results.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;()&#123; @Override public void all(Tuple2&lt;Sting,Integer tuple) throws Exception&#123; System.out.println(tuple._1 + "" + tuple._2); &#125; &#125;) sc.close;&#125; 任务调度 spark 提交应程序的机器 shell中 spark submit 脚本提交程序 启动一个driver 进程 spark submit driver actor spark context 对象 构造DAGschedule taskschedule DAGscheduler taskschedule task set 打散 taskschedule 向master注册application master接到请求，启动多个exccutor worker接受分配 DAGScheduler 切分job stage taskset taskschedule 把taskset中task提交到executer上执行 local 单机模式 本地直接运行调试 安装完全分布式 standalone 下载 http://spark.apache.org/downloads.html tar zxvf spark-1.6.3-bin-hadoop2.6.tgz 修改配置添加从节点 123cd spark-1.6.3-bin-hadoop2.6cp slaves.template slavesvi slaves 去掉localhost，添加从节点 名称 vi spark-env.sh 下面这3个值更加自己情况修改 123SPARK_MASTER_IP=sj-node1SPARK_MASTER_PORT=7077SPARK_WORKER_MEMORY=1g 发送到从节点 123scp -r spark-1.6.3-bin-hadoop2.6 root@sj-node2:`pwd`scp -r spark-1.6.3-bin-hadoop2.6 root@sj-node3:`pwd`` 启动集群 /opt/soft/spark-1.6.0-bin-hadoop2.4/sbin/start-all.sh 打开Spark WEBUI 界面查看检查 默认8080端口可能与别的应用冲突，可以修改vim /opt/soft/spark-1.6.0-bin-hadoop2.4/sbin/start-master.sh 也可以修改环境变量 运行example 求 PI standalone任务提交方式 client client模式提交任务后，会在客户端启动Driver进程。 Driver会向Master申请启动Application启动的资源。 资源申请成功，Driver端将task发送到worker端执行。 worker将task执行结果返回到Driver端。 123./spark-submit--master spark://node1:7077 --deploy-mode client--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100 cluster cluster模式提交应用程序后，会向Master请求启动Driver. Master接受请求，随机在集群一台节点启动Driver进程。 Driver启动后为当前的应用程序申请资源。 Driver端发送task到worker节点上执行。 worker将执行情况和执行结果返回给Driver端 123./spark-submit--master spark://node1:7077 --deploy-mode cluster--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100 安装yarn运行模式 vi /opt/soft/spark-1.6.0-bin-hadoop2.4/conf/spark-env.sh 添加hadoop的路径即可 yarn任务提交方式 client 客户端提交一个Application，在客户端启动一个Driver进程。 应用程序启动后会向RS(ResourceManager)发送请求，启动 AM(ApplicationMaster)的资源。 RS收到请求，随机选择一台NM(NodeManager)启动AM。这 里的 NM 相当于 Standalone 中的 Worker 节点。 AM启动后，会向RS请求一批container资源，用于启动Executor. RS会找到一批NM返回给AM,用于启动Executor。 AM会向NM发送命令启动Executor。 Executor启动后，会反向注册给Driver，Driver发送task到 Executor,执行情况和结果返回给 Driver 端。 1234./spark-submit--master yarn–client--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100 cluster 客户机提交Application应用程序，发送请求到 RS(ResourceManager),请求启动 AM(ApplicationMaster)。 RS收到请求后随机在一台NM(NodeManager)上启动AM(相 当于 Driver 端)。 AM启动，AM发送请求到RS，请求一批container用于启动 Excutor。 RS返回一批NM节点给AM。 AM连接到NM,发送请求到NM启动Excutor。 Excutor反向注册到AM所在的节点的Driver。Driver发送task 到 Excutor。 12345./spark-submit--master yarn-cluster--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100` 广播变量 123456val conf = new SparkConf() conf.setMaster("local").setAppName("brocast")val sc = new SparkContext(conf)val list = List("hello xasxt")val broadCast = sc.broadcast(list)val lineRDD = sc.textFile("./words.txt")lineRDD.filter &#123; x =&gt; broadCast.value.contains(x) &#125;.foreach &#123; println&#125; sc.stop() 累加器 1234val conf = new SparkConf() conf.setMaster("local").setAppName("accumulator")val sc = new SparkContext(conf)val accumulator = sc.accumulator(0) sc.textFile("./words.txt").foreach &#123; x =&gt;&#123;accumulator.add(1)&#125;&#125; println(accumulator.value)sc.stop() SparkUI SparkUI界面介绍 配置historyServer 临时配置，对本次提交的应用程序起作用 shell ./spark-shell --master spark://node1:7077 --name myapp1 --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=hdfs://node1:9000/spark/test 停止程序，在 Web Ui 中 Completed Applications 对应的 ApplicationID 中能查看 history spark-default.conf配置文件中配置HistoryServer，对所有提交的 Application 都起作用 vi ../spark-1.6.0/conf/ spark-defaults.conf 12345678//开启记录事件日志的功能spark.eventLog.enabled true//设置事件日志存储的目录spark.eventLog.dir hdfs://node1:9000/spark/test//设置 HistoryServer 加载事件日志的位置spark.history.fs.logDirectory hdfs://node1:9000/spark/test//日志优化选项,压缩日志spark.eventLog.compress true 启动 HistoryServer: ./start-history-server.sh 访问 HistoryServer:node4:18080,之后所有提交的应用程序运行状况都会被记录。 Master HA 高可用 zookeeper 有选举和存储功能，可以存储 Master 的元素据信息，使用 zookeeper 搭建的 Master 高可用，当 Master 挂掉时，备用的 Master 会自动切换，推荐使用这种方式搭建 Master 的 HA。 配置spark-env.sh，并发送到所有节点 vim /opt/soft/spark-1.6.0-bin-hadoop2.4/conf/spark-env.sh 1export SPARK_DAEMON_JAVA_OPTS=" -Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node3:2181,node4:2181,node5:2181 -Dspark.deploy.zookeeper.dir=/sparkmaster0821 找一台节点(非主 Master 节点)配置备用 Master,修改 spark-env.sh 配置节点上的 MasterIP SPARK_MASTER_IP=sj-node2 启动zookeeper集群 启动spark集群 sbin/start-all.sh 启动备用master sbin/start-daemon.sh 主备切换过程中不能提交 Application。主备切换过程中不影响已经在集群中运行的 Application。因为 Spark 是粗粒度资源调度。 优化 取top(N) 用手写排序，可以提高效率 资源调优 spark集群配置默认参数conf/spark-env.sh 提交application时分配资源 动态资源分配 并行度调优 hdfs的block数据就是并行度，调低block大小相当于增加并行度，也可以读的时候指定 sc.parallelize(xxx, numPartitions) sc.makeRDD(xxx, numPartitions) sc.parallelizePairs(xxx, numPartitions) repartions/coalesce redecByKey/groupByKey/join ---(xxx, numPartitions) spark.default.parallelism net set spark.sql.shuffle.partitions---200 自定义分区器 如果读取数据是在SparkStreaming中 Receiver: spark.streaming.blockInterval—200ms Direct:读取的 topic 的分区数 代码调优 避免创建重复rdd 复用同一个RDD 对多次使用的RDD进行持久化 cache persist checkpoint 尽量避免使用shuffle类的算子 join 算子=广播变量+filter、广播变量+map、广播变量+flatMap map side 预聚合的shuffle操作 combine好处 降低 shuffle write 写磁盘的数据量。 降低 shuffle read 拉取数据量的大小 降低reduce端聚合的次数。 有 combiner 的 shuffle 类算子 reduceByKey aggregateByKey combineByKey 尽量使用高性能的算子 使用 reduceByKey 替代 groupByKey - 使用 mapPartition 替代 map 使用 foreachPartition 替代 foreach - filter 后使用 coalesce 减少分区数 使用使用 repartitionAndSortWithinPartitions 替代 repartition 与 sort 类操作 使用 repartition 和 coalesce 算子操作分区 使用广播变量 使用Kryo优化序列化性能 优化数据结构 尽量使用字符串替代对象，使用 原始类型(比如 Int、Long)替代字符串，使用数组替代集合类型 使用高性能的库 fastutil 数据本地化 级别 PROCESS_LOCAL NODE_LOCAL NO_PREF RACK_LOCAL ANY Spark数据本地化调优 spark.locality.wait 参考 http://spark.apache.org/docs/1.6.0/configuration.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala入门]]></title>
    <url>%2Fml%2FScala%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[What ? 函数式编程（erlang，也是） 基于jvm 面向函数的编程语言 运行于JVM,与java代码混编 类型推测(自动推测类型) 并发和分布式(Actor) 特质，特征(类似 java 中 interfaces 和 abstract 结合) 模式匹配(类似 java switch 高阶函数 &lt;!-- more --&gt; 基本类型 byte short int long float double char string boolean unit 表示 无值 相当于 void null 空引用 nothing 所有其它类型的子类型 无任何东西 any 所有类型的超类 anyref 所有引用类型的超类 anyval 所有值类型的超类 变量常量声明 var val 常量不可再赋值 Where ? 官网 http://www.scala-lang.org/download/2.10.4.html Why? 为何学习scala 用Scala写的软件 jstorm 语言与产品 Scala - spark kafka Go - Docker Erlang - RabbitMQ clojure - Storm How? 环境搭建 brew install Scala brew install scalaenv scalaenv install intellj ideal 安装 Scala 插件 jdk 语法 集合 list map foreach distinct sortby slice /: = foldLeft reduce = redeceLeft filter take 元组 tuple 伴生对象 object 静态方法 静态变量 相当于单例 伴生类 class 非静态方法 非静态变量 特征特性 trait case class 模式匹配 隐式转换 impliit 方法 参数 高阶函数 函数的 参数 依然是个 函数 柯里化 函数参数 用 括号分割， 类和对象 注意点: 建议类名首字母大写 ，方法首字母小写，类和方法 命名建议符合驼峰命名法。 scala 中的 object 是单例对象，相当于 java 中的工具 类，可以看成是定义静态的方法的类。object 不可 以传参数。另:Trait 不可以传参数 scala 中的 class 类默认可以传参数，默认的传参数就 是默认的构造函数。 重写构造函数的时候，必须要调用默认的构造函数。 class 类属性自带 getter ，setter 方法。 使用 object 时，不用 new,使用 class 时要 new ,并且 new 的时候，class 中除了方法不执行，其他都执行。 如果在同一个文件中，object 对象和 class 类的名称 相同，则这个对象就是这个类的伴生对象，这个类 就是这个对象的伴生类。可以互相访问私有变量。 条件 if if else if else if else 循环 to 和 until for for (i &lt;- 1 to 10) 返回值yield var tmp = for ( i &lt;- 1to 10) yield i 嵌套循环 for (i &lt;- 1 to 10; j &lt;- 1 to 10) 条件过滤 for(i &lt;- 1 to 10; if(布尔表达式)) while do {循环体} while(布尔表达式) while(布尔表达式){循环体} 函数的定义 有参函数 无参函数 递归函数 嵌套函数 匿名函数 偏函数 高阶函数 柯理化函数 注意点: 函数定义语法 用 def 来定义 可以定义传入的参数，要指定传入参数的类型 方法可以写返回值的类型也可以不写，会自动推断，有 时候不能省略，必须写，比如在递归函数中或者函数的 返回值是函数类型的时候。 scala 中函数有返回值时，可以写 return，也可以不写 return，会把函数中最后一行当做结果返回。当写 return 时，必须要写函数的返回值。 如果返回值可以一行搞定，可以将{}省略不写 传递给方法的参数可以在方法中使用，并且 scala 规定 方法的传过来的参数为 val 的，不是 var 的。 如果去掉方法体前面的等号，那么这个方法返回类型必 定是 Unit 的。这种说法无论方法体里面什么逻辑都成 立，scala 可以把任意类型转换为 Unit.假设，里面的逻 辑最后返回了一个 string，那么这个返回值会被转换成 Unit，并且值会被丢弃。 字符串 集合 数组 list set map 元组 特征trait 模式]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习]]></title>
    <url>%2Fml%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[What 已有的数据 某种模型 利用此模型预测未来 机器学习界“数据为王”思想 历史 Python R 第一代工具 单机 Mahaout MR 第二代工具 分布式 Spark MLlib 第三代工具 分布式 迭代 H2O Flink 历史总是惊人的相似 体现 计算：云计算 推理：专家系统 灵敏：事件驱动 知识：数据仓库 检索：搜索引擎 智慧：机器学习 用途 分类 预测 聚类 推荐 算法分类 有监督 学习（特征-features 样本-example 有标签） 分类(做成单一决策) 逻辑回归 多元线性回归 推荐(选择许多可能，并排序) 预测 无监督 学习（有特征 无标签） 聚类 名词 建模 训练集 测试集 Where Why 做机器学习的大公司 百度 谷歌 AlphaGo 脸书 How 收集数据 准备输入数据 分析输入数据 训练算法 测试算法 使用算法 脑图 # 机器学习 ## 算法 ### 线性回归Liner Regression ![](http://wntc-1251220317.cossh.myqcloud.com/2019/1/7/1546847995671.png) #### 误差函数/损失函数 ![](http://wntc-1251220317.cossh.myqcloud.com/2019/1/7/1546849084076.png) ##### 存在最小值 ### 回归 #### 多元线性回归 ##### 梯度下降法 ## 历史 ### 1. Python R 第一代工具 单机 ### 2. Mahaout MR 第二代工具 分布式 ### 3. Spark MLlib 第三代工具 分布式 迭代 ### 4. H2O ### 5. Flink ## 体现 ### - 计算：云计算 ### - 推理：专家系统 ### - 灵敏：事件驱动 ### - 知识：数据仓库 ### - 检索：搜索引擎 ### - 智慧：机器学习 ## 用途 ### - 分类 ### - 预测 ### - 聚类 ### - 推荐 ## 算法分类 ### - 有监督 学习（特征-features 样本-example 有标签） #### - 分类(做成单一决策) ##### - 逻辑回归 ##### - 多元线性回归 #### - 预测 #### - 推荐(选择许多可能，并排序) ### - 无监督 学习（有特征 无标签） #### - 聚类 ## 做机器学习的大公司 ### - 百度 ### - 谷歌 AlphaGo ### - 脸书]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop实践之MapReduce]]></title>
    <url>%2Fbigdata%2FHadoop%E4%B9%8BMapReduce%2F</url>
    <content type="text"><![CDATA[What 分布式离线计算框架 MapReduce On YARN：MRv2 移动计算而不是移动数据 一主多从架构 JobTracker TaskTracker NameNode DateNode ResourceManager 负责整个集群的资源管理和`调度 container 一个container一个进程 ApplicationMaster：负责应用程序相关的事务，比如任务调度、任务监控和容错等。 将MapReduce作业直接运行在YARN上，而不是由JobTracker和TaskTracker构建的MRv1系统中 基本功能模块 YARN：负责资源管理和调度 MRAppMaster：负责任务切分、任务调度、任务监控和容错等 MapTask/ReduceTask：任务驱动引擎，与MRv1一致 每个MapRduce作业对应一个MRAppMaster任务调度 YARN将资源分配给MRAppMaster MRAppMaster进一步将资源分配给内部的任务 MRAppMaster容错 失败后，由YARN重新启动 任务失败后，MRAppMaster重新申请资源 Zookeeper Failover Controller： 监控NameNode健康状态，并向Zookeeper注册NameNode，NameNode挂掉后，ZKFC为NameNode竞争锁，获得ZKFC 锁的NameNode变为active 细节 input split 对应一个 block 128M output 可能大于 一个block 128M map 到 reduce 方法之间 叫 shuffle map 在相应的datanode上执行，reduce找空闲的datanode执行，不一定在block上 split后的大小不一定是一个block大小 map的输出物是经过快速排序的 &lt;div id=&quot;flowchart-0&quot; class=&quot;flow-chart&quot;&gt;&lt;/div&gt; partition hash(key) mod r 有几个reduce 就有几个partition shuffle map task &lt;div id=&quot;flowchart-1&quot; class=&quot;flow-chart&quot;&gt;&lt;/div&gt; Why 海量数据计算 Where http://hadoop.apache.org/docs/r2.5.2/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html http://www.cnblogs.com/luogankun/p/4019303.html https://note.youdao.com/share/?id=95d458d779f8e9f391d6ea06b6c6d122&amp;type=note#/ https://note.youdao.com/share/?id=86ca5c96d13413f789164ff92f9ab4f9&amp;type=note#/ https://note.youdao.com/share/?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;type=note#/ https://note.youdao.com/share/?id=a518dfed10d824b0995380669ddd28c9&amp;type=note#/ How 配置文件yarn-site.xml 123456789101112131415161718192021222324252627282930 &lt;configuration&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;appcity&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;sj-node3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;sj-node4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;sj-node1:2181,sj-node2:2181,sj-node3:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml使用yarn来调度 cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml vi etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 将配置文件拷贝到其它机器上 123456scp etc/hadoop/yarn-site.xml sj-node2:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/yarn-site.xml sj-node3:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/yarn-site.xml sj-node4:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/mapred-site.xml sj-node2:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/mapred-site.xml sj-node3:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/mapred-site.xml sj-node4:/opt/soft/hadoop-2.5.1/etc/hadoop/ 由于配置了3，4节点为yarn的主节点，所以进入3或4执行命令 ./sbin/start-yarn.sh 如果某个节点故障 kill掉之后 执行单独启动命令./sbin/yarn-daemon.sh start resourcemanager 检查 http://sj-node3:8088/cluster 能够访问代表主节点启动成功 http://sj-node4:8088/cluster 能够访问并自动跳转到node3代表热备机启动成功 &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js&quot;&gt;&lt;/script&gt;st=>start: input op1=>operation: spliting op2=>operation: mapping op3=>operation: shuffing op4=>operation: reducing ed=>inputoutput: final result st->op1(right)->op2(right)->op3(right)->op4(right)->ed{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: input split op1=>operation: map op2=>operation: buffer in memory op3=>operation: partion sort op4=>operation: spill to disk op5=>operation: merge on disk op6=>operation: fetch to reduce op7=>operation: sort and merge partition ed=>inputoutput: final result st->op1(right)->op2(right)->op3(right)->op4(right)->op5(right)->ed{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>ha</tag>
        <tag>高可用</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2实践之HDFS进阶篇-高可用]]></title>
    <url>%2Fbigdata%2FHadoop%E4%B9%8Bhdfs%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[What Hadoop 2.x由HDFS、MapReduce和YARN三个分支构 HDFS：分布式文件存储系统； YARN：资源管理系统 MapReduce：运行在YARN上的MR； Hadoop 2.0产生背景 Hadoop 1.0中HDFS和MapReduce在高可用、扩展性等方面存在问题。 HDFS存在的问题 NameNode单点故障，难以应用于在线场景 NameNode压力过大，且内存受限，影响系统扩展性 MapReduce存在的问题 JobTracker访问压力大，影响系统扩展性 难以支持除MapReduce之外的计算框架，比如Spark、Storm等. Yarn 资源管理系统 ，由jobTracker 演化而来 资源管理 ResourceManager 任务调度 ApplicationMaster 资源管理调度流程 流程大致如下 client客户端向yarn集群(resourcemanager)提交任务 resourcemanager选择一个node创建appmaster appmaster根据任务向rm申请资源 rm返回资源申请的结果 appmaster去对应的node上创建任务需要的资源（container形式，包括内存和CPU） appmaster负责与nodemanager进行沟通，监控任务运行 最后任务运行成功，汇总结果。 其中Resourcemanager里面一个很重要的东西，就是调度器Scheduler，调度规则可以使用官方提供的，也可以自定义。 Where http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html Why zookeeper 选举机制 How hdfs-site.xml cd /opt/soft/hadoop-2.5.1/ vi etc/hadoop/hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;appcity&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.appcity&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.appcity.nn1&lt;/name&gt; &lt;value&gt;sj-node1:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.appcity.nn2&lt;/name&gt; &lt;value&gt;sj-node2:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.appcity.nn1&lt;/name&gt; &lt;value&gt;sj-node1:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.appcity.nn2&lt;/name&gt; &lt;value&gt;sj-node2:50070&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://sj-node2:8485;sj-node2:8485;sj-node4:8485/appcity&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.appcity&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/soft/hadoop-2.5.1/data/jn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; core-site.xml vim etc/hadoop/core-site.xml 1234567891011121314151617181920212223242526&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://appcity&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://appcity&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;sj-node2:2181,sj-node3:2181,sj-node4:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;Indicates the number of retries a client will make to establish a server connection. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;description&gt;Indicates the number of milliseconds a client will wait for before retrying to establish a server connection. &lt;/description&gt;&lt;/property&gt; 复制配置文件 123456scp etc/hadoop/hdfs-site.xml sj-node2:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/hdfs-site.xml sj-node3:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/hdfs-site.xml sj-node4:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/core-site.xml sj-node2:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/core-site.xml sj-node3:/opt/soft/hadoop-2.5.1/etc/hadoop/scp etc/hadoop/core-site.xml sj-node4:/opt/soft/hadoop-2.5.1/etc/hadoop/ 删除masters cp ./etc/hadoop/masters ./etc/hadoop/masters.bak rm ./etc/hadoop/masters 删除之前存储的一些元信息和数据 譬如/opt/hadoop rm -rf data/dfs/ 启动JNs 至少三个节点 分别在2，3，4节点执行./sbin/hadoop-daemon.sh start journalnode 在其中一个namenode所在的节点来执行 ./bin/hdfs namenode -format 把刚才格式化好的那个namenode启动起来 ./sbin/hadoop-daemon.sh start namenode 在另外的其他你配置的namenode上面去执行 ./bin/hdfs namenode -bootstrapStandby 把刚才第8步启动那个namenode先停掉 ./sbin/hadoop-daemon.sh stop namenode 启动ZK集群 3台机器上执行/opt/soft/zookeeper-3.4.8/bin/zkServer.sh start 在其中某一台namenode节点上面来完成 ./bin/hdfs zkfc -formatZK 在其中某一台namenode节点上面来完成 启动集群 ./sbin/start-dfs.sh 启动在3或4节点上启动yarn yarn-daemon.sh start resourcemanager 有可能会出错的地方 确认每台机器防火墙均关掉 确认每台机器的时间是一致的 确认配置文件无误，并且确认每台机器上面的配置文件一样 如果还有问题想重新格式化，那么先把所有节点的进程关掉 删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录 接上面的第6步又可以重新格式化已经启动了 最终Active Namenode停掉的时候，StandBy可以自动接管！ 脑裂（brain-split） 脑裂是指在主备切换时，由于切换不彻底或其他原因，导致客户端和Slave误以为出现两个active master，最终使得整个集群处于混乱状态。解决脑裂问题，通常采用隔离(Fencing)机制，包括三个方面： 共享存储fencing：确保只有一个Master往共享存储中写数据。 客户端fencing：确保只有一个Master可以响应客户端的请求。 Slave fencing：确保只有一个Master可以向Slave下发命令。 Hadoop公共库中对外提供了两种fenching实现，分别是sshfence和shellfence（缺省实现），其中sshfence是指通过ssh登陆目标Master节点上，使用命令fuser将进程杀死（通过tcp端口号定位进程pid，该方法比jps命令更准确），shellfence是指执行一个用户事先定义的shell命令（脚本）完成隔离。 参考 https://blog.csdn.net/oaimm/article/details/38336089 问题 hdfs权限问题 动namenode失败，Error replaying edit]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop2</tag>
        <tag>ha</tag>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop实践之HDFS]]></title>
    <url>%2Fbigdata%2Fhadoop%E4%B9%8Bhdfs%2F</url>
    <content type="text"><![CDATA[What? Hadoop 生态 来自于 Google 03 年发布 3 大论文， GFS、mapreduce、Bigtable ;Dougcutting 用 Java 实现) HDFS: Hadoop Distributed File System 分布式存储系统 http://hadoop.apache.org high-throughput 高吞吐 Hadoop Common Hadoop Distributed File System (HDFS™) Hadoop YARN Hadoop MapReduce 组件 name node 主节点 接收客户端读写服务 接收datanode 汇报的block位置信息 保存metadate元信息（基于内存存储，不会和磁盘发生交换） 文件 owership(归属)和 permissions(权限) 文件大小 时间 Block 列表[偏移量]：即一个完整文件有哪些 block metadata 存储到磁盘文件名为”fsimage”的镜像文件 Block 的位置信息不会保存到 fsimage edits 记录对 metadata 的操作日志 secondary name node 整理 fsimage和 edites 日志 得到最新fsimage 避免nn重启后重新汇集所有文件meta信息 SNN执行合并时间和机制 根据配置文件设置的时间间隔 fs.checkpoint.period默认3600秒 根据配置文件设置 edits log 大小 fs.checkpoint.size 规定 edits 文件的最大值默认是 64MB data node 从节点 存储block 启动DN线程的时候会向NameNode汇报block位置信息 通过向NN发送心跳保持与其联系(3秒一次)，如果NN10分钟没有收到 DN 的心跳，则认为其已经 lost，并 copy 其上的 block 到其它 DN 存储模型block 偏移量标记位置 块大小默认hadoop1.x 是64M，hadoop2.x是128M（物理暂用空间按实际文件大小不会浪费） 默认情况下每个block 都有2个副本共3个副本 副本数不大于节点数 存放策略 存储方式 按大小被切分成若干个 block ，存储到不同节点上 放置策略 第一个副本:放置在上传文件的DN;如果是集群外提交，则随 第二个副本:放置在于第一个副本不同的机架的节点上。 第三个副本:与第二个副本相同机架的不同节点。 更多副本:随机节点 内含机制 心跳 负载均衡 多副本 Why? 分布式 容错 突破单机性能 效率高 可扩展 优点 分布式的特性: 适合大数据处理:GB 、TB 、甚至 PB 级及以上的数据 百万规模以上的文件数量:10K+ 节点。 适合批处理:移动计算而非数据(MR),数据位置暴露给计算框架 自身特性: 可构建在廉价机器上: 高可靠性:通过多副本提提高 高容错性:数据自动保存多个副本;副本丢失后，自动恢复,提供 了恢复机制 缺点 低延迟高数据吞吐访问问题 比如不支持毫秒级 吞吐量大但有限制于其延迟 小文件存取占用 NameNode 大量内存(寻道时间超过读取时间 (99%)) 不支持文件修改:一个文件只能有一个写者(深入) How? HDFS 读文件流程 通过拓扑网络寻找最近的节点 首先调用 FileSystem 对象的 open 方法，其实是一个 DistributedFileSystem 的实例。 DistributedFileSystem 通过 rpc 协议获得文件的第 一批 block 的 locations 地址，(同一个 block 按照重复数 会返回多个 locations，因为同一文件的 block 分布式存储 在不同节点上)，这些 locations 按照 hadoop 拓扑结构排 序，距离客户端近的排在前面(就近原则选择)。 前两步会返回一个 FSDataInputStream 对象，该对象 会被封装 DFSInputStream 对象，DFSInputStream 可以方便 的管理 datanode 和 namenode 数据流。客户端调用 read 方 法，DFSInputStream 最会找出离客户端最近的 datanode 并连接。 数据从 datanode 源源不断的流向客户端。 这些操作对客户端来说是透明的，客户端的角度看来只 是读一个持续不断的流。 如果第一批 block 都读完了,DFSInputStream 就会去namenode 拿下一批 block的locations，然后继续读，如 果所有的块都读完，这时就会关闭掉所有的流。如果在读数据的时候， DFSInputStream 和 datanode 的通讯发生异常，就会尝试正在读的 block 的排序第二近的 datanode,并且会记录哪个 datanode 发生错误，剩余的 blocks 读的时候就会直接跳过该 datanode。 DFSInputStream 也会检查 block 数据校验和，如果发现一个 坏的 block,就会先报告到 namenode 节点，然后 DFSInputStream 在其他的 datanode 上读该 block 的镜像。 该设计就是客户端直接连接 datanode 来检索数据并且 namenode 来负责为每一个 block 提供最优的 datanode， namenode 仅仅处理 block location 的请求，这些信息都加 载在 namenode 的内存中，hdfs 通过 datanode 集群可以承受 大量客户端的并发访问。 HDFS 写文件流程 客户端通过调用 DistributedFileSystem 的 create 方 法创建新文件。 DistributedFileSystem 通过 RPC 调用 namenode 去创 建一个没有 blocks 关联的新文件，创建前， namenode 会做 各种校验，比如文件是否存在，客户端有无权限去创建等。 如果校验通过， namenode 就会记录下新文件，否则就会抛 出IO异常。 前两步结束后，会返回 FSDataOutputStream 的对象， 与读文件的时候相似，FSDataOutputStream 被封装成 DFSOutputStream。 DFSOutputStream 可以协调 namenode 和 datanode。客户端 开始写数据到 DFSOutputStream，DFSOutputStream 会把数 据切成一个个小的 packet，然后排成队列 data quene。 DataStreamer 会去处理接受 data quene，它先询问 namenode 这个新的 block 最适合存储的在哪几个 datanode 里(比如重复数是3那么就找到3个最适合的 datanode)， 把他们排成一个管道pipeline 输出。DataStreamer 把packet按队列输出到管道的第一个 datanode 中，第一个datanode又把packet输出到第二个 datanode中，以此类推。 DFSOutputStream 还有一个对列叫 ack quene，也是 由 packet 组成等待 datanode 的收到响应，当 pipeline 中 的 datanode 都表示已经收到数据的时候，这时 ack quene 才会把对应的 packet 包移除掉。 如果在写的过程中某个 datanode 发生错误，会采取以下几步: pipeline 被关闭掉; 为了防止防止丢包。ack quene 里的 packet 会同步 到 data quene 里;创建新的 pipeline 管道怼到其他正常 DN 上 剩下的部分被写到剩下的两个正常的 datanode 中; namenode 找到另外的 datanode 去创建这个块的复 制。当然，这些操作对客户端来说是无感知的。 客户端完成写数据后调用 close 方法关闭写入流。 深入 DFSOutputStream 内部原理 是否按照block list 顺序读取block 信息？ 多节点并行读，单节点串读？ 安装 http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html 必备： java ssh 检查： echo JAVA_HOME ssh免密钥设置参照我的VMWare虚拟机笔记中的配置 防火墙是否关闭service iptables status 时间同步软件是否安装 下载 http://www.apache.org/dyn/closer.cgi/hadoop/common/ 下载至目录 /opt/soft 解压 tar zxvf hadoop-2.5.1_x64.tar.gz 修改配置 cd /opt/soft/hadoop-2.5.1 vi etc/hadoop/hadoop-env.sh export JAVA_HOME=/opt/soft/jdk1.7.0_25将jdk变量指向自己的jdk路径,保存退出 mkdir data vi etc/hadoop/core-site.xml添加下列配置 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://sj-node1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/soft/hadoop-2.5.1/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi ./etc/hadoop/hdfs-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;sj-node2:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;sj-node2:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改slavesvi ./etc/hadoop/slaves 新建masters文件vi ./etc/hadoop/masters以sj-node2作为第二主机 将配置好的传到其它节点 [root@sj-node1 soft]# scp -r hadoop-2.5.1 sj-node2:/opt/soft/ [root@sj-node1 soft]# scp -r hadoop-2.5.1 sj-node3:/opt/soft/ [root@sj-node1 soft]# scp -r hadoop-2.5.1 sj-node4:/opt/soft/ 格式化NN [root@sj-node1 hadoop-2.5.1]# bin/hdfs namenode -format 只需在NN几点也就是sj-node1上执行，确认没有错误,data目录下有文件生成 启动 [root@sj-node1 hadoop-2.5.1]# sbin/start-dfs.sh 注意检查启动的时候控制台有没有报错 启动后检查访问web 命令行检查 hdfs haadmin -getServiceState nn1 hdfs haadmin -getServiceState nn2 NN主节点 http://192.168.199.101:50070/ SNN http://192.168.199.102:50090/status.html 目录访问http://192.168.199.101:50070/explorer.html#/ 命令测试 创建文件夹 bin/hadoop fs -mkdir /testdir 上传文件 [root@sj-node1 hadoop-2.5.1]# bin/hadoop fs -put README.txt /testdir]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>ha</tag>
        <tag>高可用</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper实践]]></title>
    <url>%2Fbigdata%2FZookeeper%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[What? 保证分布式数据一致性 配置维护 域名服务 分布式同步 组服务 分布式锁 paxos 一致性协议 拜占庭将军问题 zab zookeeper atom broadcast 奇数个可以防止脑裂 dubbo &lt;!-- more --&gt; 角色 leader follower observer 提升吞吐量，不参与投票 znode节点 短暂的 持久的 短暂有序 持久有序 观察 Where http://zookeeper.apache.org/ Why? 特点 最终一致性 可靠性 实时性 独立性 原子性 顺序性 How? 准备3台虚拟机 在每台虚拟机上都添加hosts配置vi /etc/hosts 下载并配置http://www.apache.org/dyn/closer.cgi/zookeeper/ cd /opt/soft wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.8/zookeeper-3.4.8.tar.gz tar zxvf zookeeper-3.4.8.tar.gz cd zookeeper-3.4.8 mkdir data mkdir dataLog vi data/myid 输入1 并保存退出 cp conf/zoo_sample.cfg conf/zoo.cfg vi conf/zoo.cfg 1234567dataDir=/opt/soft/zookeeper-3.4.8/datadataLogDir=/opt/soft/zookeeper-3.4.8/dataLoginitLimit=5syncLimit=2server.1=sj-node1:2888:3888server.2=sj-node2:2888:3888server.3=sj-node3:2888:3888 单台配置完毕 将配置好的拷贝到其它两天虚拟机 scp -r zookeeper-3.4.8 root@sj-nod2:/opt/soft 登录第二台虚拟机，记得hosts文件已配置好 cd /opt/soft/zookeeper-3.4.8 vi data/myid 删除1 输入2 保存退出 启动zookeeper,在3台虚拟机上都输入 /opt/soft/zookeeper-3.4.8/bin/zkServer.sh start 查看集群状态 /opt/soft/zookeeper-3.4.8/bin/zkServer.sh status 主节点 从节点 设置观察节点 观察节点 客户端访问 /opt/soft/zookeeper-3.4.8/bin/zkCli.sh 列出所有1级节点 ls / 创建节点 create /zktest testdata 查询节点数据 get /zktest 设置节点数据 set /zktest testset 删除节点数据 delete /zktest 参考 https://www.cnblogs.com/shenguanpu/p/4048660.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop2</tag>
        <tag>ha</tag>
        <tag>高可用</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMWare安装CentOS6虚拟机笔记]]></title>
    <url>%2FLinux%2FVMWare%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[新建虚拟机后启动， &lt;!-- more --&gt; ifconfig 无ip vi /etc/sysconfig/network-scripts/ifcfg-eth0 service network restart 如果不能解析域名则 route add default gw 192.168.199.2 或者将其它能够正常访问的虚拟机的配置/etc/sysconfig/network-scripts/ifcfg-eth0拷贝过来后修改 删除生成的文件使重新生成 rm -rf /etc/udev/rules.d/70-persistent-net.rules 修改主机名（克隆过来的虚拟机需要改） vi /etc/sysconfig/network 把HOSTNAME的值修改成你想要的主机名 vi /etc/hosts 把localhost.localdomain替换成你的主机名 reboot hostname 启动后输入查看主机名 虚拟机的ip 修改的 ip要服务虚拟机的网段 ifconfig eth1:vip 192.168.57.180 netmask 255.255.255.0 ip addr del 命令 注释 service network restart 网卡重启 service iptables restart 重启防火墙 chkconfig iptables off 关闭防火墙 chkconfig iptables --list 防火墙列表 linux 级别 init 0 关闭 关机 shutdown reboot init 1 init 2 init 3 图形 init 4 init 5 init 6 重启 reboot 系统服务文件夹 /etc/init.d 切换yum源，使自动寻找最近的可用yum源码 12yum install -y epel-releaseyum clean all 若yum出现repomd.xml does not match metalink for epel vi /etc/yum.repos.d/epel.repo 注释掉 mirror list yum clean all wget http://yum.baseurl.org/download/3.4/yum-3.4.3.tar.gz 必备 1yum install gcc wget openssl-devel pcre-devel zlib-devel vim 安装时钟同步 yum -y install ntp vim /etc/ntp.conf 国内ntp时钟服务器 server ntp.aliyun.com ibrust 搜索 能够ping通 执行命令同步 service ntpd restart chkconfig ntpd on 查看同步情况watch ntpd -p 或则通过系统的定时任务crontab -e回车 输入 */10 * * * * /usr/sbin/ntpdate ntp.aliyun.com 创建用户及组 123groupadd bigdatauseradd -g bigdata bigdataecho &quot;123456&quot;|passwd --stdin bigdata 更换组及拥有人 1chown -R bigdata:bigdata elastic 安装memcached yum -y install memcached 安装jdk yum search java | grep -i --color JDK yum install java-1.6.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64 vi /etc/profile在最后加上 123export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk.x86_64 export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 让配置生效 source /etc/profile 复制远程好的 安装tomcat 下载 https://tomcat.apache.org/download-70.cgi wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.91/bin/apache-tomcat-7.0.91.tar.gz 解压 tar -zxvf apache-tomcat-7.0.91.tar.gz 启动 apache-tomcat-7.0.91/bin/startup.sh 查看进程 ps -ef|grep java 解析不了主机 route 命令查看路由情况 1route add default gw 192.168.1.1 配置hosts文件 vi /etc/hosts在最后加上 1234192.168.199.101 sj-node1192.168.199.102 sj-node2192.168.199.103 sj-node3192.168.199.104 sj-node4 ip根据自己的虚拟机ip填写 后面的机器别名sj-node*根据自己的喜好随意修改 无密码ssh登录 ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys scp ~/.ssh/id_dsa sj-node2:~/.ssh/ scp ~/.ssh/id_dsa.pub sj-node2:~/.ssh/ scp ~/.ssh/authorized_keys sj-node2:~/.ssh/ 最后scp 的三个步骤重复操作 将文件拷贝到 sj-node3,sj-node4]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>虚拟机 vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS]]></title>
    <url>%2Fbigdata%2Flvs%2F</url>
    <content type="text"><![CDATA[背景 架构图 LVS三种工作模式 NAT 地址转换 DR 直接路由 架构图 3台IP一样，但LVS ARP开启 其他的都关闭 TUN 隧道 架构图 原理:首先要知道,互联网上的大多Internet服务的请求包很短小,而 应答包通常很大。那么隧道模式就是,把客户端发来的数据包,封装 一个新的IP头标记(仅目的IP)发给RS,RS收到后,先把数据包的头解开,还 原数据包,处理后,直接返回给客户端,不需要再经过负载均衡器。注意,由 于RS需要对负载均衡器发过来的数据包进行还原,所以说必须支持 IPTUNNEL协议。所以,在RS的内核中,必须编译支持IPTUNNEL这个选 项 优点:负载均衡器只负责将请求包分发给后端节点服务器,而RS将应 答包直接发给用户。所以,减少了负载均衡器的大量数据流动,负载 均衡器不再是系统的瓶颈,就能处理很巨大的请求量,这种方式,一 台负载均衡器能够为很多RS进行分发。而且跑在公网上就能进行不同 地域的分发。 缺点:隧道模式的RS节点需要合法IP,这种方式需要所有的服务器支 持”IP Tunneling”(IP Encapsulation)协议,服务器可能只局限在部 分Linux系统上。 调度算法 轮叫调度（Round Robin） 加权轮叫（Weighted Round Robin） 最少链接（Least Connections） 加权最少链接（Weighted Least Connections） 基于局部性的最少链接（Locality-Based Least Connections） 带复制的基于局部性最少链接（Locality-Based Least Connections with Replication） 目标地址散列（Destination Hashing） 源地址散列（Source Hashing） 最短的期望的延迟（Shortest Expected Delay Scheduling SED） 最少队列调度（Never Queue Scheduling NQ） 实战 yum -y install ipvsadm cat /proc/sys/net/ipv4/ip_forward ipvsadm -At sysctl -P 生效 ipvsadm -Ln 查看 ipvsadm -C 清空负载配置 ipvsadm -L --timeout ipvsadm --set 1 NAT模式 -m DR模式 -g LVS 服务器 route 路由 ifconfig eth0:vip 192.168.199.100 netmask 255.255.255.0 echo 1 &gt;/proc/sys/net/ipv4/ip_forward ifconfig eth0:1 192.168.199.100 broadcast 192.168.199.100 netmask 255.255.255.255 up route add -host 192.168.199.100 dev eth0:1 ipvsadm -C ipvsadm -At 192.168.199.100:80 -s rr ipvsadm -at 192.168.199.100:80 -r 192.168.199.102:80 -g -w 1 ipvsadm -at 192.168.199.100:80 -r 192.168.199.103:80 -g -w 1 两台真实的服务器 关闭ARP响应 1234echo &quot;1&quot; &gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho &quot;2&quot; &gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho &quot;1&quot; &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho &quot;2&quot; &gt; /proc/sys/net/ipv4/conf/all/arp_announce ifconfig lo:0 192.168.199.100 broadcast 192.168.199.100 netmask 255.255.255.255 up route add -host 192.168.199.100 dev lo:0 删除默认网关 route del default TUN模式 -i LVS服务器 vi /etc/sysctl init 6]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
        <tag>大数据</tag>
        <tag>lvs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx/tengine编译安装]]></title>
    <url>%2Flinux%2Fnginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[应用场景 http://blog.csdn.net/chenyulancn/article/details/70843696 &lt;!-- more --&gt; 步骤 切换yum源，使自动寻找最近的可用yum源码 12yum install -y epel-releaseyum clean all 安装必须依赖 1yum install gcc wget openssl-devel pcre-devel zlib-devel 创建用户和用户组，为了方便nginx运行而不影响linux安全 12groupadd -r nginxuseradd -r -g nginx -M nginx 下载nginx 12cd /optwget wget http://nginx.org/download/nginx-1.9.9.tar.gz 解压nginx 12mkdir softtar zxvf nginx-1.9.9.tar.gz 编译前配置nginx 12cd nginx-1.9.9./configure --prefix=/usr/local/nginx 最好用配置 开始编译 make &amp;&amp; make install mkdir -p /var/tmp/nginx/client cd /etc/init.d vi nginxd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#!/bin/bash## chkconfig: - 85 15# description: nginx is a World Wide Web server. It is used to serve# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ "$NETWORKING" = "no" ] &amp;&amp; exit 0nginx="/opt/sxt/soft/tengine-2.1.0/sbin/nginx"prog=$(basename $nginx)NGINX_CONF_FILE="/opt/sxt/soft/tengine-2.1.0/conf/nginx.conf"#[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginx#make_dirs() &#123;# # make required directories# user=`nginx -V 2&gt;&amp;1 | grep "configure arguments:" | sed 's/[^*]*--user=\([^ ]*\).*/\1/g' -`# options=`$nginx -V 2&gt;&amp;1 | grep 'configure arguments:'`# for opt in $options; do# if [ `echo $opt | grep '.*-temp-path'` ]; then# value=`echo $opt | cut -d "=" -f 2`# if [ ! -d "$value" ]; then# # echo "creating" $value# mkdir -p $value &amp;&amp; chown -R $user $value# fi# fi# done#&#125;start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6# make_dirs echo -n $"Starting $prog: " daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $"Stopping $prog: " killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; configtest || return $? stop sleep 1 start &#125;reload() &#123; configtest || return $? echo -n $"Reloading $prog: "# -HUP是nginx平滑重启参数 killproc $nginx -HUP RETVAL=$? echo&#125;force_reload() &#123; restart&#125;configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125;rh_status() &#123; status $prog&#125;h_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1ase "$1" in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $"Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;" exit 2esac cd /opt rm -rf tengine-2.1.2 chmod +x nginxd chkconfig --add nginxd 加入系统服务 chkconfig --list nginxd 查看是否加入成功 启动,停止,重新装载 service nginxd start|stop|reload /opt/soft/tengine-2.1.2/sbin/nginx -c /opt/soft/tengine-2.1.2/conf/nginx.conf curl localhost 出现 welcome 就说明 nginx 启动成功了 chkconfig --list nginx 文档 http://tengine.taobao.org/nginx_docs/cn/docs/http nginx.org/en/docs/http End]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>负载 nginx linux 大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alfred配合截图上传七牛图床]]></title>
    <url>%2Fmac%2Falfred%E9%85%8D%E5%90%88%E6%88%AA%E5%9B%BE%E4%B8%8A%E4%BC%A0%E4%B8%83%E7%89%9B%2F</url>
    <content type="text"><![CDATA[七牛域名限制，不在使用此种方式2018-12 背景 &lt;!-- more --&gt; 实际上，保存上传找图片地址等步骤可以省略的，方法如下 一、 购买afred workflow，没它你啥都干不了 二、 注册七牛 https://portal.qiniu.com/signup?code=3lm22tsycqogi 三、 下载workflow，七牛那个 https://github.com/bestswifter/my-workflow 四、 安装完后按照说明配置，就是一点找到conf.txt 五、 进入alredworkflwo配置 六、 右击七牛——&gt;open in Finder 七、 单击就可以配置 记住一点 必须有http前缀 最后的步骤就是： ctrl+cmd+a alfred 输入 qn 博文中 ctrl+v 就行 BTY:本文的截图就是用刚刚配置好的图床搞的，效率特高]]></content>
      <categories>
        <category>mac</category>
      </categories>
      <tags>
        <tag>随笔 技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建github Pages 博客]]></title>
    <url>%2Fessay%2F%E6%90%AD%E5%BB%BAgithubpages%2F</url>
    <content type="text"><![CDATA[注册github账号 选择静态博客引擎 hexo 或者 jekyll &lt;!-- more --&gt; 选择博客模板 安装静态博客引擎 选择写博客工具markdown编辑器 自动部署ftp服务器上glynn 下载https://github.com/dmathieu/glynn sudo gem install -n /usr/local/bin glynn 在jekyll目录执行命令glynn，注意远程服务器的读写权限配置，以及自己jekyll目录中_glynn.yml文件的每一个配置]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>随笔 技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World!]]></title>
    <url>%2Fessay%2Fhello-world%2F</url>
    <content type="text"><![CDATA[一直在记笔记，以前在笔记中记，有用的发布到博客园，现在感觉要多记一点有意义的，分享出去，早听说github的pages功能最适合码农了，现在在跟上别人的脚步。Any Way, Hello World!]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
